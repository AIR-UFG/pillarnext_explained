{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models/model_backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "from torch import nn\n",
    "import spconv\n",
    "import spconv.pytorch\n",
    "from spconv.pytorch import SparseSequential, SparseConv2d, SparseConv3d\n",
    "from pillarnext_explained.models.model_utils import SparseConvBlock, SparseBasicBlock, SparseConv3dBlock, SparseBasicBlock3d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "class SparseResNet(spconv.pytorch.SparseModule):\n",
    "    \"\"\"\n",
    "    SparseResNet is a neural network model built using sparse convolutions for processing sparse input data, like LiDAR point cloud processing.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer_nums: list, # Number of blocks in each layer\n",
    "            ds_layer_strides: list, # Strides for each downsampling layer\n",
    "            ds_num_filters: list, # Number of filters for each downsampling layer\n",
    "            num_input_features: int, # Number of input features\n",
    "            kernel_size: list = [3, 3, 3, 3], # Kernel sizes for each layer\n",
    "            out_channels: int = 256 # Number of output channels\n",
    "            ):\n",
    "\n",
    "        super(SparseResNet, self).__init__()  # Call the constructor of the parent class\n",
    "        self._layer_strides = ds_layer_strides  # Store the layer strides\n",
    "        self._num_filters = ds_num_filters  # Store the number of filters for each layer\n",
    "        self._layer_nums = layer_nums  # Store the number of blocks in each layer\n",
    "        self._num_input_features = num_input_features  # Store the number of input features\n",
    "\n",
    "        # Ensure the lengths of the strides, filters, and layer numbers are consistent\n",
    "        assert len(self._layer_strides) == len(self._layer_nums)\n",
    "        assert len(self._num_filters) == len(self._layer_nums)\n",
    "\n",
    "        # Define the number of input filters for each layer\n",
    "        in_filters = [self._num_input_features, *self._num_filters[:-1]]\n",
    "        blocks = []  # Initialize the list to hold all the blocks\n",
    "\n",
    "        # Create the layers for the network\n",
    "        for i, layer_num in enumerate(self._layer_nums):\n",
    "            block = self._make_layer(\n",
    "                in_filters[i],\n",
    "                self._num_filters[i],\n",
    "                kernel_size[i],\n",
    "                self._layer_strides[i],\n",
    "                layer_num)\n",
    "            blocks.append(block)  # Add the created block to the blocks list\n",
    "\n",
    "        # Convert blocks list to a PyTorch ModuleList for proper handling in forward pass\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "        # Create the final mapping layer\n",
    "        self.mapping = spconv.pytorch.SparseSequential(\n",
    "            spconv.SparseConv2d(self._num_filters[-1],\n",
    "                                out_channels, 1, 1, bias=False),  # 1x1 convolution\n",
    "            nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01),  # Batch normalization\n",
    "            nn.ReLU(),  # Activation function\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n",
    "        \"\"\"\n",
    "        Helper function to create a layer consisting of several blocks.\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        # Add the first convolutional block with stride (downsampling)\n",
    "        layers.append(SparseConvBlock(inplanes, planes,\n",
    "                      kernel_size=kernel_size, stride=stride, use_subm=False))\n",
    "\n",
    "        # Add subsequent blocks without stride\n",
    "        for j in range(num_blocks):\n",
    "            layers.append(SparseBasicBlock(planes, kernel_size=kernel_size))\n",
    "\n",
    "        # Return the layers as a SparseSequential module\n",
    "        return spconv.pytorch.SparseSequential(*layers)\n",
    "\n",
    "    def forward(self, pillar_features, coors, input_shape):\n",
    "        \"\"\"\n",
    "        Forward pass of the network.\n",
    "        \"\"\"\n",
    "        # Determine batch size from the unique coordinates\n",
    "        batch_size = len(torch.unique(coors[:, 0]))\n",
    "        # Create a SparseConvTensor from the input features, coordinates, and shape\n",
    "        x = spconv.pytorch.SparseConvTensor(\n",
    "            pillar_features, coors, input_shape, batch_size)\n",
    "        \n",
    "        # Pass the tensor through each block sequentially\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        \n",
    "        # Apply the final mapping\n",
    "        x = self.mapping(x)\n",
    "        # Convert the sparse tensor to a dense tensor and return\n",
    "        return x.dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse ResNet 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "class SparseResNet3D(spconv.pytorch.SparseModule):\n",
    "    \"\"\"\n",
    "    SparseResNet3D is a 3D variant of the SparseResNet model, designed for processing 3D sparse input data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_nums: list,  # Number of blocks in each layer\n",
    "        ds_layer_strides: list,  # Strides for each downsampling layer\n",
    "        ds_num_filters: list,  # Number of filters for each downsampling layer\n",
    "        num_input_features: int,  # Number of input features\n",
    "        kernel_size: list = [3, 3, 3, 3],  # Kernel sizes for each layer\n",
    "        out_channels: int = 128  # Number of output channels\n",
    "    ):\n",
    "        super(SparseResNet3D, self).__init__()\n",
    "        \n",
    "        # Initialize the instance variables\n",
    "        self._layer_strides = ds_layer_strides\n",
    "        self._num_filters = ds_num_filters\n",
    "        self._layer_nums = layer_nums\n",
    "        self._num_input_features = num_input_features\n",
    "\n",
    "        # Ensure the lengths of the lists match\n",
    "        assert len(self._layer_strides) == len(self._layer_nums)\n",
    "        assert len(self._num_filters) == len(self._layer_nums)\n",
    "\n",
    "        # List of input filters for each layer\n",
    "        in_filters = [self._num_input_features, *self._num_filters[:-1]]\n",
    "        blocks = []\n",
    "\n",
    "        # Create layers based on the given parameters\n",
    "        for i, layer_num in enumerate(self._layer_nums):\n",
    "            block = self._make_layer(\n",
    "                in_filters[i],\n",
    "                self._num_filters[i],\n",
    "                kernel_size[i],\n",
    "                self._layer_strides[i],\n",
    "                layer_num)\n",
    "            blocks.append(block)\n",
    "\n",
    "        # Store the blocks in a ModuleList\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        \n",
    "        # Define the mapping layer\n",
    "        self.mapping = SparseConv3dBlock(\n",
    "            self._num_filters[-1], out_channels, kernel_size=1, stride=1, use_subm=True)\n",
    "        \n",
    "        # Define the extra convolutional layer\n",
    "        self.extra_conv = SparseSequential(\n",
    "            SparseConv3d(\n",
    "                self._num_filters[-1], self._num_filters[-1], (3, 1, 1), (2, 1, 1), bias=False),\n",
    "            nn.BatchNorm1d(self._num_filters[-1], eps=1e-3, momentum=0.01),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n",
    "        \"\"\"\n",
    "        Creates a single layer composed of sparse 3D convolution blocks.\n",
    "        \"\"\"\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        # Add the first convolution block with downsampling\n",
    "        layers.append(SparseConv3dBlock(inplanes, planes,\n",
    "                      kernel_size=kernel_size, stride=stride, use_subm=False))\n",
    "\n",
    "        # Add the remaining blocks without downsampling\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(SparseBasicBlock3d(planes, kernel_size=kernel_size))\n",
    "\n",
    "        return spconv.pytorch.SparseSequential(*layers)\n",
    "\n",
    "    def forward(self, pillar_features, coors, input_shape):\n",
    "        # Get the batch size from the unique coordinates\n",
    "        batch_size = len(torch.unique(coors[:, 0]))\n",
    "        \n",
    "        # Create a sparse tensor\n",
    "        x = spconv.pytorch.SparseConvTensor(\n",
    "            pillar_features, coors, input_shape, batch_size)\n",
    "        \n",
    "        # Pass the tensor through the blocks\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        \n",
    "        # Apply the extra convolution and mapping layers\n",
    "        x = self.extra_conv(x)\n",
    "        x = self.mapping(x)\n",
    "        \n",
    "        # Convert the sparse tensor to a dense format\n",
    "        x = x.dense()\n",
    "        \n",
    "        # Reshape the output tensor\n",
    "        B, C, D, H, W = x.shape\n",
    "        x = x.view(B, C * D, H, W)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
