{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build loader\n",
    "\n",
    "> Module to load the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp build_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from pillarnext_explained import dataset as pillarnext_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def collate(batch_list):\n",
    "    \"\"\"This function is designed to merge a batch of data examples into a format suitable for further processing.\"\"\"\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == \"token\":\n",
    "            ret[key] = elems\n",
    "        elif 'point' in key:\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode=\"constant\", constant_values=i\n",
    "                )\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = torch.tensor(np.concatenate(coors, axis=0))\n",
    "        elif isinstance(elems[0], list):\n",
    "            ret[key] = defaultdict(list)\n",
    "            res = []\n",
    "            for elem in elems:\n",
    "                for idx, ele in enumerate(elem):\n",
    "                    ret[key][str(idx)].append(torch.tensor(ele))\n",
    "            for kk, vv in ret[key].items():\n",
    "                res.append(torch.stack(vv))\n",
    "            ret[key] = res\n",
    "        else:\n",
    "            ret[key] = torch.tensor(np.stack(elems, axis=0)).float()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: [[1, 2, 3], [4, 5, 6]]\n",
      "point1: tensor([[0., 1., 2.],\n",
      "        [0., 3., 4.],\n",
      "        [1., 7., 8.]], dtype=torch.float64)\n",
      "point2: tensor([[ 0.,  5.,  6.],\n",
      "        [ 1.,  9., 10.],\n",
      "        [ 1., 11., 12.]], dtype=torch.float64)\n",
      "nested_list: [tensor([[1, 2],\n",
      "        [5, 6]]), tensor([[3, 4],\n",
      "        [7, 8]])]\n",
      "value: tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Sample batch list of examples\n",
    "batch_list = [\n",
    "    {\n",
    "        \"token\": [1, 2, 3],\n",
    "        \"point1\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n",
    "        \"point2\": np.array([[5.0, 6.0]]),\n",
    "        \"nested_list\": [[1, 2], [3, 4]],\n",
    "        \"value\": np.array([1.0, 2.0])\n",
    "    },\n",
    "    {\n",
    "        \"token\": [4, 5, 6],\n",
    "        \"point1\": np.array([[7.0, 8.0]]),\n",
    "        \"point2\": np.array([[9.0, 10.0], [11.0, 12.0]]),\n",
    "        \"nested_list\": [[5, 6], [7, 8]],\n",
    "        \"value\": np.array([3.0, 4.0])\n",
    "    }\n",
    "]\n",
    "\n",
    "# Using the collate function\n",
    "collated_batch = collate(batch_list)\n",
    "\n",
    "# Display the collated result\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def build_dataloader(dataset, # Dataset object\n",
    "                     batch_size=4, # Batch size\n",
    "                     num_workers=8, # Number of workers\n",
    "                     shuffle:bool=False, # Shuffle the data\n",
    "                     pin_memory=False # Pin memory\n",
    "                     ):\n",
    "    \"\"\"This function is designed to build a DataLoader object for a given dataset.\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        sampler = DistributedSampler(\n",
    "            dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=(sampler is None and shuffle),\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 303\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "train_dataset = pillarnext_dataset.NuScenesDataset(\"infos_train_10sweeps_withvelo_filterZero.pkl\",\n",
    "                                \"/root/nuscenes-dataset/v1.0-mini\",\n",
    "                                10,\n",
    "                                class_names=[[\"car\"], [\"truck\", \"construction_vehicle\"], [\"bus\", \"trailer\"], [\"barrier\"], [\"motorcycle\", \"bicycle\"], [\"pedestrian\", \"traffic_cone\"]],\n",
    "                                resampling=True)\n",
    "\n",
    "train_loader = build_dataloader(train_dataset)\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
