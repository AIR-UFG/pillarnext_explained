{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch_scatter\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pillar Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PFNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pillar Feature Net Layer.\n",
    "    The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only\n",
    "    used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,  # Number of input channels\n",
    "                 out_channels: int,  # Number of output channels\n",
    "                 norm_cfg=None,  # Normalization config (not used here, but could be for future extensions)\n",
    "                 last_layer: bool = False  # If last_layer, there is no concatenation of features\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.last_vfe = last_layer  # Check if this is the last layer\n",
    "        if not self.last_vfe:\n",
    "            out_channels = out_channels // 2  # If not the last layer, half the output channels\n",
    "        self.units = out_channels\n",
    "\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)  # Linear layer to transform inputs\n",
    "        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)  # Batch normalization\n",
    "\n",
    "    def forward(self, inputs, unq_inv):\n",
    "        torch.backends.cudnn.enabled = False  # Disable cuDNN for compatibility reasons\n",
    "        x = self.linear(inputs)  # Apply linear transformation\n",
    "        x = self.norm(x)  # Apply batch normalization\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        torch.backends.cudnn.enabled = True  # Re-enable cuDNN\n",
    "\n",
    "        # max pooling\n",
    "        feat_max = torch_scatter.scatter_max(x, unq_inv, dim=0)[0]  # Perform scatter max pooling\n",
    "        x_max = feat_max[unq_inv]  # Gather the max features for each point\n",
    "\n",
    "        if self.last_vfe:\n",
    "            return x_max  # If this is the last layer, return the max features\n",
    "        else:\n",
    "            x_concatenated = torch.cat([x, x_max], dim=1)  # Otherwise, concatenate the original and max features\n",
    "            return x_concatenated  # Return the concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PillarNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PillarNet.\n",
    "    The network performs dynamic pillar scatter that convert point cloud into pillar representation\n",
    "    and extract pillar features\n",
    "\n",
    "    Reference:\n",
    "    PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784)\n",
    "    End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_features: int, # Number of input features\n",
    "                 voxel_size: list, # Size of voxels, only utilize x and y size\n",
    "                 pc_range: list, # Point cloud range, only utilize x and y min\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self,\n",
    "                points: torch.Tensor # Points in LiDAR coordinate, shape: (N, d), format: batch_id, x, y, z, feat1, ...\n",
    "                ):\n",
    "\n",
    "        device = points.device\n",
    "        dtype = points.dtype\n",
    "\n",
    "        # discard out of range points\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n",
    "                     )/self.voxel_size  # x,  y, z\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n",
    "\n",
    "        mask = reduce(torch.logical_and, (points_coords[:, 0] >= 0,\n",
    "                                          points_coords[:, 0] < grid_size[0],\n",
    "                                          points_coords[:, 1] >= 0,\n",
    "                                          points_coords[:, 1] < grid_size[1]))\n",
    "\n",
    "        points = points[mask]\n",
    "        points_coords = points_coords[mask]\n",
    "\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points[:, 0:1].long()\n",
    "\n",
    "        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n",
    "        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()\n",
    "\n",
    "        points_mean_scatter = torch_scatter.scatter_mean(\n",
    "            points[:, 1:4], unq_inv, dim=0)\n",
    "\n",
    "        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n",
    "\n",
    "        # Find distance of x, y, and z from pillar center\n",
    "        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n",
    "                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n",
    "\n",
    "        # Combine together feature decorations\n",
    "        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n",
    "\n",
    "        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PillarFeatureNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pillar Feature Net.\n",
    "    The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a\n",
    "    similar role to SECOND's second.pytorch.voxelnet.VoxelFeatureExtractor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int, # Number of input features\n",
    "        num_filters: list, # Number of features in each of the N PFNLayers\n",
    "        voxel_size: list, # Size of voxels, only utilize x and y size\n",
    "        pc_range: list, # Point cloud range, only utilize x and y min\n",
    "        norm_cfg:None, # Normalization config\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        assert len(num_filters) > 0\n",
    "        num_input_features += 5\n",
    "\n",
    "        # Create PillarFeatureNet layers\n",
    "        num_filters = [num_input_features] + list(num_filters)\n",
    "        pfn_layers = []\n",
    "        for i in range(len(num_filters) - 1):\n",
    "            in_filters = num_filters[i]\n",
    "            out_filters = num_filters[i + 1]\n",
    "            if i < len(num_filters) - 2:\n",
    "                last_layer = False\n",
    "            else:\n",
    "                last_layer = True\n",
    "            pfn_layers.append(\n",
    "                PFNLayer(\n",
    "                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n",
    "                )\n",
    "            )\n",
    "        self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    "\n",
    "        self.feature_output_dim = num_filters[-1]\n",
    "\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "        self.voxelization = PillarNet(num_input_features, voxel_size, pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        features, coords, unq_inv, grid_size = self.voxelization(points)\n",
    "        # Forward pass through PFNLayers\n",
    "        for pfn in self.pfn_layers:\n",
    "            features = pfn(features, unq_inv)  # num_points, dim_feat\n",
    "\n",
    "        feat_max = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n",
    "\n",
    "        return feat_max, coords, grid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voxel Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DynamicVoxelEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic version of VoxelFeatureExtractorV3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DynamicVoxelEncoder, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, unq_inv):\n",
    "        features = torch_scatter.scatter_mean(inputs, unq_inv, dim=0)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VoxelNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic voxelization for point clouds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                voxel_size, # size of voxel\n",
    "                pc_range # point cloud range\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        \"\"\"\n",
    "        points: Tensor: (N, d), batch_id, x, y, z, ...\n",
    "        \"\"\"\n",
    "        device = points.device\n",
    "\n",
    "        # voxel range of x, y, z\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]) / self.voxel_size\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)  # x, y, z\n",
    "\n",
    "        mask = reduce(torch.logical_and, (points_coords[:, 0] >= 0,\n",
    "                                          points_coords[:, 0] < grid_size[0],\n",
    "                                          points_coords[:, 1] >= 0,\n",
    "                                          points_coords[:, 1] < grid_size[1],\n",
    "                                          points_coords[:, 2] >= 0,\n",
    "                                          points_coords[:, 2] < grid_size[2]))  # remove the points out of range\n",
    "\n",
    "        points = points[mask]\n",
    "        points_coords = points_coords[mask]\n",
    "\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points[:, 0:1].long()\n",
    "        point_index = torch.cat((batch_idx, points_coords), dim=1)\n",
    "\n",
    "        unq, unq_inv = torch.unique(point_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()\n",
    "\n",
    "        features = points[:, 1:]\n",
    "\n",
    "        return features, unq[:, [0, 3, 2, 1]], unq_inv, grid_size[[2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VoxelFeatureNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                voxel_size, # size of voxel\n",
    "                pc_range # point cloud range\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.voxelization = VoxelNet(voxel_size, pc_range)\n",
    "        self.voxel_encoder = DynamicVoxelEncoder()\n",
    "\n",
    "    def forward(self, points):\n",
    "        features, coords, unq_inv, grid_size = self.voxelization(points)\n",
    "\n",
    "        features = self.voxel_encoder(features, unq_inv)\n",
    "\n",
    "        return features, coords, grid_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
