{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model_readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import torch_scatter\n",
    "from functools import reduce\n",
    "import spconv\n",
    "import spconv.pytorch\n",
    "from pillarnext_explained.model_utils import SparseConvBlock, SparseBasicBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pillar Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PFNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Pillar Feature Net Layer.\n",
    "    The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only\n",
    "    used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_channels: int,  # Number of input channels\n",
    "                 out_channels: int,  # Number of output channels\n",
    "                 norm_cfg=None,  # Normalization config (not used here, but could be for future extensions)\n",
    "                 last_layer: bool = False  # If last_layer, there is no concatenation of features\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.last_vfe = last_layer  # Check if this is the last layer\n",
    "        if not self.last_vfe:\n",
    "            out_channels = out_channels // 2  # If not the last layer, half the output channels\n",
    "        self.units = out_channels\n",
    "\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)  # Linear layer to transform inputs\n",
    "        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)  # Batch normalization\n",
    "\n",
    "    def forward(self, inputs, unq_inv):\n",
    "        torch.backends.cudnn.enabled = False  # Disable cuDNN for compatibility reasons\n",
    "        x = self.linear(inputs)  # Apply linear transformation\n",
    "        x = self.norm(x)  # Apply batch normalization\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        torch.backends.cudnn.enabled = True  # Re-enable cuDNN\n",
    "\n",
    "        # max pooling\n",
    "        feat_max = torch_scatter.scatter_max(x, unq_inv, dim=0)[0]  # Perform scatter max pooling\n",
    "        x_max = feat_max[unq_inv]  # Gather the max features for each point\n",
    "\n",
    "        if self.last_vfe:\n",
    "            return x_max  # If this is the last layer, return the max features\n",
    "        else:\n",
    "            x_concatenated = torch.cat([x, x_max], dim=1)  # Otherwise, concatenate the original and max features\n",
    "            return x_concatenated  # Return the concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PillarNet(nn.Module):\n",
    "    \"\"\"\n",
    "    PillarNet.\n",
    "    The network performs dynamic pillar scatter that convert point cloud into pillar representation\n",
    "    and extract pillar features\n",
    "\n",
    "    Reference:\n",
    "    PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784)\n",
    "    End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_features: int, # Number of input features\n",
    "                 voxel_size: list, # Size of voxels, only utilize x and y size\n",
    "                 pc_range: list, # Point cloud range, only utilize x and y min\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self,\n",
    "                points: torch.Tensor # Points in LiDAR coordinate, shape: (N, d), format: batch_id, x, y, z, feat1, ...\n",
    "                ):\n",
    "\n",
    "        device = points.device\n",
    "        dtype = points.dtype\n",
    "\n",
    "        # discard out of range points\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n",
    "                     )/self.voxel_size  # x,  y, z\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n",
    "\n",
    "        mask = reduce(torch.logical_and, (points_coords[:, 0] >= 0,\n",
    "                                          points_coords[:, 0] < grid_size[0],\n",
    "                                          points_coords[:, 1] >= 0,\n",
    "                                          points_coords[:, 1] < grid_size[1]))\n",
    "\n",
    "        points = points[mask]\n",
    "        points_coords = points_coords[mask]\n",
    "\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points[:, 0:1].long()\n",
    "\n",
    "        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n",
    "        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()\n",
    "\n",
    "        points_mean_scatter = torch_scatter.scatter_mean(\n",
    "            points[:, 1:4], unq_inv, dim=0)\n",
    "\n",
    "        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n",
    "\n",
    "        # Find distance of x, y, and z from pillar center\n",
    "        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n",
    "                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n",
    "\n",
    "        # Combine together feature decorations\n",
    "        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n",
    "\n",
    "        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PillarFeatureNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pillar Feature Net.\n",
    "    The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a\n",
    "    similar role to SECOND's second.pytorch.voxelnet.VoxelFeatureExtractor.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_input_features: int, # Number of input features\n",
    "        num_filters: list, # Number of features in each of the N PFNLayers\n",
    "        voxel_size: list, # Size of voxels, only utilize x and y size\n",
    "        pc_range: list, # Point cloud range, only utilize x and y min\n",
    "        norm_cfg:None, # Normalization config\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        assert len(num_filters) > 0\n",
    "        num_input_features += 5\n",
    "\n",
    "        # Create PillarFeatureNet layers\n",
    "        num_filters = [num_input_features] + list(num_filters)\n",
    "        pfn_layers = []\n",
    "        for i in range(len(num_filters) - 1):\n",
    "            in_filters = num_filters[i]\n",
    "            out_filters = num_filters[i + 1]\n",
    "            if i < len(num_filters) - 2:\n",
    "                last_layer = False\n",
    "            else:\n",
    "                last_layer = True\n",
    "            pfn_layers.append(\n",
    "                PFNLayer(\n",
    "                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n",
    "                )\n",
    "            )\n",
    "        self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    "\n",
    "        self.feature_output_dim = num_filters[-1]\n",
    "\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "        self.voxelization = PillarNet(num_input_features, voxel_size, pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        features, coords, unq_inv, grid_size = self.voxelization(points)\n",
    "        # Forward pass through PFNLayers\n",
    "        for pfn in self.pfn_layers:\n",
    "            features = pfn(features, unq_inv)  # num_points, dim_feat\n",
    "\n",
    "        feat_max = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n",
    "\n",
    "        return feat_max, coords, grid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voxel Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class DynamicVoxelEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic version of VoxelFeatureExtractorV3\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DynamicVoxelEncoder, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, unq_inv):\n",
    "        features = torch_scatter.scatter_mean(inputs, unq_inv, dim=0)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VoxelNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic voxelization for point clouds\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                voxel_size, # size of voxel\n",
    "                pc_range # point cloud range\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        \"\"\"\n",
    "        points: Tensor: (N, d), batch_id, x, y, z, ...\n",
    "        \"\"\"\n",
    "        device = points.device\n",
    "\n",
    "        # voxel range of x, y, z\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]) / self.voxel_size\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)  # x, y, z\n",
    "\n",
    "        mask = reduce(torch.logical_and, (points_coords[:, 0] >= 0,\n",
    "                                          points_coords[:, 0] < grid_size[0],\n",
    "                                          points_coords[:, 1] >= 0,\n",
    "                                          points_coords[:, 1] < grid_size[1],\n",
    "                                          points_coords[:, 2] >= 0,\n",
    "                                          points_coords[:, 2] < grid_size[2]))  # remove the points out of range\n",
    "\n",
    "        points = points[mask]\n",
    "        points_coords = points_coords[mask]\n",
    "\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points[:, 0:1].long()\n",
    "        point_index = torch.cat((batch_idx, points_coords), dim=1)\n",
    "\n",
    "        unq, unq_inv = torch.unique(point_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()\n",
    "\n",
    "        features = points[:, 1:]\n",
    "\n",
    "        return features, unq[:, [0, 3, 2, 1]], unq_inv, grid_size[[2, 1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class VoxelFeatureNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                voxel_size, # size of voxel\n",
    "                pc_range # point cloud range\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.voxelization = VoxelNet(voxel_size, pc_range)\n",
    "        self.voxel_encoder = DynamicVoxelEncoder()\n",
    "\n",
    "    def forward(self, points):\n",
    "        features, coords, unq_inv, grid_size = self.voxelization(points)\n",
    "\n",
    "        features = self.voxel_encoder(features, unq_inv)\n",
    "\n",
    "        return features, coords, grid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MVF Enconder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PointNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Process for point feature\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                in_channels:int, # Number of input channels\n",
    "                out_channels:int # Number of output channels\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n",
    "\n",
    "    def forward(self, points):\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        x = self.linear(points)\n",
    "        x = self.norm(x)\n",
    "        x = F.relu(x)\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class PillarVoxelNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                voxel_size, # Size of voxels, only utilize x and y size\n",
    "                pc_range # Point cloud range, only utilize x and y min\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        device = points.device\n",
    "        dtype = points.dtype\n",
    "\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n",
    "                     )/self.voxel_size  # x,  y, z\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n",
    "        points_coords[:, 0] = torch.clamp(\n",
    "            points_coords[:, 0], 0, grid_size[0] - 1)\n",
    "        points_coords[:, 1] = torch.clamp(\n",
    "            points_coords[:, 1], 0, grid_size[1] - 1)\n",
    "        points_coords[:, 2] = torch.clamp(\n",
    "            points_coords[:, 2], 0, grid_size[2] - 1)\n",
    "\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points[:, 0:1].long()\n",
    "\n",
    "        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n",
    "        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()        # breakpoint()\n",
    "\n",
    "        points_mean_scatter = torch_scatter.scatter_mean(\n",
    "            points[:, 1:4], unq_inv, dim=0)\n",
    "\n",
    "        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n",
    "\n",
    "        # Find distance of x, y, and z from pillar center\n",
    "        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n",
    "                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n",
    "\n",
    "        # Combine together feature decorations\n",
    "        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n",
    "\n",
    "        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CylinderNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                voxel_size, # Size of voxels, only utilize x and y size\n",
    "                pc_range # Point cloud range, only utilize x and y min\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.voxel_size = np.array(voxel_size)\n",
    "        self.pc_range = np.array(pc_range)\n",
    "\n",
    "    def forward(self, points):\n",
    "        device = points.device\n",
    "        dtype = points.dtype\n",
    "        points_x = points[:, 1:2]\n",
    "        points_y = points[:, 2:3]\n",
    "        points_z = points[:, 3:4]\n",
    "        points_phi = torch.atan2(points_y, points_x) / np.pi * 180\n",
    "        points_rho = torch.sqrt(points_x ** 2 + points_y ** 2)\n",
    "        points_cylinder = torch.cat(\n",
    "            (points[:, 0:1], points_phi, points_z, points_rho, points[:, 4:]), dim=-1)\n",
    "\n",
    "        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n",
    "                     )/self.voxel_size  # phi, z, rho\n",
    "        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n",
    "\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(points).to(device)\n",
    "        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n",
    "\n",
    "        points_coords = (\n",
    "            points_cylinder[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)\n",
    "        points_coords[:, 0] = torch.clamp(\n",
    "            points_coords[:, 0], 0, grid_size[0] - 1)\n",
    "        points_coords[:, 1] = torch.clamp(\n",
    "            points_coords[:, 1], 0, grid_size[1] - 1)\n",
    "        points_coords[:, 2] = torch.clamp(\n",
    "            points_coords[:, 2], 0, grid_size[2] - 1)\n",
    "        points_coords = points_coords.long()\n",
    "        batch_idx = points_cylinder[:, 0:1].long()\n",
    "\n",
    "        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n",
    "        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n",
    "        unq = unq.int()\n",
    "\n",
    "        points_mean_scatter = torch_scatter.scatter_mean(\n",
    "            points_cylinder[:, 1:4], unq_inv, dim=0)\n",
    "        f_cluster = points_cylinder[:, 1:4] - points_mean_scatter[unq_inv]\n",
    "\n",
    "        # Find distance of x, y, and z from pillar center\n",
    "        f_center = points_cylinder[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n",
    "                                              voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n",
    "\n",
    "        # Combine together feature decorations\n",
    "        features = torch.cat(\n",
    "            [points_cylinder[:, 1:], f_cluster, f_center], dim=-1)\n",
    "\n",
    "        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class SingleView(nn.Module):\n",
    "    \"\"\"\n",
    "    authoured by Beijing-jinyu\n",
    "    convolution for single view\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,  # Number of input channels\n",
    "                 num_filters,  # Number of features in each of the N PFNLayers\n",
    "                 layer_nums,  # Number of blocks in each layer\n",
    "                 ds_layer_strides,  # Strides of each layer\n",
    "                 ds_num_filters,  # Number of features in each layer\n",
    "                 kernel_size,  # Kernel size of each layer\n",
    "                 mode,  # Mode of the network\n",
    "                 voxel_size,  # Size of voxels, only utilize x and y size\n",
    "                 pc_range,  # Point cloud range, only utilize x and y min\n",
    "                 norm_cfg=None,  # Normalization config\n",
    "                 act_cfg=None,  # Activation config\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        self.voxel_size = np.array(voxel_size[:2])\n",
    "        self.bias = np.array(pc_range[:2])\n",
    "        num_filters = [in_channels] + list(num_filters)\n",
    "        pfn_layers = []\n",
    "        for i in range(len(num_filters) - 1):\n",
    "            in_filters = num_filters[i]\n",
    "            out_filters = num_filters[i + 1]\n",
    "            if i < len(num_filters) - 2:\n",
    "                last_layer = False\n",
    "            else:\n",
    "                last_layer = True\n",
    "            pfn_layers.append(\n",
    "                PFNLayer(\n",
    "                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n",
    "                )\n",
    "            )\n",
    "        in_filters = [num_filters[-1], *ds_num_filters[:-1]]\n",
    "        self.pfn_layers = nn.ModuleList(pfn_layers)\n",
    "        blocks = []\n",
    "        for i, layer_num in enumerate(layer_nums):\n",
    "            block = self._make_layer(\n",
    "                in_filters[i],\n",
    "                ds_num_filters[i],\n",
    "                kernel_size[i],\n",
    "                ds_layer_strides[i],\n",
    "                layer_num)\n",
    "            blocks.append(block)\n",
    "\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "        self.ds_rate = np.prod(np.array(ds_layer_strides))\n",
    "\n",
    "    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(SparseConvBlock(inplanes, planes,\n",
    "                      kernel_size=kernel_size, stride=stride, use_subm=False))\n",
    "\n",
    "        for j in range(num_blocks):\n",
    "            layers.append(SparseBasicBlock(planes, kernel_size=kernel_size))\n",
    "\n",
    "        return spconv.pytorch.SparseSequential(*layers)\n",
    "\n",
    "    def forward(self, features, unq, unq_inv, grid_size):\n",
    "        feature_pos = features[:,\n",
    "                               0:2] if self.mode == 'pillar' else features[:, 10:12]\n",
    "        device = feature_pos.device\n",
    "        voxel_size = torch.from_numpy(\n",
    "            self.voxel_size).type_as(feature_pos).to(device)\n",
    "        bias = torch.from_numpy(self.bias).type_as(feature_pos).to(device)\n",
    "        feature_pos = (feature_pos - bias) / voxel_size\n",
    "\n",
    "        for pfn in self.pfn_layers:\n",
    "            features = pfn(features, unq_inv)  # num_points, dim_feat\n",
    "        features_voxel = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n",
    "        batch_size = len(torch.unique(unq[:, 0]))\n",
    "        x = spconv.pytorch.SparseConvTensor(\n",
    "            features_voxel, unq, grid_size, batch_size)\n",
    "\n",
    "        for i in range(len(self.blocks)):\n",
    "            x = self.blocks[i](x)\n",
    "        x = x.dense()\n",
    "        feature_pos = torch.cat(\n",
    "            (unq[unq_inv][:, 0:1], feature_pos / self.ds_rate), dim=-1)\n",
    "\n",
    "        return self.bilinear_interpolate(x, feature_pos)\n",
    "\n",
    "    def bilinear_interpolate(self, image, coords):\n",
    "        \"\"\"\n",
    "        image: (B, C, H, W)\n",
    "        coords: (N, 3): (B, y, x)\n",
    "        \"\"\"\n",
    "        x = coords[:, 1]\n",
    "        x0 = torch.floor(x).long()\n",
    "        x1 = x0 + 1\n",
    "\n",
    "        y = coords[:, 2]\n",
    "        y0 = torch.floor(y).long()\n",
    "        y1 = y0 + 1\n",
    "\n",
    "        B = coords[:, 0].long()\n",
    "\n",
    "        x0 = torch.clamp(x0, 0, image.shape[3] - 1)\n",
    "        x1 = torch.clamp(x1, 0, image.shape[3] - 1)\n",
    "        y0 = torch.clamp(y0, 0, image.shape[2] - 1)\n",
    "        y1 = torch.clamp(y1, 0, image.shape[2] - 1)\n",
    "\n",
    "        Ia = image[B, :, y0, x0]\n",
    "        Ib = image[B, :, y1, x0]\n",
    "        Ic = image[B, :, y0, x1]\n",
    "        Id = image[B, :, y1, x1]\n",
    "\n",
    "        wa = ((x1.type(torch.float32)-x) *\n",
    "              (y1.type(torch.float32)-y)).unsqueeze(-1)\n",
    "        wb = ((x1.type(torch.float32)-x) *\n",
    "              (y-y0.type(torch.float32))).unsqueeze(-1)\n",
    "        wc = ((x-x0.type(torch.float32)) *\n",
    "              (y1.type(torch.float32)-y)).unsqueeze(-1)\n",
    "        wd = ((x-x0.type(torch.float32)) *\n",
    "              (y-y0.type(torch.float32))).unsqueeze(-1)\n",
    "\n",
    "        features = Ia * wa + Ib * wb + Ic * wc + Id * wd\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MVFFeatureNet(nn.Module):\n",
    "    \"\"\"\n",
    "    authoured by Beijing-jinyu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init(self,\n",
    "                in_channels, # Number of input channels\n",
    "                voxel_size, # Size of voxels, only utilize x and y size\n",
    "                pc_range, # Point cloud range, only utilize x and y min\n",
    "                cylinder_size, # Size of cylinders, only utilize x and y size\n",
    "                cylinder_range, # Cylinder range, only utilize x and y min\n",
    "                num_filters, # Number of features in each of the N PFNLayers\n",
    "                layer_nums, # Number of blocks in each layer\n",
    "                ds_layer_strides, # Strides of each layer\n",
    "                ds_num_filters, # Number of features in each layer\n",
    "                kernel_size, # Kernel size of each layer\n",
    "                out_channels # Number of output channels\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.voxel_size = voxel_size\n",
    "        self.pc_range = pc_range\n",
    "        self.cylinder_range = cylinder_range\n",
    "        self.cylinder_size = cylinder_size\n",
    "\n",
    "        self.voxelization = PillarVoxelNet(voxel_size, pc_range)\n",
    "        self.cylinderlization = CylinderNet(cylinder_size, cylinder_range)\n",
    "\n",
    "        self.pillarview = SingleView((in_channels + 5) * 2, num_filters, layer_nums, ds_layer_strides,\n",
    "                                     ds_num_filters, kernel_size, 'pillar', self.voxel_size, self.pc_range)\n",
    "        self.cylinderview = SingleView((in_channels + 5) * 2, num_filters, layer_nums, ds_layer_strides,\n",
    "                                       ds_num_filters, kernel_size, 'cylinder', self.cylinder_size, self.cylinder_range)\n",
    "        self.ds_rate = np.prod(np.array(ds_layer_strides))\n",
    "\n",
    "        self.pointnet1 = PointNet((in_channels + 5) * 2, ds_num_filters[-1])\n",
    "        self.pointnet2 = PointNet(ds_num_filters[-1] * 3, out_channels)\n",
    "\n",
    "    def forward(self, points):\n",
    "        dtype = points.dtype\n",
    "        pc_range = torch.tensor(self.pc_range, dtype=dtype)\n",
    "        mask = reduce(torch.logical_and, (points[:, 1] >= pc_range[0],\n",
    "                                          points[:, 1] < pc_range[3],\n",
    "                                          points[:, 2] >= pc_range[1],\n",
    "                                          points[:, 2] < pc_range[4],\n",
    "                                          points[:, 3] >= pc_range[2],\n",
    "                                          points[:, 3] < pc_range[5]))\n",
    "        points = points[mask]\n",
    "\n",
    "        pillar_feature, pillar_coords, pillar_inv, pillar_size = self.voxelization(\n",
    "            points)\n",
    "        cylinder_feature, cylinder_coords, cylinder_inv, cylinder_size = self.cylinderlization(\n",
    "            points)\n",
    "        points_feature = torch.cat((pillar_feature, cylinder_feature), dim=-1)\n",
    "\n",
    "        pillar_view = self.pillarview(\n",
    "            points_feature, pillar_coords, pillar_inv, pillar_size)\n",
    "        cylinder_view = self.cylinderview(\n",
    "            points_feature, cylinder_coords, cylinder_inv, cylinder_size)\n",
    "\n",
    "        points_feature = self.pointnet1(points_feature)\n",
    "        points_feature = torch.cat(\n",
    "            (points_feature, pillar_view, cylinder_view), dim=-1)\n",
    "        pillar_feature = self.pointnet2(points_feature)\n",
    "        pillar_feature = torch_scatter.scatter_max(\n",
    "            pillar_feature, pillar_inv, dim=0)[0]\n",
    "        batch_size = len(torch.unique(pillar_coords[:, 0]))\n",
    "        pillar_coords[:, 1:] = pillar_coords[:, 1:] // self.ds_rate\n",
    "        pillar_size = pillar_size // self.ds_rate\n",
    "        x = spconv.pytorch.SparseConvTensor(\n",
    "            pillar_feature, pillar_coords, pillar_size, batch_size)\n",
    "        return x.dense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
