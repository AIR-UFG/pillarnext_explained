{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build loader\n",
    "\n",
    "> Module to load the data from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp datasets/build_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from pillarnext_explained.datasets import dataset as pillarnext_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def collate(batch_list):\n",
    "    \"\"\"This function is designed to merge a batch of data examples into a format suitable for further processing.\"\"\"\n",
    "    example_merged = defaultdict(list)\n",
    "    for example in batch_list:\n",
    "        for k, v in example.items():\n",
    "            example_merged[k].append(v)\n",
    "    ret = {}\n",
    "    for key, elems in example_merged.items():\n",
    "        if key == \"token\":\n",
    "            ret[key] = elems\n",
    "        elif 'point' in key:\n",
    "            coors = []\n",
    "            for i, coor in enumerate(elems):\n",
    "                coor_pad = np.pad(\n",
    "                    coor, ((0, 0), (1, 0)), mode=\"constant\", constant_values=i\n",
    "                )\n",
    "                coors.append(coor_pad)\n",
    "            ret[key] = torch.tensor(np.concatenate(coors, axis=0))\n",
    "        elif isinstance(elems[0], list):\n",
    "            ret[key] = defaultdict(list)\n",
    "            res = []\n",
    "            for elem in elems:\n",
    "                for idx, ele in enumerate(elem):\n",
    "                    ret[key][str(idx)].append(torch.tensor(ele))\n",
    "            for kk, vv in ret[key].items():\n",
    "                res.append(torch.stack(vv))\n",
    "            ret[key] = res\n",
    "        else:\n",
    "            ret[key] = torch.tensor(np.stack(elems, axis=0)).float()\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: [[1, 2, 3], [4, 5, 6]]\n",
      "point1: tensor([[0., 1., 2.],\n",
      "        [0., 3., 4.],\n",
      "        [1., 7., 8.]], dtype=torch.float64)\n",
      "point2: tensor([[ 0.,  5.,  6.],\n",
      "        [ 1.,  9., 10.],\n",
      "        [ 1., 11., 12.]], dtype=torch.float64)\n",
      "nested_list: [tensor([[1, 2],\n",
      "        [5, 6]]), tensor([[3, 4],\n",
      "        [7, 8]])]\n",
      "value: tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "# Sample batch list of examples\n",
    "batch_list = [\n",
    "    {\n",
    "        \"token\": [1, 2, 3],\n",
    "        \"point1\": np.array([[1.0, 2.0], [3.0, 4.0]]),\n",
    "        \"point2\": np.array([[5.0, 6.0]]),\n",
    "        \"nested_list\": [[1, 2], [3, 4]],\n",
    "        \"value\": np.array([1.0, 2.0])\n",
    "    },\n",
    "    {\n",
    "        \"token\": [4, 5, 6],\n",
    "        \"point1\": np.array([[7.0, 8.0]]),\n",
    "        \"point2\": np.array([[9.0, 10.0], [11.0, 12.0]]),\n",
    "        \"nested_list\": [[5, 6], [7, 8]],\n",
    "        \"value\": np.array([3.0, 4.0])\n",
    "    }\n",
    "]\n",
    "\n",
    "# Using the collate function\n",
    "collated_batch = collate(batch_list)\n",
    "\n",
    "# Display the collated result\n",
    "for key, value in collated_batch.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DataLoader\n",
    "\n",
    "The `build_dataloader` function is a utility for creating a PyTorch `DataLoader` with added support for distributed training. Here's a breakdown of what the function does:\n",
    "\n",
    "1. **Distributed Training Support**:\n",
    "   - The function first checks if distributed training is initialized using `dist.is_initialized()`, if distributed training is active, it retrieves the rank and world size of the current process using `dist.get_rank()` and `dist.get_world_size()`.\n",
    "   - It then creates a `DistributedSampler`, which ensures that each process gets a different subset of the dataset. This sampler is used to handle data loading in a distributed manner.\n",
    "   - If distributed training is not initialized, it defaults to using no sampler.\n",
    "\n",
    "2. **Creating the DataLoader**:\n",
    "   - The function creates a `DataLoader` using the provided dataset, batch size, number of workers, shuffle, and pin memory options.\n",
    "   - It uses the sampler if one was created; otherwise, it shuffles the data if `shuffle` is set to `True`.\n",
    "\n",
    "### Parameters Abstracted from PyTorch Direct Implementation\n",
    "\n",
    "The function abstracts away the following details from a direct PyTorch `DataLoader` implementation:\n",
    "- **DistributedSampler**: Automatically handles creating and using a `DistributedSampler` when distributed training is initialized.\n",
    "- **Sampler Management**: Abstracts the logic for deciding when to use a sampler and whether to shuffle the data.\n",
    "- **Collate Function**: Assumes a specific `collate_fn` (`collate`) is used, simplifying the `DataLoader` creation by not requiring the user to specify it.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Fixed Collate Function**: The function uses a predefined `collate_fn`. If a different collate function is needed, the user must manually modify the function.\n",
    "- **Limited Customization**: The function only exposes a subset of possible `DataLoader` parameters (batch size, number of workers, shuffle, and pin memory). For more advanced customization, the user might need to modify the function or revert to directly creating a `DataLoader`. PyTorch `DataLoader` supports advanced features such as `persistent_workers`, `worker_init_fn`, and `timeout`. The function does not expose these features, limiting its flexibility for more complex use cases.\n",
    "- **Distributed Training Dependency**: The function relies on PyTorch's distributed package (`torch.distributed`) to determine if distributed training is initialized. If used in a non-distributed context without the appropriate setup, the distributed checks and sampler creation might add unnecessary complexity.\n",
    "\n",
    "### Further Enhancements\n",
    "\n",
    "Some potential enhancements to the function include:\n",
    "\n",
    "- **Custom Collate Function**: Allow users to specify a custom `collate_fn` for more flexibility in data processing.\n",
    "- **Expose Advanced DataLoader Parameters**: Provide additional parameters for more advanced `DataLoader` configurations using **kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exports\n",
    "def build_dataloader(dataset, # Dataset object\n",
    "                     batch_size=4, # Batch size\n",
    "                     num_workers=8, # Number of workers\n",
    "                     shuffle:bool=False, # Shuffle the data\n",
    "                     pin_memory=False # Pin memory\n",
    "                     ): # A PyTorch DataLoader instance with the specified configuration.\n",
    "    \"\"\"This function is designed to build a DataLoader object for a given dataset with optional distributed training support.\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        rank = dist.get_rank()\n",
    "        world_size = dist.get_world_size()\n",
    "        sampler = DistributedSampler(\n",
    "            dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        shuffle=(sampler is None and shuffle),\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 303\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "train_dataset = pillarnext_dataset.NuScenesDataset(\"infos_train_10sweeps_withvelo_filterZero.pkl\",\n",
    "                                \"/root/nuscenes-dataset/v1.0-mini\",\n",
    "                                10,\n",
    "                                class_names=[[\"car\"], [\"truck\", \"construction_vehicle\"], [\"bus\", \"trailer\"], [\"barrier\"], [\"motorcycle\", \"bicycle\"], [\"pedestrian\", \"traffic_cone\"]],\n",
    "                                resampling=True)\n",
    "\n",
    "train_loader = build_dataloader(train_dataset)\n",
    "print(f\"Number of batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
