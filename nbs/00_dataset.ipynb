{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset\n",
    "\n",
    "> Module to load and preprocess the NuScenes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "import itertools\n",
    "import pickle\n",
    "import numba\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.eval.detection.config import config_factory\n",
    "from nuscenes.eval.detection.evaluate import NuScenesEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@numba.njit\n",
    "def points_in_boxes_jit(points: np.ndarray, # Float array [N, *]\n",
    "                        boxes: np.ndarray, # Float array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n",
    "                        indices: np.ndarray # Bool array of shape [N, M]\n",
    "                        ): # Bool array of shape [N, M]\n",
    "    \"\"\"This function determines if points are within a set of 3D boxes.\"\"\"\n",
    "    num_points = points.shape[0]\n",
    "    num_boxes = boxes.shape[0]\n",
    "    for j in range(num_boxes):\n",
    "        for i in range(num_points):\n",
    "            if np.abs(points[i, 2] - boxes[j, 2]) <= boxes[j, 5] / 2.0:\n",
    "                cosa = np.cos(boxes[j, -1])\n",
    "                sina = np.sin(boxes[j, -1])\n",
    "                shift_x = points[i, 0] - boxes[j, 0]\n",
    "                shift_y = points[i, 1] - boxes[j, 1]\n",
    "                local_x = shift_x * cosa + shift_y * sina\n",
    "                local_y = -shift_x * sina + shift_y * cosa\n",
    "                indices[i, j] = np.logical_and(np.abs(local_x) <= boxes[j, 3] / 2.0,\n",
    "                                               np.abs(local_y) <= boxes[j, 4] / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def points_in_rbbox(points: np.ndarray, # Float array [N, *]\n",
    "                    boxes: np.ndarray # Float array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n",
    "                    ): # Bool array of shape [N, M]\n",
    "    \"\"\"This function determines if points are within a set of rotated 3D boxes and returns a boolean array indicating the results.\"\"\"\n",
    "    indices = np.zeros((points.shape[0], boxes.shape[0]), dtype=bool) # Bool array of shape [N, M]\n",
    "    points_in_boxes_jit(points, boxes, indices)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Points:\n",
      " [[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]]\n",
      "\n",
      "Boxes:\n",
      " [[1.         1.         1.         2.         2.         2.\n",
      "  0.        ]\n",
      " [2.         2.         2.         2.         2.         2.\n",
      "  0.78539816]]\n",
      "\n",
      "Indices (Points in Boxes):\n",
      " [[ True False]\n",
      " [ True  True]\n",
      " [False False]]\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# Test Data\n",
    "points = np.array([\n",
    "    [1.0, 1.0, 1.0],\n",
    "    [2.0, 2.0, 2.0],\n",
    "    [3.0, 3.0, 3.0]\n",
    "])\n",
    "\n",
    "boxes = np.array([\n",
    "    [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0],  # Box centered at (1,1,1) with length=2, width=2, height=2, no rotation\n",
    "    [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, np.pi/4]  # Box centered at (2,2,2) with length=2, width=2, height=2, rotated 45 degrees\n",
    "])\n",
    "\n",
    "# Expected output: array of shape (3, 2)\n",
    "# For each point, we check if it is within each of the two boxes\n",
    "indices = points_in_rbbox(points, boxes)\n",
    "\n",
    "print(\"Points:\\n\", points)\n",
    "print(\"\\nBoxes:\\n\", boxes)\n",
    "print(\"\\nIndices (Points in Boxes):\\n\", indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseDataset(Dataset):\n",
    "    \"\"\"An abstract class representing a pytorch-like Dataset.\n",
    "    All other datasets should subclass it. All subclasses should override\n",
    "    ``__getitem__`` supporting integer indexing in range from 0 to len(self) exclusive.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            root_path, # Root path of the dataset\n",
    "            info_path, # Path to the info file\n",
    "            sampler=None, # Sampler for sampling data\n",
    "            loading_pipelines=None, # Loading pipelines\n",
    "            augmentation=None, # Augmentation pipelines\n",
    "            prepare_label=None, # Prepare label pipelines\n",
    "            evaluations=None, # Evaluation pipelines\n",
    "            create_database=False, # Whether to create database\n",
    "            use_gt_sampling=True # Whether to use ground truth sampling\n",
    "            ):\n",
    "\n",
    "        self._info_path = info_path\n",
    "        self._root_path = Path(root_path)\n",
    "        self.loading_pipelines = loading_pipelines\n",
    "        self.augmentations = augmentation\n",
    "        self.prepare_label = prepare_label\n",
    "        self.evaluations = evaluations\n",
    "        self.create_database = create_database\n",
    "        self.use_gt_sampling = use_gt_sampling\n",
    "        self.load_infos()\n",
    "        if use_gt_sampling and sampler is not None:\n",
    "            self.sampler = sampler()\n",
    "        else:\n",
    "            self.sampler = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.infos)\n",
    "\n",
    "    def load_infos(self):\n",
    "        with open(os.path.join(self._root_path, self._info_path), \"rb\") as f:\n",
    "            self.infos = pickle.load(f)\n",
    "\n",
    "    def evaluation(self):\n",
    "        \"\"\"Dataset must provide a evaluation function to evaluate model.\"\"\"\n",
    "        # support different evaluation tasks\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_pointcloud(self, res, info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def load_box3d(self, res, info):\n",
    "        res[\"annotations\"] = {\n",
    "            'gt_boxes': info[\"gt_boxes\"].astype(np.float32).copy(),\n",
    "            'gt_names': np.array(info[\"gt_names\"]).reshape(-1).copy(),\n",
    "        }\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.infos[idx]\n",
    "        res = {\"token\": info[\"token\"]}\n",
    "\n",
    "        if self.loading_pipelines is not None:\n",
    "            for lp in self.loading_pipelines:\n",
    "                res = getattr(self, lp)(res, info)\n",
    "        if self.sampler is not None:\n",
    "            sampled_dict = self.sampler.sample_all(\n",
    "                res['annotations']['gt_boxes'],\n",
    "                res[\"annotations\"]['gt_names']\n",
    "            )\n",
    "            if sampled_dict is not None:\n",
    "                sampled_gt_names = sampled_dict[\"gt_names\"]\n",
    "                sampled_gt_boxes = sampled_dict[\"gt_boxes\"]\n",
    "                sampled_points = sampled_dict[\"points\"]\n",
    "                sampled_gt_masks = sampled_dict[\"gt_masks\"]\n",
    "                res['annotations'][\"gt_names\"] = np.concatenate(\n",
    "                    [res['annotations'][\"gt_names\"], sampled_gt_names], axis=0\n",
    "                )\n",
    "                res['annotations'][\"gt_boxes\"] = np.concatenate(\n",
    "                    [res['annotations'][\"gt_boxes\"], sampled_gt_boxes]\n",
    "                )\n",
    "\n",
    "                # remove points in sampled gt boxes\n",
    "                sampled_point_indices = points_in_rbbox(\n",
    "                    res['points'], sampled_gt_boxes[sampled_gt_masks])\n",
    "                res['points'] = res['points'][np.logical_not(\n",
    "                    sampled_point_indices.any(-1))]\n",
    "\n",
    "                res['points'] = np.concatenate(\n",
    "                    [sampled_points, res['points']], axis=0)\n",
    "        if self.augmentations is not None:\n",
    "            for aug in self.augmentations.values():\n",
    "                res = aug(res)\n",
    "\n",
    "        if self.prepare_label is not None:\n",
    "            for _, pl in self.prepare_label.items():\n",
    "                res = pl(res)\n",
    "\n",
    "        if 'annotations' in res and (not self.create_database):\n",
    "            del res['annotations']\n",
    "\n",
    "        return res\n",
    "\n",
    "    def format_eval(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _second_det_to_nusc_box(detection):\n",
    "    \"\"\"\n",
    "    Convert a detection output from a second model to nuScenes box format.\n",
    "    \n",
    "    Args:\n",
    "        detection (dict): A dictionary containing detection outputs with keys:\n",
    "                          - \"box3d_lidar\": 3D bounding boxes in LiDAR coordinates.\n",
    "                          - \"scores\": Confidence scores of the detections.\n",
    "                          - \"label_preds\": Predicted labels for the detections.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of nuScenes Box objects.\n",
    "    \"\"\"\n",
    "    box3d = detection[\"box3d_lidar\"].detach().cpu().numpy()\n",
    "    scores = detection[\"scores\"].detach().cpu().numpy()\n",
    "    labels = detection[\"label_preds\"].detach().cpu().numpy()\n",
    "    box3d = box3d[:, [0, 1, 2, 4, 3, 5, 6, 7, 8]]\n",
    "    box_list = []\n",
    "    for i in range(box3d.shape[0]):\n",
    "        quat = Quaternion(axis=[0, 0, 1], radians=box3d[i, -1])\n",
    "        velocity = (*box3d[i, 6:8], 0.0)\n",
    "        box = Box(\n",
    "            box3d[i, :3],\n",
    "            box3d[i, 3:6],\n",
    "            quat,\n",
    "            label=labels[i],\n",
    "            score=scores[i],\n",
    "            velocity=velocity,\n",
    "        )\n",
    "        box_list.append(box)\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _lidar_nusc_box_to_global(nusc, boxes, sample_token):\n",
    "    \"\"\"\n",
    "    Transform nuScenes boxes from the LiDAR coordinate system to the global coordinate system.\n",
    "    \n",
    "    Args:\n",
    "        nusc (NuScenes): The nuScenes dataset object.\n",
    "        boxes (list): A list of nuScenes Box objects.\n",
    "        sample_token (str): The sample token for the current frame.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of transformed nuScenes Box objects in the global coordinate system.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s_record = nusc.get(\"sample\", sample_token)\n",
    "        sample_data_token = s_record[\"data\"][\"LIDAR_TOP\"]\n",
    "    except:\n",
    "        sample_data_token = sample_token\n",
    "\n",
    "    sd_record = nusc.get(\"sample_data\", sample_data_token)\n",
    "    cs_record = nusc.get(\"calibrated_sensor\",\n",
    "                         sd_record[\"calibrated_sensor_token\"])\n",
    "    pose_record = nusc.get(\"ego_pose\", sd_record[\"ego_pose_token\"])\n",
    "\n",
    "    box_list = []\n",
    "    for box in boxes:\n",
    "        # Move box to ego vehicle coord system\n",
    "        box.rotate(Quaternion(cs_record[\"rotation\"]))\n",
    "        box.translate(np.array(cs_record[\"translation\"]))\n",
    "        # Move box to global coord system\n",
    "        box.rotate(Quaternion(pose_record[\"rotation\"]))\n",
    "        box.translate(np.array(pose_record[\"translation\"]))\n",
    "        box_list.append(box)\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boxes from _second_det_to_nusc_box:\n",
      "{'center': array([1., 2., 3.], dtype=float32), 'wlh': array([2.5, 1.5, 1. ], dtype=float32), 'orientation': Quaternion(0.9238795283293805, 0.0, 0.0, 0.38268344246110436), 'label': 1, 'score': 0.8999999761581421, 'velocity': array([0.1, 0.2, 0. ]), 'name': None, 'token': None}\n",
      "\n",
      "Global Boxes from _lidar_nusc_box_to_global:\n",
      "{'center': array([1., 2., 3.]), 'wlh': array([2.5, 1.5, 1. ], dtype=float32), 'orientation': Quaternion(-0.9238795283293805, 0.0, 0.0, -0.38268344246110436), 'label': 1, 'score': 0.8999999761581421, 'velocity': array([0.1, 0.2, 0. ]), 'name': None, 'token': None}\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "# Mock detection input data\n",
    "detection = {\n",
    "    \"box3d_lidar\": torch.tensor([\n",
    "        [1, 2, 3, 1.5, 2.5, 1.0, 0.1, 0.2, np.pi/4]\n",
    "    ]),\n",
    "    \"scores\": torch.tensor([0.9]),\n",
    "    \"label_preds\": torch.tensor([1])\n",
    "}\n",
    "\n",
    "# Mock nusc object with necessary methods\n",
    "class MockNusc:\n",
    "    def get(self, table_name, token):\n",
    "        if table_name == \"sample\":\n",
    "            return {\n",
    "                \"data\": {\n",
    "                    \"LIDAR_TOP\": \"lidar_top_token\"\n",
    "                }\n",
    "            }\n",
    "        elif table_name == \"sample_data\":\n",
    "            return {\n",
    "                \"calibrated_sensor_token\": \"calibrated_sensor_token\",\n",
    "                \"ego_pose_token\": \"ego_pose_token\"\n",
    "            }\n",
    "        elif table_name == \"calibrated_sensor\":\n",
    "            return {\n",
    "                \"rotation\": [0, 0, 0, 1],\n",
    "                \"translation\": [0, 0, 0]\n",
    "            }\n",
    "        elif table_name == \"ego_pose\":\n",
    "            return {\n",
    "                \"rotation\": [0, 0, 0, 1],\n",
    "                \"translation\": [0, 0, 0]\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "nusc = MockNusc()\n",
    "\n",
    "# Test _second_det_to_nusc_box function\n",
    "boxes = _second_det_to_nusc_box(detection)\n",
    "print(\"Boxes from _second_det_to_nusc_box:\")\n",
    "for box in boxes:\n",
    "    print(vars(box))\n",
    "\n",
    "# Test _lidar_nusc_box_to_global function\n",
    "sample_token = \"sample_token\"\n",
    "global_boxes = _lidar_nusc_box_to_global(nusc, boxes, sample_token)\n",
    "print(\"\\nGlobal Boxes from _lidar_nusc_box_to_global:\")\n",
    "for box in global_boxes:\n",
    "    print(vars(box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Class attribute distribution\n",
    "cls_attr_dist = {\n",
    "    \"barrier\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 0,\n",
    "        \"vehicle.parked\": 0,\n",
    "        \"vehicle.stopped\": 0,\n",
    "    },\n",
    "    \"bicycle\": {\n",
    "        \"cycle.with_rider\": 2791,\n",
    "        \"cycle.without_rider\": 8946,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 0,\n",
    "        \"vehicle.parked\": 0,\n",
    "        \"vehicle.stopped\": 0,\n",
    "    },\n",
    "    \"bus\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 9092,\n",
    "        \"vehicle.parked\": 3294,\n",
    "        \"vehicle.stopped\": 3881,\n",
    "    },\n",
    "    \"car\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 114304,\n",
    "        \"vehicle.parked\": 330133,\n",
    "        \"vehicle.stopped\": 46898,\n",
    "    },\n",
    "    \"construction_vehicle\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 882,\n",
    "        \"vehicle.parked\": 11549,\n",
    "        \"vehicle.stopped\": 2102,\n",
    "    },\n",
    "    \"ignore\": {\n",
    "        \"cycle.with_rider\": 307,\n",
    "        \"cycle.without_rider\": 73,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 165,\n",
    "        \"vehicle.parked\": 400,\n",
    "        \"vehicle.stopped\": 102,\n",
    "    },\n",
    "    \"motorcycle\": {\n",
    "        \"cycle.with_rider\": 4233,\n",
    "        \"cycle.without_rider\": 8326,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 0,\n",
    "        \"vehicle.parked\": 0,\n",
    "        \"vehicle.stopped\": 0,\n",
    "    },\n",
    "    \"pedestrian\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 157444,\n",
    "        \"pedestrian.sitting_lying_down\": 13939,\n",
    "        \"pedestrian.standing\": 46530,\n",
    "        \"vehicle.moving\": 0,\n",
    "        \"vehicle.parked\": 0,\n",
    "        \"vehicle.stopped\": 0,\n",
    "    },\n",
    "    \"traffic_cone\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 0,\n",
    "        \"vehicle.parked\": 0,\n",
    "        \"vehicle.stopped\": 0,\n",
    "    },\n",
    "    \"trailer\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 3421,\n",
    "        \"vehicle.parked\": 19224,\n",
    "        \"vehicle.stopped\": 1895,\n",
    "    },\n",
    "    \"truck\": {\n",
    "        \"cycle.with_rider\": 0,\n",
    "        \"cycle.without_rider\": 0,\n",
    "        \"pedestrian.moving\": 0,\n",
    "        \"pedestrian.sitting_lying_down\": 0,\n",
    "        \"pedestrian.standing\": 0,\n",
    "        \"vehicle.moving\": 21339,\n",
    "        \"vehicle.parked\": 55626,\n",
    "        \"vehicle.stopped\": 11097,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def eval_main(nusc, # NuScenes dataset object.\n",
    "              eval_version, # Version of the evaluation configuration to use.\n",
    "              res_path, # Path to the results file.\n",
    "              eval_set, # The dataset split to evaluate on (e.g., 'val', 'test').\n",
    "              output_dir # Directory to store the evaluation results.\n",
    "              ):\n",
    "\n",
    "    cfg = config_factory(eval_version)\n",
    "\n",
    "    nusc_eval = NuScenesEval(\n",
    "        nusc,\n",
    "        config=cfg,\n",
    "        result_path=res_path,\n",
    "        eval_set=eval_set,\n",
    "        output_dir=output_dir,\n",
    "        verbose=True,\n",
    "    )\n",
    "    _ = nusc_eval.main(plot_examples=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class NuScenesDataset(BaseDataset): # NuScenes dataset class\n",
    "\n",
    "    def __init__(self,\n",
    "                 info_path,  # Path to dataset information file\n",
    "                 root_path,  # Path to root directory of dataset\n",
    "                 nsweeps,  # Number of sweeps (LiDAR frames) to use\n",
    "                 sampler=None,  # Sampler for dataset\n",
    "                 loading_pipelines=None,  # Loading pipelines for data processing\n",
    "                 augmentation=None,  # Data augmentation methods\n",
    "                 prepare_label=None,  # Method for preparing labels\n",
    "                 class_names=[],  # List of class names\n",
    "                 resampling=False,  # Whether to resample dataset\n",
    "                 evaluations=None,  # Evaluation methods\n",
    "                 create_database=False,  # Whether to create a database\n",
    "                 use_gt_sampling=True,  # Whether to use ground truth sampling\n",
    "                 version=\"v1.0-trainval\" # Dataset version\n",
    "                 ): # NuScenes dataset\n",
    "\n",
    "        super(NuScenesDataset, self).__init__(\n",
    "            root_path, info_path, sampler, loading_pipelines, augmentation, prepare_label, evaluations, create_database,\n",
    "            use_gt_sampling=use_gt_sampling)  # Initialize base class\n",
    "\n",
    "        self.nsweeps = nsweeps\n",
    "        assert self.nsweeps > 0, \"At least input one sweep please!\"  # Ensure at least one sweep is used\n",
    "\n",
    "        self._class_names = list(itertools.chain(*[t for t in class_names]))  # Flatten class names list\n",
    "        self.version = version\n",
    "\n",
    "        if resampling:\n",
    "            self.cbgs()  # Resample dataset if needed\n",
    "\n",
    "    def cbgs(self): # Performs class-balanced resampling on the dataset\n",
    "        _cls_infos = {name: [] for name in self._class_names}  # Initialize dictionary for class info\n",
    "        for info in self.infos:  # Iterate over dataset information\n",
    "            for name in set(info[\"gt_names\"]):  # For each unique ground truth name\n",
    "                if name in self._class_names:\n",
    "                    _cls_infos[name].append(info)  # Add info to corresponding class\n",
    "\n",
    "        duplicated_samples = sum([len(v) for _, v in _cls_infos.items()])  # Total number of samples after duplication\n",
    "        _cls_dist = {k: len(v) / duplicated_samples for k, v in _cls_infos.items()}  # Distribution of classes\n",
    "\n",
    "        _nusc_infos = []\n",
    "\n",
    "        frac = 1.0 / len(self._class_names)  # Fraction for resampling\n",
    "        ratios = [frac / v for v in _cls_dist.values()]  # Calculate resampling ratios\n",
    "\n",
    "        for cls_infos, ratio in zip(list(_cls_infos.values()), ratios):\n",
    "            _nusc_infos += np.random.choice(cls_infos, int(len(cls_infos) * ratio)).tolist()  # Resample and add to infos\n",
    "\n",
    "        self.infos = _nusc_infos  # Update dataset information\n",
    "\n",
    "    def read_file(self, path, num_point_feature=4): # Reads a point cloud file and returns the points in the specified format\n",
    "        points = np.fromfile(os.path.join(self._root_path, path),\n",
    "                             dtype=np.float32).reshape(-1, 5)[:, :num_point_feature]  # Read point cloud file and reshape\n",
    "        return points  # Return points of shape (N, num_point_feature)\n",
    "\n",
    "    def read_sweep(self, sweep, min_distance=1.0): # Reads a sweep file, applies transformations, removes points too close to the origin, and returns the points and their timestamps\n",
    "        points_sweep = self.read_file(str(sweep[\"lidar_path\"])).T  # Read sweep file and transpose, shape (num_point_feature, N)\n",
    "\n",
    "        nbr_points = points_sweep.shape[1]\n",
    "        if sweep[\"transform_matrix\"] is not None:\n",
    "            points_sweep[:3, :] = sweep[\"transform_matrix\"].dot(\n",
    "                np.vstack((points_sweep[:3, :], np.ones(nbr_points))))[:3, :]  # Apply transformation matrix\n",
    "        points_sweep = self.remove_close(points_sweep, min_distance)  # Remove points too close to the origin\n",
    "        curr_times = sweep[\"time_lag\"] * np.ones((1, points_sweep.shape[1]))  # Create current times array\n",
    "\n",
    "        return points_sweep.T, curr_times.T  # Return points and times of shape (N, num_point_feature), (N, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_close(points, radius: float): # Removes points that are too close to the origin\n",
    "        \"\"\"\n",
    "        Removes point too close within a certain radius from origin.\n",
    "        :param radius: Radius below which points are removed.\n",
    "        \"\"\"\n",
    "        x_filt = np.abs(points[0, :]) < radius\n",
    "        y_filt = np.abs(points[1, :]) < radius\n",
    "        not_close = np.logical_not(np.logical_and(x_filt, y_filt))  # Create filter for points outside the radius\n",
    "        points = points[:, not_close]  # Apply filter to points\n",
    "        return points  # Return filtered points\n",
    "\n",
    "    def load_pointcloud(self, res, info): # Loads a point cloud and its sweeps, concatenating them together with their timestamps\n",
    "\n",
    "        lidar_path = info[\"lidar_path\"]\n",
    "\n",
    "        points = self.read_file(str(lidar_path))  # Read point cloud file\n",
    "\n",
    "        sweep_points_list = [points]  # Initialize sweep points list\n",
    "        sweep_times_list = [np.zeros((points.shape[0], 1))]  # Initialize sweep times list\n",
    "\n",
    "        for i in range(len(info[\"sweeps\"])):  # Iterate over sweeps\n",
    "            sweep = info[\"sweeps\"][i]\n",
    "            points_sweep, times_sweep = self.read_sweep(sweep)  # Read each sweep\n",
    "            sweep_points_list.append(points_sweep)  # Add sweep points to list\n",
    "            sweep_times_list.append(times_sweep)  # Add sweep times to list\n",
    "\n",
    "        points = np.concatenate(sweep_points_list, axis=0)  # Concatenate all points\n",
    "        times = np.concatenate(sweep_times_list, axis=0).astype(points.dtype)  # Concatenate all times\n",
    "\n",
    "        res[\"points\"] = np.hstack([points, times])  # Combine points and times\n",
    "\n",
    "        return res  # Return updated result\n",
    "\n",
    "    def evaluation(self, detections, output_dir=None, testset=False): # Evaluates detections against the dataset, calculates metrics, and optionally performs resampling. It returns the results or None if the evaluation is not performed\n",
    "        version = self.version\n",
    "        eval_set_map = {\n",
    "            \"v1.0-mini\": \"mini_val\",\n",
    "            \"v1.0-trainval\": \"val\",\n",
    "            \"v1.0-test\": \"test\",\n",
    "        }\n",
    "\n",
    "        dets = [v for _, v in detections.items()]  # Get list of detections\n",
    "\n",
    "        nusc_annos = {\n",
    "            \"results\": {},\n",
    "            \"meta\": None,\n",
    "        }\n",
    "\n",
    "        nusc = NuScenes(version=version, dataroot=str(\n",
    "            self._root_path), verbose=True)  # Initialize NuScenes dataset\n",
    "\n",
    "        mapped_class_names = []\n",
    "        for n in self._class_names:\n",
    "            mapped_class_names.append(n)  # Map class names\n",
    "\n",
    "        for det in dets:  # Iterate over detections\n",
    "            annos = []\n",
    "            boxes = _second_det_to_nusc_box(det) # Convert detection to NuScenes box format\n",
    "            boxes = _lidar_nusc_box_to_global(nusc, boxes, det[\"token\"]) # Convert lidar boxes to global coordinates\n",
    "            for i, box in enumerate(boxes):\n",
    "                name = mapped_class_names[box.label]\n",
    "                if np.sqrt(box.velocity[0] ** 2 + box.velocity[1] ** 2) > 0.2:\n",
    "                    if name in [\n",
    "                        \"car\",\n",
    "                        \"construction_vehicle\",\n",
    "                        \"bus\",\n",
    "                        \"truck\",\n",
    "                        \"trailer\",\n",
    "                    ]:\n",
    "                        attr = \"vehicle.moving\"\n",
    "                    elif name in [\"bicycle\", \"motorcycle\"]:\n",
    "                        attr = \"cycle.with_rider\"\n",
    "                    else:\n",
    "                        attr = None\n",
    "                else:\n",
    "                    if name in [\"pedestrian\"]:\n",
    "                        attr = \"pedestrian.standing\"\n",
    "                    elif name in [\"bus\"]:\n",
    "                        attr = \"vehicle.parked\"\n",
    "                    else:\n",
    "                        attr = None\n",
    "\n",
    "                nusc_anno = {\n",
    "                    \"sample_token\": det[\"token\"],\n",
    "                    \"translation\": box.center.tolist(),  # Box center coordinates\n",
    "                    \"size\": box.wlh.tolist(),  # Box size (width, length, height)\n",
    "                    \"rotation\": box.orientation.elements.tolist(),  # Box rotation (quaternion)\n",
    "                    \"velocity\": box.velocity[:2].tolist(),  # Box velocity (x, y)\n",
    "                    \"detection_name\": name,  # Class name\n",
    "                    \"detection_score\": box.score,  # Detection score\n",
    "                    \"attribute_name\": attr\n",
    "                    if attr is not None\n",
    "                    else max(cls_attr_dist[name].items(), key=operator.itemgetter(1))[0],  # Attribute name\n",
    "                }\n",
    "                annos.append(nusc_anno)\n",
    "            nusc_annos[\"results\"].update({det[\"token\"]: annos})  # Add annotations to results\n",
    "\n",
    "        nusc_annos[\"meta\"] = {\n",
    "            \"use_camera\": False,\n",
    "            \"use_lidar\": True,\n",
    "            \"use_radar\": False,\n",
    "            \"use_map\": False,\n",
    "            \"use_external\": False,\n",
    "        }\n",
    "\n",
    "        name = self._info_path.split(\"/\")[-1].split(\".\")[0]\n",
    "        res_path = str(Path(output_dir) / Path(name + \".json\"))\n",
    "        with open(res_path, \"w\") as f:\n",
    "            json.dump(nusc_annos, f)  # Save annotations to JSON file\n",
    "\n",
    "        print(f\"Finish generate predictions for testset, save to {res_path}\")\n",
    "\n",
    "        if not testset:\n",
    "            eval_main(\n",
    "                nusc,\n",
    "                \"detection_cvpr_2019\",\n",
    "                res_path,\n",
    "                eval_set_map[self.version],\n",
    "                output_dir,\n",
    "            )  # Run evaluation\n",
    "\n",
    "            with open(Path(output_dir) / \"metrics_summary.json\", \"r\") as f:\n",
    "                metrics = json.load(f)  # Load evaluation metrics\n",
    "\n",
    "            detail = {}\n",
    "            result = f\"Nusc {version} Evaluation\\n\"\n",
    "            for name in mapped_class_names:  # Iterate over class names\n",
    "                detail[name] = {}\n",
    "                for k, v in metrics[\"label_aps\"][name].items():  # Iterate over evaluation metrics\n",
    "                    detail[name][f\"dist@{k}\"] = v\n",
    "                threshs = \", \".join(list(metrics[\"label_aps\"][name].keys()))  # Distance thresholds\n",
    "                scores = list(metrics[\"label_aps\"][name].values())  # Scores\n",
    "                mean = sum(scores) / len(scores)  # Mean score\n",
    "                scores = \", \".join([f\"{s * 100:.2f}\" for s in scores])  # Format scores\n",
    "                result += f\"{name} Nusc dist AP@{threshs}\\n\"\n",
    "                result += scores\n",
    "                result += f\" mean AP: {mean}\"\n",
    "                result += \"\\n\"\n",
    "            res_nusc = {\n",
    "                \"results\": {\"nusc\": result},\n",
    "                \"detail\": {\"nusc\": detail},\n",
    "            }\n",
    "        else:\n",
    "            res_nusc = None\n",
    "\n",
    "        if res_nusc is not None:\n",
    "            res = {\n",
    "                \"results\": {\"nusc\": res_nusc[\"results\"][\"nusc\"], },\n",
    "                \"detail\": {\"eval.nusc\": res_nusc[\"detail\"][\"nusc\"], },\n",
    "            }\n",
    "            return res['results']  # Return results\n",
    "        else:\n",
    "            return None  # Return None if no results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
