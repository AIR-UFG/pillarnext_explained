[
  {
    "objectID": "build_loader.html",
    "href": "build_loader.html",
    "title": "build loader",
    "section": "",
    "text": "source",
    "crumbs": [
      "build loader"
    ]
  },
  {
    "objectID": "build_loader.html#build-dataloader",
    "href": "build_loader.html#build-dataloader",
    "title": "build loader",
    "section": "Build DataLoader",
    "text": "Build DataLoader\nThe build_dataloader function is a utility for creating a PyTorch DataLoader with added support for distributed training. Here’s a breakdown of what the function does:\n\nDistributed Training Support:\n\nThe function first checks if distributed training is initialized using dist.is_initialized(), if distributed training is active, it retrieves the rank and world size of the current process using dist.get_rank() and dist.get_world_size().\nIt then creates a DistributedSampler, which ensures that each process gets a different subset of the dataset. This sampler is used to handle data loading in a distributed manner.\nIf distributed training is not initialized, it defaults to using no sampler.\n\nCreating the DataLoader:\n\nThe function creates a DataLoader using the provided dataset, batch size, number of workers, shuffle, and pin memory options.\nIt uses the sampler if one was created; otherwise, it shuffles the data if shuffle is set to True.\n\n\n\nParameters Abstracted from PyTorch Direct Implementation\nThe function abstracts away the following details from a direct PyTorch DataLoader implementation: - DistributedSampler: Automatically handles creating and using a DistributedSampler when distributed training is initialized. - Sampler Management: Abstracts the logic for deciding when to use a sampler and whether to shuffle the data. - Collate Function: Assumes a specific collate_fn (collate) is used, simplifying the DataLoader creation by not requiring the user to specify it.\n\n\nLimitations\n\nFixed Collate Function: The function uses a predefined collate_fn. If a different collate function is needed, the user must manually modify the function.\nLimited Customization: The function only exposes a subset of possible DataLoader parameters (batch size, number of workers, shuffle, and pin memory). For more advanced customization, the user might need to modify the function or revert to directly creating a DataLoader. PyTorch DataLoader supports advanced features such as persistent_workers, worker_init_fn, and timeout. The function does not expose these features, limiting its flexibility for more complex use cases.\nDistributed Training Dependency: The function relies on PyTorch’s distributed package (torch.distributed) to determine if distributed training is initialized. If used in a non-distributed context without the appropriate setup, the distributed checks and sampler creation might add unnecessary complexity.\n\n\n\nFurther Enhancements\nSome potential enhancements to the function include:\n\nCustom Collate Function: Allow users to specify a custom collate_fn for more flexibility in data processing.\nExpose Advanced DataLoader Parameters: Provide additional parameters for more advanced DataLoader configurations using **kwargs.\n\n\nsource\n\n\nbuild_dataloader\n\n build_dataloader (dataset, batch_size=4, num_workers=8,\n                   shuffle:bool=False, pin_memory=False)\n\nThis function is designed to build a DataLoader object for a given dataset with optional distributed training support.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndataset\n\n\nDataset object\n\n\nbatch_size\nint\n4\nBatch size\n\n\nnum_workers\nint\n8\nNumber of workers\n\n\nshuffle\nbool\nFalse\nShuffle the data\n\n\npin_memory\nbool\nFalse\nPin memory\n\n\n\n\n\nExported source\ndef build_dataloader(dataset, # Dataset object\n                     batch_size=4, # Batch size\n                     num_workers=8, # Number of workers\n                     shuffle:bool=False, # Shuffle the data\n                     pin_memory=False # Pin memory\n                     ): # A PyTorch DataLoader instance with the specified configuration.\n    \"\"\"This function is designed to build a DataLoader object for a given dataset with optional distributed training support.\"\"\"\n    if dist.is_initialized():\n        rank = dist.get_rank()\n        world_size = dist.get_world_size()\n        sampler = DistributedSampler(\n            dataset, num_replicas=world_size, rank=rank, shuffle=shuffle)\n    else:\n        sampler = None\n\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        shuffle=(sampler is None and shuffle),\n        num_workers=num_workers,\n        collate_fn=collate,\n        pin_memory=pin_memory,\n    )\n\n    return data_loader\n\n\n\ntrain_dataset = pillarnext_dataset.NuScenesDataset(\"infos_train_10sweeps_withvelo_filterZero.pkl\",\n                                \"/root/nuscenes-dataset/v1.0-mini\",\n                                10,\n                                class_names=[[\"car\"], [\"truck\", \"construction_vehicle\"], [\"bus\", \"trailer\"], [\"barrier\"], [\"motorcycle\", \"bicycle\"], [\"pedestrian\", \"traffic_cone\"]],\n                                resampling=True)\n\ntrain_loader = build_dataloader(train_dataset)\nprint(f\"Number of batches: {len(train_loader)}\")\n\nNumber of batches: 303",
    "crumbs": [
      "build loader"
    ]
  },
  {
    "objectID": "model_backbones.html",
    "href": "model_backbones.html",
    "title": "Model: backbones",
    "section": "",
    "text": "“A backbone performs further feature abstraction based on the preliminary features extracted by the grid encoder” (https://arxiv.org/abs/2305.04925).\nThe following classes form the backbone of the network in the object detection pipeline. The backbone is responsible for refining and abstracting features extracted by the initial grid or voxel encoder (model_readers), ensuring that the subsequent detection layers have rich, informative features to work with. Using sparse convolutions in these backbones is important, as LiDAR data is inherently sparse, with many regions containing no information.",
    "crumbs": [
      "Model: backbones"
    ]
  },
  {
    "objectID": "model_backbones.html#sparse-resnet",
    "href": "model_backbones.html#sparse-resnet",
    "title": "Model: backbones",
    "section": "Sparse ResNet",
    "text": "Sparse ResNet\n\nsource\n\nSparseResNet\n\n SparseResNet (layer_nums:list, ds_layer_strides:list,\n               ds_num_filters:list, num_input_features:int,\n               kernel_size:list=[3, 3, 3, 3], out_channels:int=256)\n\nSparseResNet is a neural network model built using sparse convolutions for processing sparse input data, like LiDAR point cloud processing.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlayer_nums\nlist\n\nNumber of blocks in each layer\n\n\nds_layer_strides\nlist\n\nStrides for each downsampling layer\n\n\nds_num_filters\nlist\n\nNumber of filters for each downsampling layer\n\n\nnum_input_features\nint\n\nNumber of input features\n\n\nkernel_size\nlist\n[3, 3, 3, 3]\nKernel sizes for each layer\n\n\nout_channels\nint\n256\nNumber of output channels\n\n\n\n\n\nExported source\nclass SparseResNet(spconv.pytorch.SparseModule):\n    \"\"\"\n    SparseResNet is a neural network model built using sparse convolutions for processing sparse input data, like LiDAR point cloud processing.\n    \"\"\"\n    def __init__(\n            self,\n            layer_nums: list, # Number of blocks in each layer\n            ds_layer_strides: list, # Strides for each downsampling layer\n            ds_num_filters: list, # Number of filters for each downsampling layer\n            num_input_features: int, # Number of input features\n            kernel_size: list = [3, 3, 3, 3], # Kernel sizes for each layer\n            out_channels: int = 256 # Number of output channels\n            ):\n\n        super(SparseResNet, self).__init__()  # Call the constructor of the parent class\n        self._layer_strides = ds_layer_strides  # Store the layer strides\n        self._num_filters = ds_num_filters  # Store the number of filters for each layer\n        self._layer_nums = layer_nums  # Store the number of blocks in each layer\n        self._num_input_features = num_input_features  # Store the number of input features\n\n        # Ensure the lengths of the strides, filters, and layer numbers are consistent\n        assert len(self._layer_strides) == len(self._layer_nums)\n        assert len(self._num_filters) == len(self._layer_nums)\n\n        # Define the number of input filters for each layer\n        in_filters = [self._num_input_features, *self._num_filters[:-1]]\n        blocks = []  # Initialize the list to hold all the blocks\n\n        # Create the layers for the network\n        for i, layer_num in enumerate(self._layer_nums):\n            block = self._make_layer(\n                in_filters[i],\n                self._num_filters[i],\n                kernel_size[i],\n                self._layer_strides[i],\n                layer_num)\n            blocks.append(block)  # Add the created block to the blocks list\n\n        # Convert blocks list to a PyTorch ModuleList for proper handling in forward pass\n        self.blocks = nn.ModuleList(blocks)\n\n        # Create the final mapping layer\n        self.mapping = spconv.pytorch.SparseSequential(\n            spconv.SparseConv2d(self._num_filters[-1],\n                                out_channels, 1, 1, bias=False),  # 1x1 convolution\n            nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01),  # Batch normalization\n            nn.ReLU(),  # Activation function\n        )\n\n    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n        \"\"\"\n        Helper function to create a layer consisting of several blocks.\n        \"\"\"\n        layers = []\n        # Add the first convolutional block with stride (downsampling)\n        layers.append(SparseConvBlock(inplanes, planes,\n                      kernel_size=kernel_size, stride=stride, use_subm=False))\n\n        # Add subsequent blocks without stride\n        for j in range(num_blocks):\n            layers.append(SparseBasicBlock(planes, kernel_size=kernel_size))\n\n        # Return the layers as a SparseSequential module\n        return spconv.pytorch.SparseSequential(*layers)\n\n    def forward(self, pillar_features, coors, input_shape):\n        \"\"\"\n        Forward pass of the network.\n        \"\"\"\n        # Determine batch size from the unique coordinates\n        batch_size = len(torch.unique(coors[:, 0]))\n        # Create a SparseConvTensor from the input features, coordinates, and shape\n        x = spconv.pytorch.SparseConvTensor(\n            pillar_features, coors, input_shape, batch_size)\n        \n        # Pass the tensor through each block sequentially\n        for i in range(len(self.blocks)):\n            x = self.blocks[i](x)\n        \n        # Apply the final mapping\n        x = self.mapping(x)\n        # Convert the sparse tensor to a dense tensor and return\n        return x.dense()\n\n\nThe SparseResNet class extends from spconv.pytorch.SparseModule, which is part of the spconv library. This library provides specialized operations for sparse data, particularly useful in scenarios where the input data is not densely populated, as is the case with LiDAR point clouds.\n\nLayer Construction:\n\nThe class constructs layers using a helper function _make_layer, which creates blocks consisting of sparse convolutional layers.\nThe blocks attribute is a ModuleList of all the layers in the network, enabling sequential processing in the forward pass.\n\nMapping Layer:\n\nAfter passing through all the blocks, the data goes through a final mapping layer that performs a 1x1 sparse convolution, batch normalization, and ReLU activation.\n\nLayer Construction (_make_layer Method):\n\nThis method creates a layer with multiple blocks. The first block includes downsampling (through stride), and the subsequent blocks are added without additional downsampling.\n\nForward Pass (forward Method):\n\nThe forward pass involves creating a SparseConvTensor from the input features, coordinates, and input shape.\nThe tensor is passed through each block sequentially.\nFinally, the sparse tensor is mapped to the output channels and converted to a dense tensor before being returned.",
    "crumbs": [
      "Model: backbones"
    ]
  },
  {
    "objectID": "model_backbones.html#sparse-resnet-3d",
    "href": "model_backbones.html#sparse-resnet-3d",
    "title": "Model: backbones",
    "section": "Sparse ResNet 3D",
    "text": "Sparse ResNet 3D\n\nsource\n\nSparseResNet3D\n\n SparseResNet3D (layer_nums:list, ds_layer_strides:list,\n                 ds_num_filters:list, num_input_features:int,\n                 kernel_size:list=[3, 3, 3, 3], out_channels:int=128)\n\nSparseResNet3D is a 3D variant of the SparseResNet model, designed for processing 3D sparse input data.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nlayer_nums\nlist\n\nNumber of blocks in each layer\n\n\nds_layer_strides\nlist\n\nStrides for each downsampling layer\n\n\nds_num_filters\nlist\n\nNumber of filters for each downsampling layer\n\n\nnum_input_features\nint\n\nNumber of input features\n\n\nkernel_size\nlist\n[3, 3, 3, 3]\nKernel sizes for each layer\n\n\nout_channels\nint\n128\nNumber of output channels\n\n\n\n\n\nExported source\nclass SparseResNet3D(spconv.pytorch.SparseModule):\n    \"\"\"\n    SparseResNet3D is a 3D variant of the SparseResNet model, designed for processing 3D sparse input data.\n    \"\"\"\n    def __init__(\n        self,\n        layer_nums: list,  # Number of blocks in each layer\n        ds_layer_strides: list,  # Strides for each downsampling layer\n        ds_num_filters: list,  # Number of filters for each downsampling layer\n        num_input_features: int,  # Number of input features\n        kernel_size: list = [3, 3, 3, 3],  # Kernel sizes for each layer\n        out_channels: int = 128  # Number of output channels\n    ):\n        super(SparseResNet3D, self).__init__()\n        \n        # Initialize the instance variables\n        self._layer_strides = ds_layer_strides\n        self._num_filters = ds_num_filters\n        self._layer_nums = layer_nums\n        self._num_input_features = num_input_features\n\n        # Ensure the lengths of the lists match\n        assert len(self._layer_strides) == len(self._layer_nums)\n        assert len(self._num_filters) == len(self._layer_nums)\n\n        # List of input filters for each layer\n        in_filters = [self._num_input_features, *self._num_filters[:-1]]\n        blocks = []\n\n        # Create layers based on the given parameters\n        for i, layer_num in enumerate(self._layer_nums):\n            block = self._make_layer(\n                in_filters[i],\n                self._num_filters[i],\n                kernel_size[i],\n                self._layer_strides[i],\n                layer_num)\n            blocks.append(block)\n\n        # Store the blocks in a ModuleList\n        self.blocks = nn.ModuleList(blocks)\n        \n        # Define the mapping layer\n        self.mapping = SparseConv3dBlock(\n            self._num_filters[-1], out_channels, kernel_size=1, stride=1, use_subm=True)\n        \n        # Define the extra convolutional layer\n        self.extra_conv = SparseSequential(\n            SparseConv3d(\n                self._num_filters[-1], self._num_filters[-1], (3, 1, 1), (2, 1, 1), bias=False),\n            nn.BatchNorm1d(self._num_filters[-1], eps=1e-3, momentum=0.01),\n            nn.ReLU(),\n        )\n\n    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n        \"\"\"\n        Creates a single layer composed of sparse 3D convolution blocks.\n        \"\"\"\n\n        layers = []\n        \n        # Add the first convolution block with downsampling\n        layers.append(SparseConv3dBlock(inplanes, planes,\n                      kernel_size=kernel_size, stride=stride, use_subm=False))\n\n        # Add the remaining blocks without downsampling\n        for _ in range(num_blocks):\n            layers.append(SparseBasicBlock3d(planes, kernel_size=kernel_size))\n\n        return spconv.pytorch.SparseSequential(*layers)\n\n    def forward(self, pillar_features, coors, input_shape):\n        # Get the batch size from the unique coordinates\n        batch_size = len(torch.unique(coors[:, 0]))\n        \n        # Create a sparse tensor\n        x = spconv.pytorch.SparseConvTensor(\n            pillar_features, coors, input_shape, batch_size)\n        \n        # Pass the tensor through the blocks\n        for i in range(len(self.blocks)):\n            x = self.blocks[i](x)\n        \n        # Apply the extra convolution and mapping layers\n        x = self.extra_conv(x)\n        x = self.mapping(x)\n        \n        # Convert the sparse tensor to a dense format\n        x = x.dense()\n        \n        # Reshape the output tensor\n        B, C, D, H, W = x.shape\n        x = x.view(B, C * D, H, W)\n        \n        return x\n\n\nThe SparseResNet3D class extends the functionality of SparseResNet to 3D data, making it suitable for voxel-based encoders that operate directly on 3D grids of LiDAR data.\nImplementation Details:\n\n3D Convolutional Layers: Like the 2D variant, SparseResNet3D uses multiple layers composed of sparse 3D convolutional blocks. The blocks are similar in structure to the 2D version but adapted to 3D operations.\nExtra Convolutional Layer: An additional convolutional layer is added before the final mapping to further enhance the feature representation.\nForward Pass: Similar to SparseResNet, the input features are processed through each block and additional layers, with the output converted from sparse to dense format. The final tensor is reshaped to accommodate the 3D nature of the input.",
    "crumbs": [
      "Model: backbones"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pillarnext_explained",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "pillarnext_explained"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "pillarnext_explained",
    "section": "Install",
    "text": "Install\npip install pillarnext_explained",
    "crumbs": [
      "pillarnext_explained"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "pillarnext_explained",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "pillarnext_explained"
    ]
  },
  {
    "objectID": "model_utils.html",
    "href": "model_utils.html",
    "title": "Model: utils",
    "section": "",
    "text": "These classes define various convolutional blocks for both dense (regular) and sparse convolutional neural networks (CNNs), abstracting some of the complexities and repetitive code that is often encountered when building such networks directly using PyTorch. Below is an explanation of what these classes are doing, their differences from standard PyTorch implementations, and their limitations.\n\nModule differences and limitations\n\nDifferences from PyTorch Direct Implementation\n\nAbstraction: These classes encapsulate common patterns (convolution + normalization + activation) into single modules, reducing repetitive code and making the network definitions more concise and easier to read.\nConfiguration: They provide a higher-level interface for configuring layers, automatically setting common parameters such as padding.\nSparse Convolution Support: The sparse convolution blocks use the spconv library, which is not part of standard PyTorch, to handle sparse input data more efficiently.\n\n\n\nParameters Abstracted from PyTorch Direct Implementation\n\nPadding Calculation: Automatically calculates padding based on the kernel size if not provided.\nLayer Initialization: Automatically initializes convolutional, normalization, and activation layers within the block, so users don’t need to explicitly define each component.\nResidual Connections: For the basic blocks, the residual connections (identity mappings) are integrated within the block, simplifying the addition of these connections.\n\n\n\nLimitations\n\nFlexibility: While these classes simplify the creation of common patterns, they can be less flexible than directly using PyTorch when non-standard configurations or additional customizations are required.\nDependency on spconv: The sparse convolution blocks depend on the spconv library, which might not be as widely used or supported as PyTorch’s native functionality.\nDebugging: Abstracting layers into higher-level blocks can make debugging more difficult, as the internal operations are hidden away. Users may need to dig into the class implementations to troubleshoot issues.\nPerformance Overhead: Although the abstraction can simplify code, it might introduce slight performance overhead due to additional function calls and encapsulation.\n\nOverall, these classes provide a convenient and structured way to build CNNs, particularly when using common patterns and when working with sparse data. However, for highly customized or performance-critical applications, a more direct approach using PyTorch’s lower-level APIs might be preferable.\n\nsource\n\n\n\nConv\n\n Conv (inplanes:int, planes:int, kernel_size:int, stride:int,\n       conv_layer:torch.nn.modules.module.Module=&lt;class\n       'torch.nn.modules.conv.Conv2d'&gt;, bias:bool=False, **kwargs)\n\n*A convolutional layer module for neural networks.\nThis class is a wrapper around the specified convolutional layer type, providing a convenient way to include convolutional layers in neural networks with customizable parameters such as input channels, output channels, kernel size, stride, and padding.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninplanes\nint\n\nThe number of input channels.\n\n\nplanes\nint\n\nThe number of output channels.\n\n\nkernel_size\nint\n\nThe size of the convolving kernel.\n\n\nstride\nint\n\nThe stride of the convolution.\n\n\nconv_layer\nModule\nConv2d\nThe convolutional layer class to be used.\n\n\nbias\nbool\nFalse\nIf True, adds a learnable bias to the output.\n\n\nkwargs\n\n\n\n\n\n\n\n\nExported source\nclass Conv(nn.Module):\n    \"\"\"\n    A convolutional layer module for neural networks.\n\n    This class is a wrapper around the specified convolutional layer type, \n    providing a convenient way to include convolutional layers in neural networks \n    with customizable parameters such as input channels, output channels, kernel size, \n    stride, and padding.\n    \"\"\"\n    def __init__(self,\n                 inplanes:int, # The number of input channels.\n                 planes:int, # The number of output channels.\n                 kernel_size:int, # The size of the convolving kernel.\n                 stride:int, # The stride of the convolution.\n                 conv_layer:nn.Module=nn.Conv2d, # The convolutional layer class to be used.\n                 bias:bool=False, # If `True`, adds a learnable bias to the output.\n                 **kwargs # Arbitrary keyword arguments. Currently supports 'padding'.\n                 ):\n        super(Conv, self).__init__()\n        padding = kwargs.get('padding', kernel_size // 2)  # dafault same size\n\n        self.conv = conv_layer(inplanes, planes, kernel_size=kernel_size, stride=stride,\n                               padding=padding, bias=bias)\n                        \n    def forward(self, x):\n        return self.conv(x)\n\n\n\n# Define input tensor with shape (batch_size, in_channels, height, width)\ninput_tensor = torch.randn(1, 3, 64, 64)  # Example with batch_size=1, in_channels=3, height=64, width=64\n\n# Create an instance of the Conv class\nconv_layer = Conv(inplanes=3, planes=16, kernel_size=3, stride=1)\n\n# Pass the input tensor through the convolutional layer\noutput_tensor = conv_layer(input_tensor)\n\n# Print the shape of the output tensor\nprint(\"Output tensor shape:\", output_tensor.shape)\n\nOutput tensor shape: torch.Size([1, 16, 64, 64])\n\n\n\nsource\n\n\nConvBlock\n\n ConvBlock (inplanes:int, planes:int, kernel_size:int, stride:int=1,\n            conv_layer:torch.nn.modules.module.Module=&lt;class\n            'torch.nn.modules.conv.Conv2d'&gt;,\n            norm_layer:torch.nn.modules.module.Module=&lt;class\n            'torch.nn.modules.batchnorm.BatchNorm2d'&gt;,\n            act_layer:torch.nn.modules.module.Module=&lt;class\n            'torch.nn.modules.activation.ReLU'&gt;, **kwargs)\n\n*A convolutional block module combining a convolutional layer, a normalization layer, and an activation layer.\nThis class encapsulates a common pattern found in neural networks, where a convolution is followed by batch normalization and a non-linear activation function. It provides a convenient way to stack these operations into a single module.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninplanes\nint\n\nThe number of input channels.\n\n\nplanes\nint\n\nThe number of output channels.\n\n\nkernel_size\nint\n\nThe size of the convolving kernel.\n\n\nstride\nint\n1\nThe stride of the convolution.\n\n\nconv_layer\nModule\nConv2d\nThe convolutional layer class to be used.\n\n\nnorm_layer\nModule\nBatchNorm2d\nThe normalization layer class to be used.\n\n\nact_layer\nModule\nReLU\nThe activation function class to be used.\n\n\nkwargs\n\n\n\n\n\n\n\n\nExported source\nclass ConvBlock(nn.Module):\n    \"\"\"\n    A convolutional block module combining a convolutional layer, a normalization layer, \n    and an activation layer.\n\n    This class encapsulates a common pattern found in neural networks, where a convolution \n    is followed by batch normalization and a non-linear activation function. It provides \n    a convenient way to stack these operations into a single module.\n    \"\"\"\n    def __init__(self,\n                 inplanes: int, # The number of input channels.\n                 planes: int, # The number of output channels.\n                 kernel_size: int, # The size of the convolving kernel.\n                 stride:int=1, # The stride of the convolution.\n                 conv_layer:nn.Module=nn.Conv2d, # The convolutional layer class to be used.\n                 norm_layer:nn.Module=nn.BatchNorm2d, # The normalization layer class to be used.\n                 act_layer:nn.Module=nn.ReLU, # The activation function class to be used.\n                 **kwargs # Arbitrary keyword arguments. Currently supports 'padding'.\n                 ):\n        super(ConvBlock, self).__init__()\n        padding = kwargs.get('padding', kernel_size // 2)  # dafault same size\n\n        self.conv = Conv(inplanes, planes, kernel_size=kernel_size, stride=stride,\n                               padding=padding, bias=False, conv_layer=conv_layer)\n\n        self.norm = norm_layer(planes)\n        self.act = act_layer()\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.norm(out)\n        out = self.act(out)\n        return out\n\n\n\n# Define an instance of the ConvBlock\nconv_block = ConvBlock(inplanes=3, planes=16, kernel_size=3, stride=1)\n\n# Create a dummy input tensor with shape (batch_size, channels, height, width)\ndummy_input = torch.randn(1, 3, 64, 64)  # Example: batch size of 1, 3 input channels, 64x64 image\n\n# Pass the dummy input through the ConvBlock\noutput = conv_block(dummy_input)\n\n# Print the shape of the output tensor\nprint(\"Output shape:\", output.shape)\n\nOutput shape: torch.Size([1, 16, 64, 64])\n\n\n\nsource\n\n\nBasicBlock\n\n BasicBlock (inplanes:int, kernel_size:int=3)\n\n*A basic residual block module for neural networks.\nThis class implements a basic version of the residual block, consisting of two convolutional blocks followed by an addition operation with the input (identity) and an activation function. It is a fundamental component in ResNet architectures, allowing for the training of very deep networks by addressing the vanishing gradient problem.*\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninplanes\nint\n\nNumber of input channels\n\n\nkernel_size\nint\n3\nSize of the convolving kernel\n\n\n\n\n\nExported source\nclass BasicBlock(nn.Module):\n    \"\"\"\n    A basic residual block module for neural networks.\n\n    This class implements a basic version of the residual block, consisting of two convolutional \n    blocks followed by an addition operation with the input (identity) and an activation function. \n    It is a fundamental component in ResNet architectures, allowing for the training of very deep \n    networks by addressing the vanishing gradient problem.\n    \"\"\"\n\n    def __init__(self,\n                 inplanes:int, # Number of input channels\n                 kernel_size:int=3 # Size of the convolving kernel\n                 ):\n        super(BasicBlock, self).__init__()\n        self.block1 = ConvBlock(inplanes, inplanes, kernel_size=kernel_size)\n        self.block2 = ConvBlock(inplanes, inplanes, kernel_size=kernel_size)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.block1(x)\n        out = self.block2(out)\n        out += identity  # Element-wise addition with the input tensor\n        out = self.act(out)  # Apply activation function\n\n        return out\n\n\n\n# Instantiate the BasicBlock\nbasic_block = BasicBlock(64)\n\n# Print the structure of the basic_block to understand its components\nprint(basic_block)\n\n# Create a random tensor with shape (batch_size, channels, height, width)\n# Let's assume a batch size of 1, with 64 channels, and spatial dimensions 32x32\ninput_tensor = torch.randn(1, 64, 32, 32)\n\n# Pass the input tensor through the BasicBlock\noutput_tensor = basic_block(input_tensor)\n\n# Print the shape of the output tensor\nprint(\"Output shape:\", output_tensor.shape)\n\nBasicBlock(\n  (block1): ConvBlock(\n    (conv): Conv(\n      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    )\n    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): ReLU()\n  )\n  (block2): ConvBlock(\n    (conv): Conv(\n      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    )\n    (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act): ReLU()\n  )\n  (act): ReLU()\n)\nOutput shape: torch.Size([1, 64, 32, 32])\n\n\n\nsource\n\n\nSparseConvBlock\n\n SparseConvBlock (in_channels:int, out_channels:int, kernel_size:int,\n                  stride, use_subm:bool=True, bias:bool=False)\n\n*Initializes a sparse convolutional block for 2D inputs.\nThis block uses SparseConv2d for strides greater than 1 and SubMConv2d for stride equal to 1. It includes a normalization and activation layer following the convolution.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of channels in the input tensor.\n\n\nout_channels\nint\n\nNumber of channels produced by the convolution.\n\n\nkernel_size\nint\n\nSize of the convolving kernel.\n\n\nstride\n\n\nStride of the convolution.\n\n\nuse_subm\nbool\nTrue\nWhether to use SubMConv2d for stride 1.\n\n\nbias\nbool\nFalse\nIf True, adds a learnable bias to the output.\n\n\n\n\n\nExported source\nclass SparseConvBlock(spconv.pytorch.SparseModule):\n    '''\n    Initializes a sparse convolutional block for 2D inputs.\n\n    This block uses SparseConv2d for strides greater than 1 and SubMConv2d for stride equal to 1.\n    It includes a normalization and activation layer following the convolution.\n    '''\n\n    def __init__(self,\n                 in_channels: int, # Number of channels in the input tensor.\n                 out_channels: int, # Number of channels produced by the convolution.\n                 kernel_size: int, # Size of the convolving kernel.\n                 stride, # Stride of the convolution.\n                 use_subm:bool=True, # Whether to use SubMConv2d for stride 1.\n                 bias:bool=False # If True, adds a learnable bias to the output.\n                 ):\n        super(SparseConvBlock, self).__init__()\n        if stride == 1 and use_subm:\n            self.conv = spconv.pytorch.SubMConv2d(in_channels, out_channels, kernel_size,\n                                                  padding=kernel_size//2, stride=1, bias=bias,)\n        else:\n            self.conv = spconv.pytorch.SparseConv2d(in_channels, out_channels, kernel_size,\n                                                    padding=kernel_size//2, stride=stride, bias=bias)\n\n        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = replace_feature(out, self.norm(out.features))\n        out = replace_feature(out, self.act(out.features))\n\n        return out\n\n\n\n# Example usage\ninput_tensor = spconv.pytorch.SparseConvTensor(features=torch.randn(5, 3).to(DEVICE),\n                                               indices=torch.randint(0, 10, (5, 3), dtype=torch.int32).to(DEVICE),\n                                               spatial_shape=[10, 10],\n                                               batch_size=1)\nconv_block = SparseConvBlock(3, 16, 3, 1).to(DEVICE)\noutput_tensor = conv_block(input_tensor)\nprint(output_tensor)\n\nSparseConvTensor[shape=torch.Size([5, 16])]\n\n\n\nsource\n\n\nSparseBasicBlock\n\n SparseBasicBlock (channels:int, kernel_size)\n\n*A basic block for sparse convolutional networks, specifically designed for 2D inputs.\nThis block consists of two convolutional layers, each followed by normalization and activation. The output of the second convolutional layer is added to the input feature map (residual connection) before applying the final activation function.*\n\n\n\n\nType\nDetails\n\n\n\n\nchannels\nint\nNumber of channels in the input tensor.\n\n\nkernel_size\n\nSize of the convolving kernel.\n\n\n\n\n\nExported source\nclass SparseBasicBlock(spconv.pytorch.SparseModule):\n    '''\n    A basic block for sparse convolutional networks, specifically designed for 2D inputs.\n\n    This block consists of two convolutional layers, each followed by normalization and activation.\n    The output of the second convolutional layer is added to the input feature map (residual connection)\n    before applying the final activation function.\n    '''\n\n    def __init__(self,\n                 channels:int, # Number of channels in the input tensor.\n                 kernel_size # Size of the convolving kernel.\n                 ):\n        super(SparseBasicBlock, self).__init__()\n        self.block1 = SparseConvBlock(channels, channels, kernel_size, 1)\n        self.conv2 = spconv.pytorch.SubMConv2d(channels, channels, kernel_size, padding=kernel_size//2,\n                                               stride=1, bias=False, algo=ConvAlgo.Native, )\n        self.norm2 = nn.BatchNorm1d(channels, eps=1e-3, momentum=0.01)\n        self.act2 = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.block1(x)\n        out = self.conv2(out)\n        out = replace_feature(out, self.norm2(out.features))\n        out = replace_feature(out, out.features + identity.features)\n        out = replace_feature(out, self.act2(out.features))\n\n        return out\n\n\n\n# Example usage\ninput_tensor = spconv.pytorch.SparseConvTensor(features=torch.randn(5, 3).to(DEVICE),\n                                               indices=torch.randint(0, 10, (5, 3), dtype=torch.int32).to(DEVICE),\n                                               spatial_shape=[10, 10],\n                                               batch_size=1)\nbasic_block = SparseBasicBlock(3, 3).to(DEVICE)\noutput_tensor = basic_block(input_tensor)\nprint(output_tensor)\n\nSparseConvTensor[shape=torch.Size([5, 3])]\n\n\n\nsource\n\n\nSparseConv3dBlock\n\n SparseConv3dBlock (in_channels:int, out_channels:int, kernel_size,\n                    stride, use_subm:bool=True)\n\n*Initializes a sparse convolutional block for 3D inputs.\nThis block uses SparseConv3d for strides greater than 1 and SubMConv3d for stride equal to 1. It includes a normalization and activation layer following the convolution.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of channels in the input tensor.\n\n\nout_channels\nint\n\nNumber of channels produced by the convolution.\n\n\nkernel_size\n\n\nSize of the convolving kernel.\n\n\nstride\n\n\nStride of the convolution.\n\n\nuse_subm\nbool\nTrue\nWhether to use SubMConv3d for stride 1.\n\n\n\n\n\nExported source\nclass SparseConv3dBlock(spconv.pytorch.SparseModule):\n    '''\n    Initializes a sparse convolutional block for 3D inputs.\n\n    This block uses SparseConv3d for strides greater than 1 and SubMConv3d for stride equal to 1.\n    It includes a normalization and activation layer following the convolution.\n    '''\n    def __init__(self,\n                in_channels: int, # Number of channels in the input tensor.\n                out_channels: int, # Number of channels produced by the convolution.\n                kernel_size, # Size of the convolving kernel.\n                stride, # Stride of the convolution.\n                use_subm:bool=True # Whether to use SubMConv3d for stride 1.\n                ):\n        super(SparseConv3dBlock, self).__init__()\n        if stride == 1 and use_subm:\n            self.conv = spconv.pytorch.SubMConv3d(in_channels, out_channels, kernel_size, padding=kernel_size//2,\n                                                  stride=1, bias=False)\n        else:\n            self.conv = spconv.pytorch.SparseConv3d(in_channels, out_channels, kernel_size, padding=kernel_size//2,\n                                                    stride=stride, bias=False)\n\n        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n        self.act = nn.ReLU()\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = replace_feature(out, self.norm(out.features))\n        out = replace_feature(out, self.act(out.features))\n\n        return out\n\n\n\n# Example usage\ninput_tensor = spconv.pytorch.SparseConvTensor(features=torch.randn(5, 3).to(DEVICE),\n                                               indices=torch.randint(0, 10, (5, 4), dtype=torch.int32).to(DEVICE),\n                                               spatial_shape=[10, 10, 10],\n                                               batch_size=1)\nconv3d_block = SparseConv3dBlock(3, 16, 3, 1).to(DEVICE)\noutput_tensor = conv3d_block(input_tensor)\nprint(output_tensor)\n\nSparseConvTensor[shape=torch.Size([5, 16])]\n\n\n\nsource\n\n\nSparseBasicBlock3d\n\n SparseBasicBlock3d (channels:int, kernel_size)\n\n*A basic block for sparse convolutional networks, specifically designed for 3D inputs.\nThis block consists of two convolutional layers, each followed by normalization and activation. The output of the second convolutional layer is added to the input feature map (residual connection) before applying the final activation function.*\n\n\n\n\nType\nDetails\n\n\n\n\nchannels\nint\nNumber of channels in the input tensor.\n\n\nkernel_size\n\nSize of the convolving kernel.\n\n\n\n\n\nExported source\nclass SparseBasicBlock3d(spconv.pytorch.SparseModule):\n    '''\n    A basic block for sparse convolutional networks, specifically designed for 3D inputs.\n\n    This block consists of two convolutional layers, each followed by normalization and activation.\n    The output of the second convolutional layer is added to the input feature map (residual connection)\n    before applying the final activation function.\n    '''\n    def __init__(self,\n                 channels:int, # Number of channels in the input tensor.\n                 kernel_size # Size of the convolving kernel.\n                 ):\n        super(SparseBasicBlock3d, self).__init__()\n        self.block1 = SparseConv3dBlock(channels, channels, kernel_size, 1)\n        self.conv2 = spconv.pytorch.SubMConv3d(channels, channels, kernel_size, padding=kernel_size//2,\n                                               stride=1, bias=False)\n        self.norm2 = nn.BatchNorm1d(channels, eps=1e-3, momentum=0.01)\n        self.act2 = nn.ReLU()\n\n    def forward(self, x):\n        identity = x\n        out = self.block1(x)\n        out = self.conv2(out)\n        out = replace_feature(out, self.norm2(out.features))\n        out = replace_feature(out, out.features + identity.features)\n        out = replace_feature(out, self.act2(out.features))\n\n        return out\n\n\n\n# Example usage\ninput_tensor = spconv.pytorch.SparseConvTensor(features=torch.randn(5, 3).to(DEVICE),\n                                       indices=torch.randint(0, 10, (5, 4), dtype=torch.int32).to(DEVICE),\n                                       spatial_shape=[10, 10, 10],\n                                       batch_size=1)\nbasic_block3d = SparseBasicBlock3d(3, 3).to(DEVICE)\noutput_tensor = basic_block3d(input_tensor)\nprint(output_tensor)\n\nSparseConvTensor[shape=torch.Size([5, 3])]",
    "crumbs": [
      "Model: utils"
    ]
  },
  {
    "objectID": "model_necks.html",
    "href": "model_necks.html",
    "title": "Model: necks",
    "section": "",
    "text": "“A neck can be utilized to aggregate features for enlarging receptive field and fusing multi-scale context” (https://arxiv.org/abs/2305.04925).\n\n\nsource\n\nASPPNeck\n\n ASPPNeck (in_channels:int)\n\n*Atrous Spatial Pyramid Pooling Neck Module.\nThis module applies several convolutions with different dilation rates to the input feature map and concatenates their outputs. The concatenated output is then passed through a convolutional block to produce the final output.*\n\n\n\n\nType\nDetails\n\n\n\n\nin_channels\nint\nNumber of input channels\n\n\n\n\n\nExported source\nclass ASPPNeck(nn.Module):\n    \"\"\"\n    Atrous Spatial Pyramid Pooling Neck Module.\n\n    This module applies several convolutions with different dilation rates\n    to the input feature map and concatenates their outputs. The concatenated\n    output is then passed through a convolutional block to produce the final output.\n    \"\"\"\n    def __init__(self,\n                 in_channels: int # Number of input channels\n                 ):\n\n        super(ASPPNeck, self).__init__()\n\n        self.pre_conv = BasicBlock(in_channels)\n        self.conv1x1 = nn.Conv2d(\n            in_channels, in_channels, kernel_size=1, stride=1, bias=False, padding=0)\n        self.weight = nn.Parameter(torch.randn(in_channels, in_channels, 3, 3))\n        self.post_conv = ConvBlock(in_channels * 6, in_channels, kernel_size=1, stride=1)\n\n    def _forward(self, x):\n        x = self.pre_conv(x)\n        branch1x1 = self.conv1x1(x)\n        branch1 = F.conv2d(x, self.weight, stride=1,\n                           bias=None, padding=1, dilation=1)\n        branch6 = F.conv2d(x, self.weight, stride=1,\n                           bias=None, padding=6, dilation=6)\n        branch12 = F.conv2d(x, self.weight, stride=1,\n                            bias=None, padding=12, dilation=12)\n        branch18 = F.conv2d(x, self.weight, stride=1,\n                            bias=None, padding=18, dilation=18)\n        x = self.post_conv(\n            torch.cat((x, branch1x1, branch1, branch6, branch12, branch18), dim=1))\n        return x\n\n    def forward(self, x):\n        if x.requires_grad:\n            out = cp.checkpoint(self._forward, x)\n        else:\n            out = self._forward(x)\n\n        return out\n\n\nThe ASPPNeck class is an implementation of the Atrous Spatial Pyramid Pooling (ASPP) module, which is a technique commonly used in computer vision, particularly in segmentation tasks, to effectively capture multi-scale contextual information. In the context of the framework, this module is applied within a 3D object detection framework to enhance the model’s ability to detect objects in the pointclouds.\nBy integrating the ASPP module, the architecture can effectively aggregate multi-scale contextual information, enhancing the model’s ability to detect objects at different scales and distances. The neck component in such architectures aggregates features from the backbone, thereby enhancing the receptive field and fusing multi-scale context.\n\nImplementation Details\n\nPre-Processing Block (pre_conv): The module begins with a BasicBlock that processes the input feature map. This step likely refines the features and prepares them for further processing by the ASPP layers.\n1x1 Convolution (conv1x1): A 1x1 convolution is applied to the input, producing a branch with the same number of channels as the input. This is part of the multi-branch strategy that ASPP employs to capture features at different scales.\nDilated Convolutions: The heart of ASPP lies in applying convolutions with different dilation rates (1, 6, 12, and 18). These convolutions capture features at multiple scales, with larger dilation rates corresponding to larger receptive fields. This multi-scale approach allows the model to better capture objects of varying sizes and distances in point clouds.\n\nThe authors decided to use fixed and manually chosen dilation rates, which might not be optimal for all datasets or detection tasks.\n\nFeature Concatenation: The outputs of these dilated convolutions, along with the original input and the 1x1 convolution output, are concatenated along the channel dimension. This results in a feature map that contains information from multiple scales, which is then further processed.\nPost-Processing Block (post_conv): After concatenation, a ConvBlock is applied to the combined features to produce the final output. This step integrates the multi-scale features and prepares them for subsequent stages in the detection pipeline.\nCheckpointing: The module includes a checkpointing mechanism to save memory during backpropagation, which is especially important when training deep networks on large datasets like LiDAR pointclouds.\n\n\n# Create a sample input tensor with batch size 1, 64 channels, and 128x128 spatial dimensions\ninput_tensor = torch.randn(1, 64, 128, 128)\n\n# Initialize the ASPPNeck module with 64 input channels\naspp_neck = ASPPNeck(64)\n\n# Pass the input tensor through the ASPPNeck module\noutput_tensor = aspp_neck(input_tensor)\n\n# Print the shape of the output tensor\nprint(\"Output shape:\", output_tensor.shape)\n\nOutput shape: torch.Size([1, 64, 128, 128])",
    "crumbs": [
      "Model: necks"
    ]
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "dataset",
    "section": "",
    "text": "source\n\npoints_in_boxes_jit\n\n points_in_boxes_jit (points:numpy.ndarray, boxes:numpy.ndarray,\n                      indices:numpy.ndarray)\n\nThis function determines if points are within a set of 3D boxes.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npoints\nndarray\nFloat array [N, *]\n\n\nboxes\nndarray\nFloat array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n\n\nindices\nndarray\nBool array of shape [N, M]\n\n\n\n\n\nExported source\n@numba.njit\ndef points_in_boxes_jit(points: np.ndarray, # Float array [N, *]\n                        boxes: np.ndarray, # Float array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n                        indices: np.ndarray # Bool array of shape [N, M]\n                        ): # Bool array of shape [N, M]\n    \"\"\"This function determines if points are within a set of 3D boxes.\"\"\"\n    num_points = points.shape[0]\n    num_boxes = boxes.shape[0]\n    for j in range(num_boxes):\n        for i in range(num_points):\n            if np.abs(points[i, 2] - boxes[j, 2]) &lt;= boxes[j, 5] / 2.0:\n                cosa = np.cos(boxes[j, -1])\n                sina = np.sin(boxes[j, -1])\n                shift_x = points[i, 0] - boxes[j, 0]\n                shift_y = points[i, 1] - boxes[j, 1]\n                local_x = shift_x * cosa + shift_y * sina\n                local_y = -shift_x * sina + shift_y * cosa\n                indices[i, j] = np.logical_and(np.abs(local_x) &lt;= boxes[j, 3] / 2.0,\n                                               np.abs(local_y) &lt;= boxes[j, 4] / 2.0)\n\n\n\nsource\n\n\npoints_in_rbbox\n\n points_in_rbbox (points:numpy.ndarray, boxes:numpy.ndarray)\n\nThis function determines if points are within a set of rotated 3D boxes and returns a boolean array indicating the results.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\npoints\nndarray\nFloat array [N, *]\n\n\nboxes\nndarray\nFloat array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n\n\n\n\n\nExported source\ndef points_in_rbbox(points: np.ndarray, # Float array [N, *]\n                    boxes: np.ndarray # Float array [M, 7] or [M, 9], with first 6 dimensions x, y, z, length, width, height, last dimension yaw angle\n                    ): # Bool array of shape [N, M]\n    \"\"\"This function determines if points are within a set of rotated 3D boxes and returns a boolean array indicating the results.\"\"\"\n    indices = np.zeros((points.shape[0], boxes.shape[0]), dtype=bool) # Bool array of shape [N, M]\n    points_in_boxes_jit(points, boxes, indices)\n    return indices\n\n\n\n# Test Data\npoints = np.array([\n    [1.0, 1.0, 1.0],\n    [2.0, 2.0, 2.0],\n    [3.0, 3.0, 3.0]\n])\n\nboxes = np.array([\n    [1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0],  # Box centered at (1,1,1) with length=2, width=2, height=2, no rotation\n    [2.0, 2.0, 2.0, 2.0, 2.0, 2.0, np.pi/4]  # Box centered at (2,2,2) with length=2, width=2, height=2, rotated 45 degrees\n])\n\n# Expected output: array of shape (3, 2)\n# For each point, we check if it is within each of the two boxes\nindices = points_in_rbbox(points, boxes)\n\nprint(\"Points:\\n\", points)\nprint(\"\\nBoxes:\\n\", boxes)\nprint(\"\\nIndices (Points in Boxes):\\n\", indices)\n\nPoints:\n [[1. 1. 1.]\n [2. 2. 2.]\n [3. 3. 3.]]\n\nBoxes:\n [[1.         1.         1.         2.         2.         2.\n  0.        ]\n [2.         2.         2.         2.         2.         2.\n  0.78539816]]\n\nIndices (Points in Boxes):\n [[ True False]\n [ True  True]\n [False False]]\n\n\n\n# Visualization\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot points\nax.scatter(points[:, 0], points[:, 1], points[:, 2], c='blue', marker='o')\n\n# Plot boxes\nplot_boxes(ax, boxes)\n\n# Set plot labels and limits\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nax.set_xlim(0, 4)\nax.set_ylim(0, 4)\nax.set_zlim(0, 4)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nsource\n\n\nBaseDataset\n\n BaseDataset (root_path, info_path, sampler=None, loading_pipelines=None,\n              augmentation=None, prepare_label=None, evaluations=None,\n              create_database=False, use_gt_sampling=True)\n\nThe BaseDataset class is designed to serve as a base class for different types of datasets. It provides methods and properties for loading, processing, and evaluating data, making it easier to handle different datasets in a consistent manner.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot_path\n\n\nRoot path of the dataset\n\n\ninfo_path\n\n\nPath to the info file\n\n\nsampler\nNoneType\nNone\nSampler for sampling data\n\n\nloading_pipelines\nNoneType\nNone\nLoading pipelines\n\n\naugmentation\nNoneType\nNone\nAugmentation pipelines\n\n\nprepare_label\nNoneType\nNone\nPrepare label pipelines\n\n\nevaluations\nNoneType\nNone\nEvaluation pipelines\n\n\ncreate_database\nbool\nFalse\nWhether to create database\n\n\nuse_gt_sampling\nbool\nTrue\nWhether to use ground truth sampling\n\n\n\n\n\nExported source\nclass BaseDataset(Dataset):\n    \"\"\"\n    The `BaseDataset` class is designed to serve as a base class for different types of datasets.\n    It provides methods and properties for loading, processing, and evaluating data, making it easier to handle different datasets in a consistent manner.\n    \"\"\"\n\n    def __init__(\n            self,\n            root_path, # Root path of the dataset\n            info_path, # Path to the info file\n            sampler=None, # Sampler for sampling data\n            loading_pipelines=None, # Loading pipelines\n            augmentation=None, # Augmentation pipelines\n            prepare_label=None, # Prepare label pipelines\n            evaluations=None, # Evaluation pipelines\n            create_database=False, # Whether to create database\n            use_gt_sampling=True # Whether to use ground truth sampling\n            ):\n\n        self._info_path = info_path\n        self._root_path = Path(root_path)\n        self.loading_pipelines = loading_pipelines\n        self.augmentations = augmentation\n        self.prepare_label = prepare_label\n        self.evaluations = evaluations\n        self.create_database = create_database\n        self.use_gt_sampling = use_gt_sampling\n        self.load_infos()\n        if use_gt_sampling and sampler is not None:\n            self.sampler = sampler()\n        else:\n            self.sampler = None\n\n    def __len__(self):\n        return len(self.infos)\n\n    def load_infos(self):\n        with open(os.path.join(self._root_path, self._info_path), \"rb\") as f:\n            self.infos = pickle.load(f)\n\n    def evaluation(self):\n        \"\"\"Dataset must provide a evaluation function to evaluate model.\"\"\"\n        # support different evaluation tasks\n        raise NotImplementedError\n\n    def load_pointcloud(self, res, info):\n        raise NotImplementedError\n\n    def load_box3d(self, res, info):\n        res[\"annotations\"] = {\n            'gt_boxes': info[\"gt_boxes\"].astype(np.float32).copy(),\n            'gt_names': np.array(info[\"gt_names\"]).reshape(-1).copy(),\n        }\n\n        return res\n\n    def __getitem__(self, idx):\n\n        info = self.infos[idx]\n        res = {\"token\": info[\"token\"]}\n\n        if self.loading_pipelines is not None:\n            for lp in self.loading_pipelines:\n                res = getattr(self, lp)(res, info)\n        if self.sampler is not None:\n            sampled_dict = self.sampler.sample_all(\n                res['annotations']['gt_boxes'],\n                res[\"annotations\"]['gt_names']\n            )\n            if sampled_dict is not None:\n                sampled_gt_names = sampled_dict[\"gt_names\"]\n                sampled_gt_boxes = sampled_dict[\"gt_boxes\"]\n                sampled_points = sampled_dict[\"points\"]\n                sampled_gt_masks = sampled_dict[\"gt_masks\"]\n                res['annotations'][\"gt_names\"] = np.concatenate(\n                    [res['annotations'][\"gt_names\"], sampled_gt_names], axis=0\n                )\n                res['annotations'][\"gt_boxes\"] = np.concatenate(\n                    [res['annotations'][\"gt_boxes\"], sampled_gt_boxes]\n                )\n\n                # remove points in sampled gt boxes\n                sampled_point_indices = points_in_rbbox(\n                    res['points'], sampled_gt_boxes[sampled_gt_masks])\n                res['points'] = res['points'][np.logical_not(\n                    sampled_point_indices.any(-1))]\n\n                res['points'] = np.concatenate(\n                    [sampled_points, res['points']], axis=0)\n        if self.augmentations is not None:\n            for aug in self.augmentations.values():\n                res = aug(res)\n\n        if self.prepare_label is not None:\n            for _, pl in self.prepare_label.items():\n                res = pl(res)\n\n        if 'annotations' in res and (not self.create_database):\n            del res['annotations']\n\n        return res\n\n    def format_eval(self):\n        raise NotImplementedError\n\n\nThe eval_main function is designed to evaluate the NuScenes dataset using a specified evaluation configuration. It follows these steps:\n\nConfiguration Setup: The function starts by creating a configuration object using the config_factory function, passing in the eval_version parameter. This configuration specifies the evaluation settings to be used.\nInitialization: It then initializes a NuScenesEval object, passing in the NuScenes dataset object (nusc), the evaluation configuration (cfg), the path to the results file (res_path), the dataset split to evaluate on (eval_set), and the directory to store the evaluation results (output_dir). The verbose=True parameter enables detailed logging during the evaluation process.\nEvaluation: Finally, the function calls the main method of the NuScenesEval object to perform the evaluation. The plot_examples=0 parameter indicates that no example plots should be generated during the evaluation.\n\nBy organizing the evaluation process into a function, eval_main simplifies the process of setting up and running evaluations on the NuScenes dataset, ensuring that the correct configuration and parameters are used.\n\nsource\n\n\neval_main\n\n eval_main (nusc, eval_version, res_path, eval_set, output_dir)\n\nEvaluate the detection results on the nuScenes dataset.\n\n\n\n\nDetails\n\n\n\n\nnusc\nNuScenes dataset object.\n\n\neval_version\nVersion of the evaluation configuration to use.\n\n\nres_path\nPath to the results file.\n\n\neval_set\nThe dataset split to evaluate on (e.g., ‘val’, ‘test’).\n\n\noutput_dir\nDirectory to store the evaluation results.\n\n\n\n\n\nExported source\ndef eval_main(nusc, # NuScenes dataset object.\n              eval_version, # Version of the evaluation configuration to use.\n              res_path, # Path to the results file.\n              eval_set, # The dataset split to evaluate on (e.g., 'val', 'test').\n              output_dir # Directory to store the evaluation results.\n              ):\n    \"\"\"\n    Evaluate the detection results on the nuScenes dataset.\n    \"\"\"\n\n    cfg = config_factory(eval_version)\n\n    nusc_eval = NuScenesEval(\n        nusc,\n        config=cfg,\n        result_path=res_path,\n        eval_set=eval_set,\n        output_dir=output_dir,\n        verbose=True,\n    )\n    _ = nusc_eval.main(plot_examples=0,)\n\n\n\nsource\n\n\nNuScenesDataset\n\n NuScenesDataset (info_path, root_path, nsweeps, sampler=None,\n                  loading_pipelines=None, augmentation=None,\n                  prepare_label=None, class_names=[], resampling=False,\n                  evaluations=None, create_database=False,\n                  use_gt_sampling=True, version='v1.0-trainval')\n\nThe NuScenesDataset class is designed to handle the NuScenes dataset. This class inherits from the BaseDataset class and includes methods to load, process, and evaluate data from the NuScenes dataset.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ninfo_path\n\n\nPath to dataset information file\n\n\nroot_path\n\n\nPath to root directory of dataset\n\n\nnsweeps\n\n\nNumber of sweeps (LiDAR frames) to use\n\n\nsampler\nNoneType\nNone\nSampler for dataset\n\n\nloading_pipelines\nNoneType\nNone\nLoading pipelines for data processing\n\n\naugmentation\nNoneType\nNone\nData augmentation methods\n\n\nprepare_label\nNoneType\nNone\nMethod for preparing labels\n\n\nclass_names\nlist\n[]\nList of class names\n\n\nresampling\nbool\nFalse\nWhether to resample dataset\n\n\nevaluations\nNoneType\nNone\nEvaluation methods\n\n\ncreate_database\nbool\nFalse\nWhether to create a database\n\n\nuse_gt_sampling\nbool\nTrue\nWhether to use ground truth sampling\n\n\nversion\nstr\nv1.0-trainval\nDataset version\n\n\n\n\n\nExported source\nclass NuScenesDataset(BaseDataset): # NuScenes dataset class\n    \"\"\"\n    The `NuScenesDataset` class is designed to handle the NuScenes dataset.\n    This class inherits from the `BaseDataset` class and includes methods to load, process, and evaluate data from the NuScenes dataset.\n    \"\"\"\n\n    def __init__(self,\n                 info_path,  # Path to dataset information file\n                 root_path,  # Path to root directory of dataset\n                 nsweeps,  # Number of sweeps (LiDAR frames) to use\n                 sampler=None,  # Sampler for dataset\n                 loading_pipelines=None,  # Loading pipelines for data processing\n                 augmentation=None,  # Data augmentation methods\n                 prepare_label=None,  # Method for preparing labels\n                 class_names=[],  # List of class names\n                 resampling=False,  # Whether to resample dataset\n                 evaluations=None,  # Evaluation methods\n                 create_database=False,  # Whether to create a database\n                 use_gt_sampling=True,  # Whether to use ground truth sampling\n                 version=\"v1.0-trainval\" # Dataset version\n                 ): # NuScenes dataset\n\n        super(NuScenesDataset, self).__init__(\n            root_path, info_path, sampler, loading_pipelines, augmentation, prepare_label, evaluations, create_database,\n            use_gt_sampling=use_gt_sampling)  # Initialize base class\n\n        self.nsweeps = nsweeps\n        assert self.nsweeps &gt; 0, \"At least input one sweep please!\"  # Ensure at least one sweep is used\n\n        self._class_names = list(itertools.chain(*[t for t in class_names]))  # Flatten class names list\n        self.version = version\n\n        if resampling:\n            self.cbgs()  # Resample dataset if needed\n\n    def cbgs(self): # Performs class-balanced resampling on the dataset by oversampling underrepresented classes\n        _cls_infos = {name: [] for name in self._class_names}  # Initialize dictionary for class info\n        for info in self.infos:  # Iterate over dataset information\n            for name in set(info[\"gt_names\"]):  # For each unique ground truth name\n                if name in self._class_names:\n                    _cls_infos[name].append(info)  # Add info to corresponding class\n\n        duplicated_samples = sum([len(v) for _, v in _cls_infos.items()])  # Total number of samples after duplication\n        _cls_dist = {k: len(v) / duplicated_samples for k, v in _cls_infos.items()}  # Distribution of classes\n\n        _nusc_infos = []\n\n        frac = 1.0 / len(self._class_names)  # Fraction for resampling\n        ratios = [frac / v for v in _cls_dist.values()]  # Calculate resampling ratios\n\n        for cls_infos, ratio in zip(list(_cls_infos.values()), ratios):\n            _nusc_infos += np.random.choice(cls_infos, int(len(cls_infos) * ratio)).tolist()  # Resample and add to infos\n\n        self.infos = _nusc_infos  # Update dataset information\n\n    def read_file(self, path, num_point_feature=4): # Reads a point cloud file and returns the points in the specified format\n        points = np.fromfile(os.path.join(self._root_path, path),\n                             dtype=np.float32).reshape(-1, 5)[:, :num_point_feature]  # Read point cloud file and reshape\n        return points  # Return points of shape (N, num_point_feature)\n\n    def read_sweep(self, sweep, min_distance=1.0): # Reads a sweep file, applies transformations, removes points too close to the origin, and returns the points and their timestamps\n        points_sweep = self.read_file(str(sweep[\"lidar_path\"])).T  # Read sweep file and transpose, shape (num_point_feature, N)\n\n        nbr_points = points_sweep.shape[1]\n        if sweep[\"transform_matrix\"] is not None:\n            points_sweep[:3, :] = sweep[\"transform_matrix\"].dot(\n                np.vstack((points_sweep[:3, :], np.ones(nbr_points))))[:3, :]  # Apply transformation matrix\n        points_sweep = self.remove_close(points_sweep, min_distance)  # Remove points too close to the origin\n        curr_times = sweep[\"time_lag\"] * np.ones((1, points_sweep.shape[1]))  # Create current times array\n\n        return points_sweep.T, curr_times.T  # Return points and times of shape (N, num_point_feature), (N, 1)\n\n    @staticmethod\n    def remove_close(points, radius: float): # Removes points that are too close to the origin\n        \"\"\"\n        Removes point too close within a certain radius from origin.\n        :param radius: Radius below which points are removed.\n        \"\"\"\n        x_filt = np.abs(points[0, :]) &lt; radius\n        y_filt = np.abs(points[1, :]) &lt; radius\n        not_close = np.logical_not(np.logical_and(x_filt, y_filt))  # Create filter for points outside the radius\n        points = points[:, not_close]  # Apply filter to points\n        return points  # Return filtered points\n\n    def load_pointcloud(self, res, info): # Loads a point cloud and its sweeps, concatenating them together with their timestamps\n\n        lidar_path = info[\"lidar_path\"]\n\n        points = self.read_file(str(lidar_path))  # Read point cloud file\n\n        sweep_points_list = [points]  # Initialize sweep points list\n        sweep_times_list = [np.zeros((points.shape[0], 1))]  # Initialize sweep times list\n\n        for i in range(len(info[\"sweeps\"])):  # Iterate over sweeps\n            sweep = info[\"sweeps\"][i]\n            points_sweep, times_sweep = self.read_sweep(sweep)  # Read each sweep\n            sweep_points_list.append(points_sweep)  # Add sweep points to list\n            sweep_times_list.append(times_sweep)  # Add sweep times to list\n\n        points = np.concatenate(sweep_points_list, axis=0)  # Concatenate all points\n        times = np.concatenate(sweep_times_list, axis=0).astype(points.dtype)  # Concatenate all times\n\n        res[\"points\"] = np.hstack([points, times])  # Combine points and times\n\n        return res  # Return updated result\n\n    def evaluation(self, detections, output_dir=None, testset=False): # Evaluates detections against the dataset, calculates metrics, and optionally performs resampling. It returns the results or None if the evaluation is not performed\n        version = self.version\n        eval_set_map = {\n            \"v1.0-mini\": \"mini_val\",\n            \"v1.0-trainval\": \"val\",\n            \"v1.0-test\": \"test\",\n        }\n\n        dets = [v for _, v in detections.items()]  # Get list of detections\n\n        nusc_annos = {\n            \"results\": {},\n            \"meta\": None,\n        }\n\n        nusc = NuScenes(version=version, dataroot=str(\n            self._root_path), verbose=True)  # Initialize NuScenes dataset\n\n        mapped_class_names = []\n        for n in self._class_names:\n            mapped_class_names.append(n)  # Map class names\n\n        for det in dets:  # Iterate over detections\n            annos = []\n            boxes = _second_det_to_nusc_box(det) # Convert detection to NuScenes box format\n            boxes = _lidar_nusc_box_to_global(nusc, boxes, det[\"token\"]) # Convert lidar boxes to global coordinates\n            for i, box in enumerate(boxes):\n                name = mapped_class_names[box.label]\n                if np.sqrt(box.velocity[0] ** 2 + box.velocity[1] ** 2) &gt; 0.2:\n                    if name in [\n                        \"car\",\n                        \"construction_vehicle\",\n                        \"bus\",\n                        \"truck\",\n                        \"trailer\",\n                    ]:\n                        attr = \"vehicle.moving\"\n                    elif name in [\"bicycle\", \"motorcycle\"]:\n                        attr = \"cycle.with_rider\"\n                    else:\n                        attr = None\n                else:\n                    if name in [\"pedestrian\"]:\n                        attr = \"pedestrian.standing\"\n                    elif name in [\"bus\"]:\n                        attr = \"vehicle.parked\"\n                    else:\n                        attr = None\n\n                nusc_anno = {\n                    \"sample_token\": det[\"token\"],\n                    \"translation\": box.center.tolist(),  # Box center coordinates\n                    \"size\": box.wlh.tolist(),  # Box size (width, length, height)\n                    \"rotation\": box.orientation.elements.tolist(),  # Box rotation (quaternion)\n                    \"velocity\": box.velocity[:2].tolist(),  # Box velocity (x, y)\n                    \"detection_name\": name,  # Class name\n                    \"detection_score\": box.score,  # Detection score\n                    \"attribute_name\": attr\n                    if attr is not None\n                    else max(cls_attr_dist[name].items(), key=operator.itemgetter(1))[0],  # Attribute name\n                }\n                annos.append(nusc_anno)\n            nusc_annos[\"results\"].update({det[\"token\"]: annos})  # Add annotations to results\n\n        nusc_annos[\"meta\"] = {\n            \"use_camera\": False,\n            \"use_lidar\": True,\n            \"use_radar\": False,\n            \"use_map\": False,\n            \"use_external\": False,\n        }\n\n        name = self._info_path.split(\"/\")[-1].split(\".\")[0]\n        res_path = str(Path(output_dir) / Path(name + \".json\"))\n        with open(res_path, \"w\") as f:\n            json.dump(nusc_annos, f)  # Save annotations to JSON file\n\n        print(f\"Finish generate predictions for testset, save to {res_path}\")\n\n        if not testset:\n            eval_main(\n                nusc,\n                \"detection_cvpr_2019\",\n                res_path,\n                eval_set_map[self.version],\n                output_dir,\n            )  # Run evaluation\n\n            with open(Path(output_dir) / \"metrics_summary.json\", \"r\") as f:\n                metrics = json.load(f)  # Load evaluation metrics\n\n            detail = {}\n            result = f\"Nusc {version} Evaluation\\n\"\n            for name in mapped_class_names:  # Iterate over class names\n                detail[name] = {}\n                for k, v in metrics[\"label_aps\"][name].items():  # Iterate over evaluation metrics\n                    detail[name][f\"dist@{k}\"] = v\n                threshs = \", \".join(list(metrics[\"label_aps\"][name].keys()))  # Distance thresholds\n                scores = list(metrics[\"label_aps\"][name].values())  # Scores\n                mean = sum(scores) / len(scores)  # Mean score\n                scores = \", \".join([f\"{s * 100:.2f}\" for s in scores])  # Format scores\n                result += f\"{name} Nusc dist AP@{threshs}\\n\"\n                result += scores\n                result += f\" mean AP: {mean}\"\n                result += \"\\n\"\n            res_nusc = {\n                \"results\": {\"nusc\": result},\n                \"detail\": {\"nusc\": detail},\n            }\n        else:\n            res_nusc = None\n\n        if res_nusc is not None:\n            res = {\n                \"results\": {\"nusc\": res_nusc[\"results\"][\"nusc\"], },\n                \"detail\": {\"eval.nusc\": res_nusc[\"detail\"][\"nusc\"], },\n            }\n            return res['results']  # Return results\n        else:\n            return None  # Return None if no results\n\n\n\ntrain_dataset = NuScenesDataset(\"infos_train_10sweeps_withvelo_filterZero.pkl\",\n                                \"/root/nuscenes-dataset/v1.0-mini\",\n                                10,\n                                class_names=[[\"car\"], [\"truck\", \"construction_vehicle\"], [\"bus\", \"trailer\"], [\"barrier\"], [\"motorcycle\", \"bicycle\"], [\"pedestrian\", \"traffic_cone\"]],\n                                resampling=True)\n\nThe cbgs method in the NuScenesDataset class performs class-balanced resampling on the dataset. This technique is employed to address the imbalance in the number of samples for different classes within the dataset. Here is a step-by-step breakdown of how the cbgs method works:\n\nInitialize Class Information Dictionary:\n_cls_infos = {name: [] for name in self._class_names}\nThis creates a dictionary where the keys are the class names, and the values are empty lists. This dictionary will store the dataset information for each class.\nPopulate Class Information Dictionary:\nfor info in self.infos:\n    for name in set(info[\"gt_names\"]):\n        if name in self._class_names:\n            _cls_infos[name].append(info)\nThe method iterates over the dataset information (self.infos). For each dataset entry (info), it checks the ground truth names (info[\"gt_names\"]). If a ground truth name is in the list of class names (self._class_names), it adds the corresponding info to the list for that class in _cls_infos.\nCalculate Class Distributions:\nduplicated_samples = sum([len(v) for _, v in _cls_infos.items()])\n_cls_dist = {k: len(v) / duplicated_samples for k, v in _cls_infos.items()}\nThe total number of samples after duplication is calculated (duplicated_samples). Then, the distribution of each class in the dataset is computed (_cls_dist), which is the number of samples for each class divided by the total number of samples.\nInitialize Variables for Resampling:\n_nusc_infos = []\nfrac = 1.0 / len(self._class_names)\nratios = [frac / v for v in _cls_dist.values()]\nAn empty list _nusc_infos is initialized to store the resampled dataset information. The fraction (frac) is calculated as the inverse of the number of class names. Resampling ratios are then computed for each class based on the class distribution (_cls_dist).\nPerform Resampling:\nfor cls_infos, ratio in zip(list(_cls_infos.values()), ratios):\n    _nusc_infos += np.random.choice(cls_infos, int(len(cls_infos) * ratio)).tolist()\nFor each class’s information (cls_infos) and its corresponding resampling ratio (ratio), a number of samples proportional to the ratio are randomly selected from cls_infos and added to _nusc_infos.\nUpdate Dataset Information:\nself.infos = _nusc_infos\nFinally, the dataset information (self.infos) is updated with the resampled dataset information stored in _nusc_infos.\n\nIn summary, the cbgs method addresses class imbalance by oversampling underrepresented classes to create a more balanced dataset. This is achieved by calculating the distribution of each class, determining resampling ratios, and then randomly selecting samples based on these ratios. The resulting balanced dataset is then used for further processing and training.\n\n# Test cbgs method (performs class-balanced resampling on the dataset by oversampling underrepresented classes)\n\nprint(\"Before resampling:\")\nfor cls_name in train_dataset._class_names:\n    print(f\"{cls_name}: {sum(1 for info in train_dataset.infos if cls_name in info['gt_names'])}\")\n\ntrain_dataset.cbgs()\n\nprint(\"\\nAfter resampling:\")\nfor cls_name in train_dataset._class_names:\n    print(f\"{cls_name}: {sum(1 for info in train_dataset.infos if cls_name in info['gt_names'])}\")\n\nBefore resampling:\ncar: 7350\ntruck: 5529\nconstruction_vehicle: 6160\nbus: 6002\ntrailer: 1618\nbarrier: 3350\nmotorcycle: 3397\nbicycle: 3866\npedestrian: 6660\ntraffic_cone: 5909\n\nAfter resampling:\ncar: 49840\ntruck: 41378\nconstruction_vehicle: 45470\nbus: 39848\ntrailer: 12492\nbarrier: 26659\nmotorcycle: 25617\nbicycle: 29545\npedestrian: 47074\ntraffic_cone: 44544\n\n\n\n# Simulate reading a file\npoints = train_dataset.read_file(\"/root/nuscenes-dataset/v1.0-mini/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin\")\nprint(f\"Read points: {points.shape}\")\n\nRead points: (34752, 4)\n\n\n\n# Simulate reading a sweep\nsweep = {\n    \"lidar_path\": \"/root/nuscenes-dataset/v1.0-mini/sweeps/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603597909.pcd.bin\",\n    \"transform_matrix\": None,\n    \"time_lag\": 0.1\n}\npoints, times = train_dataset.read_sweep(sweep)\nprint(f\"Read sweep points: {points.shape}, times: {times.shape}\")\n\nRead sweep points: (20486, 4), times: (20486, 1)\n\n\n\n# Simulate loading a pointcloud\ninfo = {\n    \"lidar_path\": \"/root/nuscenes-dataset/v1.0-mini/samples/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603547590.pcd.bin\",\n    \"sweeps\": [{\"lidar_path\": \"/root/nuscenes-dataset/v1.0-mini/sweeps/LIDAR_TOP/n008-2018-08-01-15-16-36-0400__LIDAR_TOP__1533151603597909.pcd.bin\", \"transform_matrix\": None, \"time_lag\": 0.1}]\n}\nres = {}\nresult = train_dataset.load_pointcloud(res, info)\nprint(f\"Loaded pointcloud: {result['points'].shape}\")\n\nLoaded pointcloud: (55238, 5)",
    "crumbs": [
      "dataset"
    ]
  },
  {
    "objectID": "model_readers.html",
    "href": "model_readers.html",
    "title": "Model: readers",
    "section": "",
    "text": "source\n\n\n\n PFNLayer (in_channels:int, out_channels:int, norm_cfg=None,\n           last_layer:bool=False)\n\nPillar Feature Net Layer. The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nnorm_cfg\nNoneType\nNone\nNormalization config (not used here, but could be for future extensions)\n\n\nlast_layer\nbool\nFalse\nIf last_layer, there is no concatenation of features\n\n\n\n\n\nExported source\nclass PFNLayer(nn.Module):\n    \"\"\"\n    Pillar Feature Net Layer.\n    The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only\n    used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n    \"\"\"\n    def __init__(self,\n                 in_channels: int,  # Number of input channels\n                 out_channels: int,  # Number of output channels\n                 norm_cfg=None,  # Normalization config (not used here, but could be for future extensions)\n                 last_layer: bool = False  # If last_layer, there is no concatenation of features\n                 ):\n        super().__init__()\n        self.last_vfe = last_layer  # Check if this is the last layer\n        if not self.last_vfe:\n            out_channels = out_channels // 2  # If not the last layer, half the output channels\n        self.units = out_channels\n\n        self.linear = nn.Linear(in_channels, out_channels, bias=False)  # Linear layer to transform inputs\n        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)  # Batch normalization\n\n    def forward(self, inputs, unq_inv):\n        torch.backends.cudnn.enabled = False  # Disable cuDNN for compatibility reasons\n        x = self.linear(inputs)  # Apply linear transformation\n        x = self.norm(x)  # Apply batch normalization\n        x = F.relu(x)  # Apply ReLU activation\n        torch.backends.cudnn.enabled = True  # Re-enable cuDNN\n\n        # max pooling\n        feat_max = torch_scatter.scatter_max(x, unq_inv, dim=0)[0]  # Perform scatter max pooling\n        x_max = feat_max[unq_inv]  # Gather the max features for each point\n\n        if self.last_vfe:\n            return x_max  # If this is the last layer, return the max features\n        else:\n            x_concatenated = torch.cat([x, x_max], dim=1)  # Otherwise, concatenate the original and max features\n            return x_concatenated  # Return the concatenated features\n\n\nThe PFNLayer Class class implements a layer of the Pillar Feature Net. It includes a linear transformation, batch normalization, and ReLU activation. It also performs scatter max pooling to extract the maximum features per pillar. The PFNLayer class essentially extracts features from point clouds.\n\n\nThis class extends basic PyTorch components to perform specific operations for 3D point cloud data. The main differences include:\n\nScatter Operations: Instead of using standard pooling operations like max pooling, the class employs torch_scatter.scatter_max, which is crucial for handling sparse point cloud data. PyTorch does not natively support this type of scatter operation.\nCustom Feature Concatenation: The concatenation of original and max-pooled features, dependent on whether the layer is the last in the network, is a custom behavior not found in standard PyTorch layers.\n\n\n\n\n\nPooling in Point Clouds: PFNLayer abstracts the complexity of performing scatter-based max pooling, which is not straightforward in PyTorch’s standard API.\nConditional Feature Concatenation: It abstracts the logic of deciding whether or not to concatenate features based on whether it’s the last layer.\nCustom Batch Normalization: The use of nn.BatchNorm1d with a custom eps and momentum is abstracted, so the user doesn’t need to handle these details directly.\n\n\n\n\n\nLimited Flexibility: The class is specialized for the PointPillars architecture. A more direct PyTorch implementation might allow for greater flexibility in terms of pooling strategies, feature transformations, and layer configurations.\nHardcoded Operations: The specific operations (like the choice of ReLU activation, BatchNorm1d, and scatter_max) are hardcoded. A more direct PyTorch approach would allow these choices to be more easily customized or replaced with alternatives.\n\n\n# Example data\ninputs = torch.rand(10, 64)\nunq_inv = torch.randint(0, 2, (10,))\n\n\nprint(f'Input shape: {inputs.shape}')\nprint(f'Unique inverse shape: {unq_inv.shape}')\n\n# Initialize PFNLayer\npfn_layer = PFNLayer(in_channels=64, out_channels=128, last_layer=False)\n\n# Forward pass\noutput = pfn_layer(inputs, unq_inv)\n\nprint(f'Output shape: {output.shape}')\n\n# Plot input features and output features\nfig, ax = plt.subplots(2, 1, figsize=(10, 8))\nax[0].imshow(inputs.numpy(), aspect='auto', cmap='viridis')\nax[0].set_title('Input Features')\nax[0].set_xlabel('Feature Dimension')\nax[0].set_ylabel('Point Index')\n\nax[1].imshow(output.detach().numpy(), aspect='auto', cmap='viridis')\nax[1].set_title('Output Features')\nax[1].set_xlabel('Feature Dimension')\nax[1].set_ylabel('Point Index')\n\nplt.tight_layout()\nplt.show()\n\nInput shape: torch.Size([10, 64])\nUnique inverse shape: torch.Size([10])\nOutput shape: torch.Size([10, 128])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n PillarNet (num_input_features:int, voxel_size:list, pc_range:list)\n\n*PillarNet. The network performs dynamic pillar scatter that convert point cloud into pillar representation and extract pillar features\nReference: PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784) End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnum_input_features\nint\nNumber of input features\n\n\nvoxel_size\nlist\nA list that defines the size of the voxels (grids) in the x and y dimensions.\n\n\npc_range\nlist\nA list defining the range of the point cloud data in the x and y dimensions. This is used to filter and normalize the point cloud data. Only utilize x and y min\n\n\n\n\n\nExported source\nclass PillarNet(nn.Module):\n    \"\"\"\n    PillarNet.\n    The network performs dynamic pillar scatter that convert point cloud into pillar representation\n    and extract pillar features\n\n    Reference:\n    PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784)\n    End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)\n    \"\"\"\n\n    def __init__(self,\n                 num_input_features: int, # Number of input features\n                 voxel_size: list, # A list that defines the size of the voxels (grids) in the x and y dimensions.\n                 pc_range: list, # A list defining the range of the point cloud data in the x and y dimensions. This is used to filter and normalize the point cloud data. Only utilize x and y min\n                 ):\n        super().__init__()\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n    def forward(self,\n                points: torch.Tensor # Points in LiDAR coordinate, shape: (N, d), format: batch_id, x, y, z, feat1, ...\n                ):\n\n        device = points.device\n        dtype = points.dtype\n\n        # discard out of range points\n        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n                     )/self.voxel_size  # x,  y, z\n        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(points).to(device)\n        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n\n        points_coords = (\n            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n\n        mask = reduce(torch.logical_and, (points_coords[:, 0] &gt;= 0,\n                                          points_coords[:, 0] &lt; grid_size[0],\n                                          points_coords[:, 1] &gt;= 0,\n                                          points_coords[:, 1] &lt; grid_size[1]))\n\n        points = points[mask]\n        points_coords = points_coords[mask]\n\n        points_coords = points_coords.long()\n        batch_idx = points[:, 0:1].long()\n\n        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n        unq = unq.int()\n\n        points_mean_scatter = torch_scatter.scatter_mean(\n            points[:, 1:4], unq_inv, dim=0)\n\n        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n\n        # Find distance of x, y, and z from pillar center\n        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n\n        # Combine together feature decorations\n        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n\n        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]\n\n\nThe PillarNet class is a neural network module designed for converting LiDAR point clouds into a structured pillar representation. It is based on the concepts introduced in papers such as PointPillars: Fast Encoders for Object Detection from Point Clouds and End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds. This network performs a dynamic pillar scatter operation, which helps in efficiently extracting pillar features from unstructured point cloud data.\n\n\n\nForward Method:\n\nPoint Filtering: The input points are filtered based on the specified pc_range to discard points that are outside the defined range.\nGrid Size Calculation: The grid size is computed based on the voxel_size and pc_range, which determines the resolution of the voxelization.\nPoint Coordinates Normalization: The point coordinates are normalized with respect to the voxel size and point cloud range, converting the continuous coordinates into discrete voxel indices.\nPoint Clustering: The points are clustered into pillars based on their voxel indices.\nFeature Decoration: Additional features are calculated, including:\n\nf_cluster: The distance of each point from the mean of its cluster (pillar).\nf_center: The distance of each point from the center of its respective voxel.\n\nFeature Aggregation: The original point features, along with the newly computed features (f_cluster and f_center), are concatenated to form the final feature set for each point.\nReturn Values: The method returns the final point features, the unique pillar indices, the inverse indices used for aggregation, and the grid size.\n\n\n\n\n\n\nAbstraction of Point Cloud Operations:\n\nThe PillarNet class abstracts the voxelization and feature extraction process, which would otherwise require manual implementation using basic PyTorch operations. It provides a higher-level interface to work with point cloud data, simplifying the process of converting point clouds into pillar-based representations.\n\nIntegration of Advanced Operations:\n\nOperations like torch_scatter.scatter_mean are used for efficiently computing cluster mean features. These operations are not directly available in standard PyTorch and require the use of external libraries like torch_scatter.\n\nGrid Management:\n\nThe class handles the computation of grid size and point normalization internally, which would otherwise require manual calculation and management in a more direct implementation.\n\n\n\n\n\n\nFlexibility:\n\nWhile PillarNet provides a streamlined approach to pillar feature extraction, it may lack the flexibility needed for more customized operations or for handling point cloud data in formats that differ from the assumptions made in the class.\n\nLimited Customization:\n\nThe class is designed for a specific type of pillar-based representation. Users who require different types of voxelization or feature extraction strategies may find it limiting and may need to modify or extend the class, which could be more cumbersome than implementing a direct approach from scratch.\n\n\n\n# Create a sample point cloud with shape (N, d)\n# Here, d includes batch_id, x, y, z, and some additional features (e.g., intensity)\npoints = torch.tensor([\n    [0, 1.0, 2.0, 3.0, 0.5],\n    [0, 2.5, 3.5, 4.5, 0.6],\n    [1, 5.0, 6.0, 7.0, 0.7],\n    [1, 8.0, 9.0, 10.0, 0.8]\n], dtype=torch.float32)\n\n# Define the number of input features (excluding batch_id, x, y, z)\nnum_input_features = points.shape[1] - 1\n\n# Define voxel size (x_size, y_size, z_size)\nvoxel_size = [0.5, 0.5, 0.5]\n\n# Define point cloud range (x_min, y_min, z_min, x_max, y_max, z_max)\npc_range = [0, 0, 0, 10, 10, 10]\n\n# Create an instance of PillarNet\npillar_net = PillarNet(num_input_features, voxel_size, pc_range)\n\n# Forward pass with the sample points\nfeatures, unique_voxel_indices, inverse_indices, grid_size = pillar_net(points)\n\n# Print the results\nprint(\"Features:\", features)\nprint(\"Unique Voxel Indices:\", unique_voxel_indices)\nprint(\"Inverse Indices:\", inverse_indices)\nprint(\"Grid Size:\", grid_size)\n\nFeatures: tensor([[ 1.0000,  2.0000,  3.0000,  0.5000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 2.5000,  3.5000,  4.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 5.0000,  6.0000,  7.0000,  0.7000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 8.0000,  9.0000, 10.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500]])\nUnique Voxel Indices: tensor([[ 0,  4,  2],\n        [ 0,  7,  5],\n        [ 1, 12, 10],\n        [ 1, 18, 16]], dtype=torch.int32)\nInverse Indices: tensor([0, 1, 2, 3])\nGrid Size: [20 20]\n\n\n\nsource\n\n\n\n\n\n PillarFeatureNet (num_input_features:int, num_filters:list,\n                   voxel_size:list, pc_range:list, norm_cfg:None)\n\nPillar Feature Net. The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a similar role to SECOND’s second.pytorch.voxelnet.VoxelFeatureExtractor.\n\n\n\n\nType\nDetails\n\n\n\n\nnum_input_features\nint\nNumber of input features\n\n\nnum_filters\nlist\nNumber of features in each of the N PFNLayers\n\n\nvoxel_size\nlist\nSize of voxels, only utilize x and y size\n\n\npc_range\nlist\nPoint cloud range, only utilize x and y min\n\n\nnorm_cfg\nNone\nNormalization config\n\n\n\n\n\nExported source\nclass PillarFeatureNet(nn.Module):\n    \"\"\"\n    Pillar Feature Net.\n    The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a\n    similar role to SECOND's second.pytorch.voxelnet.VoxelFeatureExtractor.\n    \"\"\"\n    def __init__(\n        self,\n        num_input_features: int, # Number of input features\n        num_filters: list, # Number of features in each of the N PFNLayers\n        voxel_size: list, # Size of voxels, only utilize x and y size\n        pc_range: list, # Point cloud range, only utilize x and y min\n        norm_cfg:None, # Normalization config\n    ):\n\n        super().__init__()\n        assert len(num_filters) &gt; 0\n        num_input_features += 5\n\n        # Create PillarFeatureNet layers\n        num_filters = [num_input_features] + list(num_filters)\n        pfn_layers = []\n        for i in range(len(num_filters) - 1):\n            in_filters = num_filters[i]\n            out_filters = num_filters[i + 1]\n            if i &lt; len(num_filters) - 2:\n                last_layer = False\n            else:\n                last_layer = True\n            pfn_layers.append(\n                PFNLayer(\n                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n                )\n            )\n        self.pfn_layers = nn.ModuleList(pfn_layers)\n\n        self.feature_output_dim = num_filters[-1]\n\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n        self.voxelization = PillarNet(num_input_features, voxel_size, pc_range)\n\n    def forward(self, points):\n        features, coords, unq_inv, grid_size = self.voxelization(points)\n        # Forward pass through PFNLayers\n        for pfn in self.pfn_layers:\n            features = pfn(features, unq_inv)  # num_points, dim_feat\n\n        feat_max = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n\n        return feat_max, coords, grid_size\n\n\nThe PillarFeatureNet class is designed for preparing pillar features from the point cloud data, used in 3D object detection tasks in LiDAR-based perception systems. This network handles the transformation of raw point cloud data into a more structured and feature-rich representation, which will be essential for further processing in the object detection.\n\n\n\nInitialization (__init__ method):\n\nInput Features Augmentation: The class initializes by augmenting the input features with additional spatial information, increasing the dimensionality by 5 (which might include features like the relative position of points within a pillar, etc.).\nLayer Creation: It then creates a sequence of layers, PFNLayers, which perform the feature extraction. The number of layers and their configuration are determined by the num_filters parameter.\nVoxelization: The class initializes the voxelization process, converting the raw point cloud data into a voxel grid with specified voxel_size and within a specified pc_range (point cloud range). This voxelization is essential for handling irregular point clouds by grouping points into a regular grid structure, called “pillars.”\n\nForward Pass (forward method):\n\nVoxelization: The input point cloud is first voxelized, where the points are grouped into pillars, and features are extracted.\nPFN Layers: The extracted features are passed through the PFNLayers, where each layer performs a certain amount of processing. These layers typically involve operations like PointNet-style feature learning.\nMax Pooling: Finally, the features are aggregated using a max-pooling operation (torch_scatter.scatter_max), which pools the features across the points in each pillar to get a fixed-size feature vector for each pillar.\n\n\n\n\n\nThis class abstracts several operations that would require more manual implementation in pure PyTorch. For instance, voxelization and the subsequent grouping of points into pillars are handled internally by the PillarNet class.\nThe abstraction can reduce flexibility. If you need to tweak certain aspects of the feature extraction or voxelization process, this might be harder to do compared to a more manual approach where every operation is explicit.\n\n# Define input parameters for PillarFeatureNet\n# Mock values for demonstration purposes\nnum_input_features = 3\nnum_filters = [64, 128]\nvoxel_size = [0.2, 0.2, 0.2]\npc_range = [0, 0, 0, 50, 50, 50]\nnorm_cfg = None\n\n# Instantiate the PillarFeatureNet\npillar_feature_net = PillarFeatureNet(\n    num_input_features=num_input_features,\n    num_filters=num_filters,\n    voxel_size=voxel_size,\n    pc_range=pc_range,\n    norm_cfg=norm_cfg\n)\n\n# Create some dummy input points\n# Each point might have x, y, z, intensity, etc.\nnum_points = 100  # Number of points in the point cloud\npoints = torch.rand(num_points, num_input_features + 1)  # Random points (x, y, z, intensity)\n\n# Run a forward pass\nfeat_max, coords, grid_size = pillar_feature_net(points)\n\n# Print the outputs\nprint(\"Max Features:\\n\", feat_max)\nprint(\"Voxel Coordinates:\\n\", coords)\nprint(\"Grid Size:\\n\", grid_size)\n\nMax Features:\n tensor([[0.0000, 1.9903, 1.5468,  ..., 0.1351, 1.5962, 1.8613],\n        [0.0953, 0.8951, 0.5835,  ..., 0.3777, 0.4148, 1.1723],\n        [0.0000, 0.0000, 0.0628,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.6913, 0.4461,  ..., 1.3929, 0.0906, 1.3596],\n        [0.0000, 0.0000, 0.0000,  ..., 0.1496, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 1.5835, 0.0000, 0.0030]],\n       grad_fn=&lt;CppNode&lt;ScatterMax&gt;&gt;)\nVoxel Coordinates:\n tensor([[0, 0, 0],\n        [0, 1, 0],\n        [0, 2, 0],\n        [0, 3, 0],\n        [0, 4, 0],\n        [0, 0, 1],\n        [0, 1, 1],\n        [0, 2, 1],\n        [0, 3, 1],\n        [0, 4, 1],\n        [0, 0, 2],\n        [0, 1, 2],\n        [0, 2, 2],\n        [0, 3, 2],\n        [0, 4, 2],\n        [0, 0, 3],\n        [0, 1, 3],\n        [0, 2, 3],\n        [0, 3, 3],\n        [0, 4, 3],\n        [0, 0, 4],\n        [0, 1, 4],\n        [0, 2, 4],\n        [0, 3, 4],\n        [0, 4, 4]], dtype=torch.int32)\nGrid Size:\n [250 250]\n\n\n\n# Create some dummy input points\nnum_points = 100  # Number of points in the point cloud\npoints = torch.rand(num_points, num_input_features + 1)  # Random points (x, y, z, intensity)\n\n# Visualize the input using matplotlib\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot point cloud coordinates\nax.scatter(points[:, 0].numpy(), points[:, 1].numpy(), points[:, 2].numpy(), c='r', marker='o')\n\n# Label axes\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Title\nax.set_title('Point Cloud Coordinates')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualize the output using matplotlib\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot voxel coordinates\nax.scatter(coords[:, 0].numpy(), coords[:, 1].numpy(), coords[:, 2].numpy(), c='b', marker='o')\n\n# Label axes\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Title\nax.set_title('Voxel Coordinates')\n\nplt.show()",
    "crumbs": [
      "Model: readers"
    ]
  },
  {
    "objectID": "model_readers.html#pillar-encoder",
    "href": "model_readers.html#pillar-encoder",
    "title": "Model: readers",
    "section": "",
    "text": "source\n\n\n\n PFNLayer (in_channels:int, out_channels:int, norm_cfg=None,\n           last_layer:bool=False)\n\nPillar Feature Net Layer. The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\nint\n\nNumber of input channels\n\n\nout_channels\nint\n\nNumber of output channels\n\n\nnorm_cfg\nNoneType\nNone\nNormalization config (not used here, but could be for future extensions)\n\n\nlast_layer\nbool\nFalse\nIf last_layer, there is no concatenation of features\n\n\n\n\n\nExported source\nclass PFNLayer(nn.Module):\n    \"\"\"\n    Pillar Feature Net Layer.\n    The Pillar Feature Net could be composed of a series of these layers, but the PointPillars paper results only\n    used a single PFNLayer. This layer performs a similar role as second.pytorch.voxelnet.VFELayer.\n    \"\"\"\n    def __init__(self,\n                 in_channels: int,  # Number of input channels\n                 out_channels: int,  # Number of output channels\n                 norm_cfg=None,  # Normalization config (not used here, but could be for future extensions)\n                 last_layer: bool = False  # If last_layer, there is no concatenation of features\n                 ):\n        super().__init__()\n        self.last_vfe = last_layer  # Check if this is the last layer\n        if not self.last_vfe:\n            out_channels = out_channels // 2  # If not the last layer, half the output channels\n        self.units = out_channels\n\n        self.linear = nn.Linear(in_channels, out_channels, bias=False)  # Linear layer to transform inputs\n        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)  # Batch normalization\n\n    def forward(self, inputs, unq_inv):\n        torch.backends.cudnn.enabled = False  # Disable cuDNN for compatibility reasons\n        x = self.linear(inputs)  # Apply linear transformation\n        x = self.norm(x)  # Apply batch normalization\n        x = F.relu(x)  # Apply ReLU activation\n        torch.backends.cudnn.enabled = True  # Re-enable cuDNN\n\n        # max pooling\n        feat_max = torch_scatter.scatter_max(x, unq_inv, dim=0)[0]  # Perform scatter max pooling\n        x_max = feat_max[unq_inv]  # Gather the max features for each point\n\n        if self.last_vfe:\n            return x_max  # If this is the last layer, return the max features\n        else:\n            x_concatenated = torch.cat([x, x_max], dim=1)  # Otherwise, concatenate the original and max features\n            return x_concatenated  # Return the concatenated features\n\n\nThe PFNLayer Class class implements a layer of the Pillar Feature Net. It includes a linear transformation, batch normalization, and ReLU activation. It also performs scatter max pooling to extract the maximum features per pillar. The PFNLayer class essentially extracts features from point clouds.\n\n\nThis class extends basic PyTorch components to perform specific operations for 3D point cloud data. The main differences include:\n\nScatter Operations: Instead of using standard pooling operations like max pooling, the class employs torch_scatter.scatter_max, which is crucial for handling sparse point cloud data. PyTorch does not natively support this type of scatter operation.\nCustom Feature Concatenation: The concatenation of original and max-pooled features, dependent on whether the layer is the last in the network, is a custom behavior not found in standard PyTorch layers.\n\n\n\n\n\nPooling in Point Clouds: PFNLayer abstracts the complexity of performing scatter-based max pooling, which is not straightforward in PyTorch’s standard API.\nConditional Feature Concatenation: It abstracts the logic of deciding whether or not to concatenate features based on whether it’s the last layer.\nCustom Batch Normalization: The use of nn.BatchNorm1d with a custom eps and momentum is abstracted, so the user doesn’t need to handle these details directly.\n\n\n\n\n\nLimited Flexibility: The class is specialized for the PointPillars architecture. A more direct PyTorch implementation might allow for greater flexibility in terms of pooling strategies, feature transformations, and layer configurations.\nHardcoded Operations: The specific operations (like the choice of ReLU activation, BatchNorm1d, and scatter_max) are hardcoded. A more direct PyTorch approach would allow these choices to be more easily customized or replaced with alternatives.\n\n\n# Example data\ninputs = torch.rand(10, 64)\nunq_inv = torch.randint(0, 2, (10,))\n\n\nprint(f'Input shape: {inputs.shape}')\nprint(f'Unique inverse shape: {unq_inv.shape}')\n\n# Initialize PFNLayer\npfn_layer = PFNLayer(in_channels=64, out_channels=128, last_layer=False)\n\n# Forward pass\noutput = pfn_layer(inputs, unq_inv)\n\nprint(f'Output shape: {output.shape}')\n\n# Plot input features and output features\nfig, ax = plt.subplots(2, 1, figsize=(10, 8))\nax[0].imshow(inputs.numpy(), aspect='auto', cmap='viridis')\nax[0].set_title('Input Features')\nax[0].set_xlabel('Feature Dimension')\nax[0].set_ylabel('Point Index')\n\nax[1].imshow(output.detach().numpy(), aspect='auto', cmap='viridis')\nax[1].set_title('Output Features')\nax[1].set_xlabel('Feature Dimension')\nax[1].set_ylabel('Point Index')\n\nplt.tight_layout()\nplt.show()\n\nInput shape: torch.Size([10, 64])\nUnique inverse shape: torch.Size([10])\nOutput shape: torch.Size([10, 128])\n\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n\n\n PillarNet (num_input_features:int, voxel_size:list, pc_range:list)\n\n*PillarNet. The network performs dynamic pillar scatter that convert point cloud into pillar representation and extract pillar features\nReference: PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784) End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)*\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nnum_input_features\nint\nNumber of input features\n\n\nvoxel_size\nlist\nA list that defines the size of the voxels (grids) in the x and y dimensions.\n\n\npc_range\nlist\nA list defining the range of the point cloud data in the x and y dimensions. This is used to filter and normalize the point cloud data. Only utilize x and y min\n\n\n\n\n\nExported source\nclass PillarNet(nn.Module):\n    \"\"\"\n    PillarNet.\n    The network performs dynamic pillar scatter that convert point cloud into pillar representation\n    and extract pillar features\n\n    Reference:\n    PointPillars: Fast Encoders for Object Detection from Point Clouds (https://arxiv.org/abs/1812.05784)\n    End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds (https://arxiv.org/abs/1910.06528)\n    \"\"\"\n\n    def __init__(self,\n                 num_input_features: int, # Number of input features\n                 voxel_size: list, # A list that defines the size of the voxels (grids) in the x and y dimensions.\n                 pc_range: list, # A list defining the range of the point cloud data in the x and y dimensions. This is used to filter and normalize the point cloud data. Only utilize x and y min\n                 ):\n        super().__init__()\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n    def forward(self,\n                points: torch.Tensor # Points in LiDAR coordinate, shape: (N, d), format: batch_id, x, y, z, feat1, ...\n                ):\n\n        device = points.device\n        dtype = points.dtype\n\n        # discard out of range points\n        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n                     )/self.voxel_size  # x,  y, z\n        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(points).to(device)\n        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n\n        points_coords = (\n            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n\n        mask = reduce(torch.logical_and, (points_coords[:, 0] &gt;= 0,\n                                          points_coords[:, 0] &lt; grid_size[0],\n                                          points_coords[:, 1] &gt;= 0,\n                                          points_coords[:, 1] &lt; grid_size[1]))\n\n        points = points[mask]\n        points_coords = points_coords[mask]\n\n        points_coords = points_coords.long()\n        batch_idx = points[:, 0:1].long()\n\n        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n        unq = unq.int()\n\n        points_mean_scatter = torch_scatter.scatter_mean(\n            points[:, 1:4], unq_inv, dim=0)\n\n        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n\n        # Find distance of x, y, and z from pillar center\n        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n\n        # Combine together feature decorations\n        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n\n        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]\n\n\nThe PillarNet class is a neural network module designed for converting LiDAR point clouds into a structured pillar representation. It is based on the concepts introduced in papers such as PointPillars: Fast Encoders for Object Detection from Point Clouds and End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds. This network performs a dynamic pillar scatter operation, which helps in efficiently extracting pillar features from unstructured point cloud data.\n\n\n\nForward Method:\n\nPoint Filtering: The input points are filtered based on the specified pc_range to discard points that are outside the defined range.\nGrid Size Calculation: The grid size is computed based on the voxel_size and pc_range, which determines the resolution of the voxelization.\nPoint Coordinates Normalization: The point coordinates are normalized with respect to the voxel size and point cloud range, converting the continuous coordinates into discrete voxel indices.\nPoint Clustering: The points are clustered into pillars based on their voxel indices.\nFeature Decoration: Additional features are calculated, including:\n\nf_cluster: The distance of each point from the mean of its cluster (pillar).\nf_center: The distance of each point from the center of its respective voxel.\n\nFeature Aggregation: The original point features, along with the newly computed features (f_cluster and f_center), are concatenated to form the final feature set for each point.\nReturn Values: The method returns the final point features, the unique pillar indices, the inverse indices used for aggregation, and the grid size.\n\n\n\n\n\n\nAbstraction of Point Cloud Operations:\n\nThe PillarNet class abstracts the voxelization and feature extraction process, which would otherwise require manual implementation using basic PyTorch operations. It provides a higher-level interface to work with point cloud data, simplifying the process of converting point clouds into pillar-based representations.\n\nIntegration of Advanced Operations:\n\nOperations like torch_scatter.scatter_mean are used for efficiently computing cluster mean features. These operations are not directly available in standard PyTorch and require the use of external libraries like torch_scatter.\n\nGrid Management:\n\nThe class handles the computation of grid size and point normalization internally, which would otherwise require manual calculation and management in a more direct implementation.\n\n\n\n\n\n\nFlexibility:\n\nWhile PillarNet provides a streamlined approach to pillar feature extraction, it may lack the flexibility needed for more customized operations or for handling point cloud data in formats that differ from the assumptions made in the class.\n\nLimited Customization:\n\nThe class is designed for a specific type of pillar-based representation. Users who require different types of voxelization or feature extraction strategies may find it limiting and may need to modify or extend the class, which could be more cumbersome than implementing a direct approach from scratch.\n\n\n\n# Create a sample point cloud with shape (N, d)\n# Here, d includes batch_id, x, y, z, and some additional features (e.g., intensity)\npoints = torch.tensor([\n    [0, 1.0, 2.0, 3.0, 0.5],\n    [0, 2.5, 3.5, 4.5, 0.6],\n    [1, 5.0, 6.0, 7.0, 0.7],\n    [1, 8.0, 9.0, 10.0, 0.8]\n], dtype=torch.float32)\n\n# Define the number of input features (excluding batch_id, x, y, z)\nnum_input_features = points.shape[1] - 1\n\n# Define voxel size (x_size, y_size, z_size)\nvoxel_size = [0.5, 0.5, 0.5]\n\n# Define point cloud range (x_min, y_min, z_min, x_max, y_max, z_max)\npc_range = [0, 0, 0, 10, 10, 10]\n\n# Create an instance of PillarNet\npillar_net = PillarNet(num_input_features, voxel_size, pc_range)\n\n# Forward pass with the sample points\nfeatures, unique_voxel_indices, inverse_indices, grid_size = pillar_net(points)\n\n# Print the results\nprint(\"Features:\", features)\nprint(\"Unique Voxel Indices:\", unique_voxel_indices)\nprint(\"Inverse Indices:\", inverse_indices)\nprint(\"Grid Size:\", grid_size)\n\nFeatures: tensor([[ 1.0000,  2.0000,  3.0000,  0.5000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 2.5000,  3.5000,  4.5000,  0.6000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 5.0000,  6.0000,  7.0000,  0.7000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500],\n        [ 8.0000,  9.0000, 10.0000,  0.8000,  0.0000,  0.0000,  0.0000, -0.2500,\n         -0.2500]])\nUnique Voxel Indices: tensor([[ 0,  4,  2],\n        [ 0,  7,  5],\n        [ 1, 12, 10],\n        [ 1, 18, 16]], dtype=torch.int32)\nInverse Indices: tensor([0, 1, 2, 3])\nGrid Size: [20 20]\n\n\n\nsource\n\n\n\n\n\n PillarFeatureNet (num_input_features:int, num_filters:list,\n                   voxel_size:list, pc_range:list, norm_cfg:None)\n\nPillar Feature Net. The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a similar role to SECOND’s second.pytorch.voxelnet.VoxelFeatureExtractor.\n\n\n\n\nType\nDetails\n\n\n\n\nnum_input_features\nint\nNumber of input features\n\n\nnum_filters\nlist\nNumber of features in each of the N PFNLayers\n\n\nvoxel_size\nlist\nSize of voxels, only utilize x and y size\n\n\npc_range\nlist\nPoint cloud range, only utilize x and y min\n\n\nnorm_cfg\nNone\nNormalization config\n\n\n\n\n\nExported source\nclass PillarFeatureNet(nn.Module):\n    \"\"\"\n    Pillar Feature Net.\n    The network prepares the pillar features and performs forward pass through PFNLayers. This net performs a\n    similar role to SECOND's second.pytorch.voxelnet.VoxelFeatureExtractor.\n    \"\"\"\n    def __init__(\n        self,\n        num_input_features: int, # Number of input features\n        num_filters: list, # Number of features in each of the N PFNLayers\n        voxel_size: list, # Size of voxels, only utilize x and y size\n        pc_range: list, # Point cloud range, only utilize x and y min\n        norm_cfg:None, # Normalization config\n    ):\n\n        super().__init__()\n        assert len(num_filters) &gt; 0\n        num_input_features += 5\n\n        # Create PillarFeatureNet layers\n        num_filters = [num_input_features] + list(num_filters)\n        pfn_layers = []\n        for i in range(len(num_filters) - 1):\n            in_filters = num_filters[i]\n            out_filters = num_filters[i + 1]\n            if i &lt; len(num_filters) - 2:\n                last_layer = False\n            else:\n                last_layer = True\n            pfn_layers.append(\n                PFNLayer(\n                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n                )\n            )\n        self.pfn_layers = nn.ModuleList(pfn_layers)\n\n        self.feature_output_dim = num_filters[-1]\n\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n        self.voxelization = PillarNet(num_input_features, voxel_size, pc_range)\n\n    def forward(self, points):\n        features, coords, unq_inv, grid_size = self.voxelization(points)\n        # Forward pass through PFNLayers\n        for pfn in self.pfn_layers:\n            features = pfn(features, unq_inv)  # num_points, dim_feat\n\n        feat_max = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n\n        return feat_max, coords, grid_size\n\n\nThe PillarFeatureNet class is designed for preparing pillar features from the point cloud data, used in 3D object detection tasks in LiDAR-based perception systems. This network handles the transformation of raw point cloud data into a more structured and feature-rich representation, which will be essential for further processing in the object detection.\n\n\n\nInitialization (__init__ method):\n\nInput Features Augmentation: The class initializes by augmenting the input features with additional spatial information, increasing the dimensionality by 5 (which might include features like the relative position of points within a pillar, etc.).\nLayer Creation: It then creates a sequence of layers, PFNLayers, which perform the feature extraction. The number of layers and their configuration are determined by the num_filters parameter.\nVoxelization: The class initializes the voxelization process, converting the raw point cloud data into a voxel grid with specified voxel_size and within a specified pc_range (point cloud range). This voxelization is essential for handling irregular point clouds by grouping points into a regular grid structure, called “pillars.”\n\nForward Pass (forward method):\n\nVoxelization: The input point cloud is first voxelized, where the points are grouped into pillars, and features are extracted.\nPFN Layers: The extracted features are passed through the PFNLayers, where each layer performs a certain amount of processing. These layers typically involve operations like PointNet-style feature learning.\nMax Pooling: Finally, the features are aggregated using a max-pooling operation (torch_scatter.scatter_max), which pools the features across the points in each pillar to get a fixed-size feature vector for each pillar.\n\n\n\n\n\nThis class abstracts several operations that would require more manual implementation in pure PyTorch. For instance, voxelization and the subsequent grouping of points into pillars are handled internally by the PillarNet class.\nThe abstraction can reduce flexibility. If you need to tweak certain aspects of the feature extraction or voxelization process, this might be harder to do compared to a more manual approach where every operation is explicit.\n\n# Define input parameters for PillarFeatureNet\n# Mock values for demonstration purposes\nnum_input_features = 3\nnum_filters = [64, 128]\nvoxel_size = [0.2, 0.2, 0.2]\npc_range = [0, 0, 0, 50, 50, 50]\nnorm_cfg = None\n\n# Instantiate the PillarFeatureNet\npillar_feature_net = PillarFeatureNet(\n    num_input_features=num_input_features,\n    num_filters=num_filters,\n    voxel_size=voxel_size,\n    pc_range=pc_range,\n    norm_cfg=norm_cfg\n)\n\n# Create some dummy input points\n# Each point might have x, y, z, intensity, etc.\nnum_points = 100  # Number of points in the point cloud\npoints = torch.rand(num_points, num_input_features + 1)  # Random points (x, y, z, intensity)\n\n# Run a forward pass\nfeat_max, coords, grid_size = pillar_feature_net(points)\n\n# Print the outputs\nprint(\"Max Features:\\n\", feat_max)\nprint(\"Voxel Coordinates:\\n\", coords)\nprint(\"Grid Size:\\n\", grid_size)\n\nMax Features:\n tensor([[0.0000, 1.9903, 1.5468,  ..., 0.1351, 1.5962, 1.8613],\n        [0.0953, 0.8951, 0.5835,  ..., 0.3777, 0.4148, 1.1723],\n        [0.0000, 0.0000, 0.0628,  ..., 0.0000, 0.0000, 0.0000],\n        ...,\n        [0.0000, 0.6913, 0.4461,  ..., 1.3929, 0.0906, 1.3596],\n        [0.0000, 0.0000, 0.0000,  ..., 0.1496, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.0000,  ..., 1.5835, 0.0000, 0.0030]],\n       grad_fn=&lt;CppNode&lt;ScatterMax&gt;&gt;)\nVoxel Coordinates:\n tensor([[0, 0, 0],\n        [0, 1, 0],\n        [0, 2, 0],\n        [0, 3, 0],\n        [0, 4, 0],\n        [0, 0, 1],\n        [0, 1, 1],\n        [0, 2, 1],\n        [0, 3, 1],\n        [0, 4, 1],\n        [0, 0, 2],\n        [0, 1, 2],\n        [0, 2, 2],\n        [0, 3, 2],\n        [0, 4, 2],\n        [0, 0, 3],\n        [0, 1, 3],\n        [0, 2, 3],\n        [0, 3, 3],\n        [0, 4, 3],\n        [0, 0, 4],\n        [0, 1, 4],\n        [0, 2, 4],\n        [0, 3, 4],\n        [0, 4, 4]], dtype=torch.int32)\nGrid Size:\n [250 250]\n\n\n\n# Create some dummy input points\nnum_points = 100  # Number of points in the point cloud\npoints = torch.rand(num_points, num_input_features + 1)  # Random points (x, y, z, intensity)\n\n# Visualize the input using matplotlib\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot point cloud coordinates\nax.scatter(points[:, 0].numpy(), points[:, 1].numpy(), points[:, 2].numpy(), c='r', marker='o')\n\n# Label axes\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Title\nax.set_title('Point Cloud Coordinates')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# Visualize the output using matplotlib\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\n# Plot voxel coordinates\nax.scatter(coords[:, 0].numpy(), coords[:, 1].numpy(), coords[:, 2].numpy(), c='b', marker='o')\n\n# Label axes\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Title\nax.set_title('Voxel Coordinates')\n\nplt.show()",
    "crumbs": [
      "Model: readers"
    ]
  },
  {
    "objectID": "model_readers.html#voxel-encoder",
    "href": "model_readers.html#voxel-encoder",
    "title": "Model: readers",
    "section": "Voxel Encoder",
    "text": "Voxel Encoder\n\nsource\n\nDynamicVoxelEncoder\n\n DynamicVoxelEncoder ()\n\nDynamic version of VoxelFeatureExtractorV3\n\n\nExported source\nclass DynamicVoxelEncoder(nn.Module):\n    \"\"\"\n    Dynamic version of VoxelFeatureExtractorV3\n    \"\"\"\n\n    def __init__(self):\n        super(DynamicVoxelEncoder, self).__init__()\n\n    def forward(self, inputs, unq_inv):\n        features = torch_scatter.scatter_mean(inputs, unq_inv, dim=0)\n\n        return features\n\n\nThe DynamicVoxelEncoder class is a module that serves as a custom feature extractor for voxel-based data, utilizing dynamic computations to process input features.\nThe method performs a scatter operation using the torch_scatter.scatter_mean function, which computes the mean of features that belong to the same group, as indicated by the unq_inv tensor. The result is a tensor of aggregated features, where each feature corresponds to a unique group.\nThis design makes the DynamicVoxelEncoder easy to integrate into larger architectures that require voxel-based feature extraction, such as 3D object detection networks.\n\nsource\n\n\nVoxelNet\n\n VoxelNet (voxel_size, pc_range)\n\n*Dynamic voxelization for point clouds\nThis class performs dynamic voxelization on input point clouds. It converts point coordinates into voxel grid coordinates and removes points that fall outside the specified range.*\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nvoxel_size\nThe size of each voxel in the grid. It is expected to be a 3-element list or array that defines the size of the voxel in the x, y, and z dimensions.\n\n\npc_range\nThe range of the point cloud. It’s a 6-element list or array that specifies the minimum and maximum bounds in the x, y, and z dimensions.\n\n\n\n\n\nExported source\nclass VoxelNet(nn.Module):\n    \"\"\"\n    Dynamic voxelization for point clouds\n\n    This class performs dynamic voxelization on input point clouds.\n    It converts point coordinates into voxel grid coordinates and removes points that fall outside the specified range.\n    \"\"\"\n\n    def __init__(self,\n                voxel_size, # The size of each voxel in the grid. It is expected to be a 3-element list or array that defines the size of the voxel in the x, y, and z dimensions.\n                pc_range # The range of the point cloud. It's a 6-element list or array that specifies the minimum and maximum bounds in the x, y, and z dimensions.\n                ):\n        super().__init__()\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n    def forward(self, points):\n        \"\"\"\n        points: Tensor: (N, d), batch_id, x, y, z, ...\n        \"\"\"\n        device = points.device\n\n        # voxel range of x, y, z\n        grid_size = (self.pc_range[3:] - self.pc_range[:3]) / self.voxel_size\n        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(points).to(device)\n        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n\n        points_coords = (\n            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)  # x, y, z\n\n        mask = reduce(torch.logical_and, (points_coords[:, 0] &gt;= 0,\n                                          points_coords[:, 0] &lt; grid_size[0],\n                                          points_coords[:, 1] &gt;= 0,\n                                          points_coords[:, 1] &lt; grid_size[1],\n                                          points_coords[:, 2] &gt;= 0,\n                                          points_coords[:, 2] &lt; grid_size[2]))  # remove the points out of range\n\n        points = points[mask]\n        points_coords = points_coords[mask]\n\n        points_coords = points_coords.long()\n        batch_idx = points[:, 0:1].long()\n        point_index = torch.cat((batch_idx, points_coords), dim=1)\n\n        unq, unq_inv = torch.unique(point_index, return_inverse=True, dim=0)\n        unq = unq.int()\n\n        features = points[:, 1:]\n\n        return features, unq[:, [0, 3, 2, 1]], unq_inv, grid_size[[2, 1, 0]]\n\n\nThe VoxelNet class is designed for performing dynamic voxelization on 3D point clouds. Voxelization is a process where a 3D space is divided into a grid of equally sized cubes (voxels), and points from the point cloud are assigned to these voxels based on their spatial coordinates.\nHere’s a breakdown of the core functionality of the VoxelNet class:\n\nForward Method:\n\nThe forward method takes as input a tensor of points. This tensor has the shape (N, d) where N is the number of points and d is the number of features per point. The first feature is the batch_id, and the next three are the coordinates x, y, z. Additional features can be present as well.\nThe method calculates the size of the voxel grid (grid_size) by dividing the range of the point cloud by the voxel size.\nIt then computes the voxel grid coordinates (points_coords) for each point by subtracting the minimum point cloud range and dividing by the voxel size.\nPoints that fall outside the specified range (outside the grid) are filtered out using a mask.\nThe remaining points and their corresponding voxel coordinates are then converted to integer indices.\nThe method creates a unique index (point_index) for each point using its batch_id and voxel grid coordinates.\nIt finds unique voxel indices and returns the following:\n\nfeatures: The features of the points that remain after filtering.\nunq: The unique voxel indices corresponding to the unique points.\nunq_inv: The inverse of the unique voxel indices, mapping back to the original point indices.\ngrid_size: The size of the voxel grid.\n\n\n\n\n# Create an instance of VoxelNet\nvoxel_size = [0.5, 0.5, 0.5]\npc_range = [0, 0, 0, 10, 10, 10]\nvoxel_net = VoxelNet(voxel_size, pc_range)\n\n# Generate synthetic point cloud data\nbatch_size = 1\nnum_points = 10\npoints = torch.cat((\n    torch.zeros(num_points, 1),  # batch_id\n    torch.rand(num_points, 3) * 10  # x, y, z\n), dim=1)\n\n# Forward pass\nfeatures, voxel_coords, unq_inv, grid_size = voxel_net(points)\n\n# Print input and output\nprint(\"Input Points:\")\nprint(points)\nprint(\"\\nFeatures:\")\nprint(features)\nprint(\"\\nVoxel Coordinates:\")\nprint(voxel_coords)\nprint(\"\\nUnique Inverse Indices:\")\nprint(unq_inv)\nprint(\"\\nGrid Size:\")\nprint(grid_size)\n\n# Visual demonstration using matplotlib\n# Plot original and voxelized points\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(points[:, 1], points[:, 2], points[:, 3], c='r', marker='o', label='Original Points')\n\n# Convert voxel coordinates to real world coordinates\nvoxel_size = np.array([0.5, 0.5, 0.5])\npc_range_min = np.array([0, 0, 0])\nreal_coords = voxel_coords[:, 1:4].numpy() * voxel_size + pc_range_min\n\nax.scatter(real_coords[:, 0], real_coords[:, 1], real_coords[:, 2], c='b', marker='^', label='Voxelized Points')\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\nplt.legend()\nplt.title('Voxelization of Point Cloud')\nplt.show()\n\nInput Points:\ntensor([[0.0000, 5.1454, 2.3374, 9.9073],\n        [0.0000, 8.4996, 3.2674, 0.0379],\n        [0.0000, 0.7200, 9.5793, 4.5018],\n        [0.0000, 2.4532, 3.3480, 0.9841],\n        [0.0000, 5.1391, 0.7281, 4.1405],\n        [0.0000, 9.7150, 0.4090, 0.0740],\n        [0.0000, 8.9659, 8.5211, 5.7163],\n        [0.0000, 4.9131, 7.7118, 0.5395],\n        [0.0000, 5.2191, 0.7674, 2.3421],\n        [0.0000, 2.7364, 1.3320, 4.8849]])\n\nFeatures:\ntensor([[5.1454, 2.3374, 9.9073],\n        [8.4996, 3.2674, 0.0379],\n        [0.7200, 9.5793, 4.5018],\n        [2.4532, 3.3480, 0.9841],\n        [5.1391, 0.7281, 4.1405],\n        [9.7150, 0.4090, 0.0740],\n        [8.9659, 8.5211, 5.7163],\n        [4.9131, 7.7118, 0.5395],\n        [5.2191, 0.7674, 2.3421],\n        [2.7364, 1.3320, 4.8849]])\n\nVoxel Coordinates:\ntensor([[ 0,  9, 19,  1],\n        [ 0,  1,  6,  4],\n        [ 0,  9,  2,  5],\n        [ 0,  1, 15,  9],\n        [ 0,  4,  1, 10],\n        [ 0,  8,  1, 10],\n        [ 0, 19,  4, 10],\n        [ 0,  0,  6, 16],\n        [ 0, 11, 17, 17],\n        [ 0,  0,  0, 19]], dtype=torch.int32)\n\nUnique Inverse Indices:\ntensor([6, 7, 0, 1, 5, 9, 8, 3, 4, 2])\n\nGrid Size:\n[20 20 20]\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nVoxelFeatureNet\n\n VoxelFeatureNet (voxel_size, pc_range)\n\nThis class performs dynamic voxelization of point clouds and then encodes the voxel features using DynamicVoxelEncoder.\n\n\n\n\nDetails\n\n\n\n\nvoxel_size\nsize of voxel\n\n\npc_range\npoint cloud range\n\n\n\n\n\nExported source\nclass VoxelFeatureNet(nn.Module):\n    \"\"\"\n    This class performs dynamic voxelization of point clouds and then encodes the voxel features using DynamicVoxelEncoder.\n    \"\"\"\n    def __init__(self,\n                voxel_size, # size of voxel\n                pc_range # point cloud range\n                ):\n        super().__init__()\n\n        self.voxelization = VoxelNet(voxel_size, pc_range)\n        self.voxel_encoder = DynamicVoxelEncoder()\n\n    def forward(self, points):\n        features, coords, unq_inv, grid_size = self.voxelization(points)\n\n        features = self.voxel_encoder(features, unq_inv)\n\n        return features, coords, grid_size\n\n\nThe VoxelFeatureNet class is designed to perform dynamic voxelization and then encode the voxelized features. It integrates the functionality of the VoxelNet and DynamicVoxelEncoder classes to provide a streamlined process for transforming raw point cloud data into meaningful features that can be used in further processing, such as in neural networks for tasks like object detection, segmentation, or 3D scene understanding.\n\nHow VoxelFeatureNet Works:\n\nVoxelNet Integration:\n\n\nVoxelNet is responsible for dynamically voxelizing the input point clouds.\nIt first takes raw point cloud data (points), which includes the batch index and the coordinates (x, y, z), along with other associated features.\nThe point cloud data is mapped onto a voxel grid based on the specified voxel_size and pc_range.\nPoints that fall outside the specified range are filtered out.\nThe voxel grid coordinates for each point are calculated and stored, and duplicate voxel indices are merged, with unique voxel indices being identified (unq), and a mapping from points to voxels is maintained (unq_inv).\n\n\nDynamicVoxelEncoder Integration:\n\n\nOnce the voxelization process is completed, the DynamicVoxelEncoder takes over.\nThis encoder aggregates the features of all points that fall into the same voxel by computing the mean of the features (scatter_mean function).\nThis step effectively reduces the point cloud data into a more compact voxel-based representation, where each voxel holds the average feature of all points within it.\n\n\nReturn Values:\n\n\nThe class returns the aggregated voxel features, the voxel coordinates (coords), and the grid size (grid_size), which can be used for further processing in downstream tasks.\n\n\nOverall, VoxelFeatureNet provides a structured way to convert raw point cloud data into voxelized features.\n\n# Create an instance of VoxelFeatureNet\nvoxel_size = [0.1, 0.1, 0.1]\npc_range = [0, -40, -3, 70.4, 40, 1]\n\nvoxel_feature_net = VoxelFeatureNet(voxel_size, pc_range)\n\n# Generate synthetic point cloud data\n# Format: [batch_id, x, y, z, ...]\nbatch_size = 2\nnum_points = 500\nnum_features = 4  # x, y, z, intensity\n\npoints = torch.cat([\n    torch.randint(0, batch_size, (num_points, 1)).float(),  # batch_id\n    torch.rand(num_points, 3) * torch.tensor([70.4, 80, 4]) - torch.tensor([0, 40, 3]),  # x, y, z\n    torch.rand(num_points, num_features - 3)  # additional features\n], dim=1)\n\n# Forward pass through the VoxelFeatureNet\nfeatures, coords, grid_size = voxel_feature_net(points)\n\n\n# Visual demonstration using matplotlib\ndef plot_voxel_grid(coords, grid_size):\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Plot the voxel grid\n    ax.set_xlim(0, grid_size[2])\n    ax.set_ylim(0, grid_size[1])\n    ax.set_zlim(0, grid_size[0])\n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n\n    # Plot the occupied voxels\n    for coord in coords:\n        ax.scatter(coord[1], coord[2], coord[3], c='b', marker='o')\n\n    plt.title('Voxel Grid')\n    plt.show()\n\n# Plot the voxel grid with occupied coordinates\nplot_voxel_grid(coords, grid_size)",
    "crumbs": [
      "Model: readers"
    ]
  },
  {
    "objectID": "model_readers.html#mvf-enconder",
    "href": "model_readers.html#mvf-enconder",
    "title": "Model: readers",
    "section": "MVF Enconder",
    "text": "MVF Enconder\n\nsource\n\nPointNet\n\n PointNet (in_channels:int, out_channels:int)\n\nLinear Process for point feature\n\n\n\n\nType\nDetails\n\n\n\n\nin_channels\nint\nNumber of input channels\n\n\nout_channels\nint\nNumber of output channels\n\n\n\n\n\nExported source\nclass PointNet(nn.Module):\n    \"\"\"\n    Linear Process for point feature\n    \"\"\"\n\n    def __init__(self,\n                in_channels:int, # Number of input channels\n                out_channels:int # Number of output channels\n                ):\n        super().__init__()\n        self.linear = nn.Linear(in_channels, out_channels, bias=False)\n        self.norm = nn.BatchNorm1d(out_channels, eps=1e-3, momentum=0.01)\n\n    def forward(self, points):\n        torch.backends.cudnn.enabled = False\n        x = self.linear(points)\n        x = self.norm(x)\n        x = F.relu(x)\n        torch.backends.cudnn.enabled = True\n\n        return x\n\n\nThe PointNet class is a simple neural network module designed for processing point features. This class abstracts a linear transformation followed by normalization and activation.\nThe core functionality of the PointNet class:\n\nLinear Transformation: It transforms the input features (point features) using a linear layer, effectively mapping the input dimension (in_channels) to the desired output dimension (out_channels).\nNormalization: After the linear transformation, the features are normalized using Batch Normalization to ensure stable and faster training by maintaining consistent feature distributions.\nActivation: The normalized features are passed through a ReLU activation function, introducing non-linearity and enabling the network to model complex relationships between input features.\n\nFixed Architecture:\n\nThe class is hardcoded to a single linear layer followed by batch normalization and ReLU activation. This limits its flexibility if more complex architectures or other types of layers (e.g., convolutional layers, more sophisticated non-linearities) are needed. Also it isn’t possible to change the number of layers or the activation function, nor the epsilon and momentum values for batch normalization, since they are hardcoded.\n\n\nsource\n\n\nPillarVoxelNet\n\n PillarVoxelNet (voxel_size, pc_range)\n\nThis class implements the voxelization process, converting point clouds into voxel grid indices and computing features for each point relative to the voxel grid.\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nvoxel_size\nSize of of each voxel in the grid, only utilize x and y size.\n\n\npc_range\nPoint cloud range. Only utilize x and y min.\n\n\n\n\n\nExported source\nclass PillarVoxelNet(nn.Module):\n    \"\"\"\n    This class implements the voxelization process, converting point clouds into voxel grid indices and computing features for each point relative to the voxel grid.\n    \"\"\"\n    def __init__(self,\n                voxel_size, # Size of of each voxel in the grid, only utilize x and y size.\n                pc_range # Point cloud range. Only utilize x and y min.\n                ):\n        super().__init__()\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n    def forward(self, points):\n        device = points.device\n        dtype = points.dtype\n\n        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n                     )/self.voxel_size  # x,  y, z\n        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(points).to(device)\n        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n\n        points_coords = (\n            points[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)   # x, y, z\n        points_coords[:, 0] = torch.clamp(\n            points_coords[:, 0], 0, grid_size[0] - 1)\n        points_coords[:, 1] = torch.clamp(\n            points_coords[:, 1], 0, grid_size[1] - 1)\n        points_coords[:, 2] = torch.clamp(\n            points_coords[:, 2], 0, grid_size[2] - 1)\n\n        points_coords = points_coords.long()\n        batch_idx = points[:, 0:1].long()\n\n        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n        unq = unq.int()        # breakpoint()\n\n        points_mean_scatter = torch_scatter.scatter_mean(\n            points[:, 1:4], unq_inv, dim=0)\n\n        f_cluster = points[:, 1:4] - points_mean_scatter[unq_inv]\n\n        # Find distance of x, y, and z from pillar center\n        f_center = points[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n                                     voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n\n        # Combine together feature decorations\n        features = torch.cat([points[:, 1:], f_cluster, f_center], dim=-1)\n\n        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]\n\n\nThe PillarVoxelNet class is a component in the process of converting raw LiDAR point cloud data into a structured grid format for 3D object detection tasks. It is designed to convert unstructured LiDAR point cloud data into a structured voxel grid. By transforming the raw point data into a format that can be processed by the subsequent network layers, the class enables the network to capture spatial relationships.\nComponents of the Class:\nForward Pass (forward Method):\nThe forward method takes in a batch of point cloud data and performs the following steps:\n\nGrid Size Calculation: The grid size (number of voxels along each axis) is calculated by dividing the range of the point cloud by the voxel size. This defines how many voxels fit into the defined point cloud range.\nVoxelization: Each point in the point cloud is converted to a coordinate within the voxel grid. This is done by normalizing the point coordinates by the voxel size and adjusting them relative to the grid.\nThe coordinates are then clamped to ensure they lie within the bounds of the grid.\nUnique Voxel Identification: The points are then assigned to unique voxels based on their calculated voxel coordinates. The torch.unique function is used to identify unique voxel indices and the corresponding inverse indices.\nFeature Computation:\n\nPoint-to-Pillar Mean: The mean position of points within each voxel (or pillar) is calculated.\nCluster Features (f_cluster): These features represent the offset of each point from the mean position of its corresponding voxel.\nCentering Features (f_center): These features represent the distance of each point from the center of the voxel grid.\n\nFinally, the original point features, cluster features, and centering features are concatenated to form the final set of features for each point.\nReturn Values: The method returns:\n\nfeatures: The enhanced point features incorporating spatial relationships.\nunq: The unique voxel indices.\nunq_inv: Inverse indices for reconstructing the original point set.\ngrid_size: The size of the voxel grid in the x and y dimensions.\n\n\n\n# Create an instance of PillarVoxelNet\nvoxel_size = [0.2, 0.2, 4]\npc_range = [0, 0, -3, 70.4, 40, 1]\npillar_voxel_net = PillarVoxelNet(voxel_size, pc_range)\n\n# Generate synthetic point cloud data\nbatch_size = 1\nnum_points = 10\npoints = torch.rand(num_points, 4)\npoints[:, 0] = 0  # All points belong to the same batch\npoints[:, 1] = points[:, 1] * (pc_range[3] - pc_range[0]) + pc_range[0]\npoints[:, 2] = points[:, 2] * (pc_range[4] - pc_range[1]) + pc_range[1]\npoints[:, 3] = points[:, 3] * (pc_range[5] - pc_range[2]) + pc_range[2]\n\n# Forward pass\nfeatures, unq, unq_inv, grid_size = pillar_voxel_net(points)\n\n# Print the results\nprint(\"Features:\")\nprint(features)\nprint(\"\\nUnique Pillar Indices (Batch, X, Y):\")\nprint(unq)\nprint(\"\\nGrid Size (Y, X):\")\nprint(grid_size)\n\nFeatures:\ntensor([[ 2.3704e+01,  2.8256e+01, -1.9798e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  3.5553e-03, -4.4041e-02],\n        [ 1.0087e+01,  6.1095e+00, -3.1646e-01,  0.0000e+00,  0.0000e+00,\n          0.0000e+00, -1.3090e-02,  9.5429e-03],\n        [ 5.4948e+01,  1.8746e+01, -2.3587e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  4.7882e-02,  4.5704e-02],\n        [ 5.5123e+01,  1.7003e+01, -2.0925e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  2.3098e-02, -9.7160e-02],\n        [ 5.7509e+01,  1.8222e+01, -2.2636e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  9.4337e-03, -7.7551e-02],\n        [ 4.1481e+01,  2.1276e+01, -6.7486e-01,  0.0000e+00,  0.0000e+00,\n          0.0000e+00, -1.9176e-02, -2.3525e-02],\n        [ 9.8798e-01,  3.4320e+01,  1.4192e-01,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  8.7983e-02,  1.9520e-02],\n        [ 5.6135e+01,  2.2593e+01, -1.4039e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  3.4702e-02,  9.2648e-02],\n        [ 6.3527e+01,  2.6058e+00, -2.0011e+00,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  2.6817e-02, -9.4187e-02],\n        [ 5.2936e+01,  1.1646e+01,  8.8765e-01,  0.0000e+00,  0.0000e+00,\n          0.0000e+00,  3.5934e-02, -5.4329e-02]])\n\nUnique Pillar Indices (Batch, X, Y):\ntensor([[  0, 171,   4],\n        [  0,  30,  50],\n        [  0, 141, 118],\n        [  0, 106, 207],\n        [  0,  58, 264],\n        [  0,  93, 274],\n        [  0,  85, 275],\n        [  0, 112, 280],\n        [  0,  91, 287],\n        [  0,  13, 317]], dtype=torch.int32)\n\nGrid Size (Y, X):\n[200 352]\n\n\n\n# Create an instance of PillarVoxelNet\nvoxel_size = [0.2, 0.2, 4]\npc_range = [0, 0, -3, 70.4, 40, 1]\npillar_voxel_net = PillarVoxelNet(voxel_size, pc_range)\n\n# Generate synthetic point cloud data\nbatch_size = 1\nnum_points = 1000\npoints = torch.rand(num_points, 4)\npoints[:, 0] = 0  # All points belong to the same batch\npoints[:, 1] = points[:, 1] * (pc_range[3] - pc_range[0]) + pc_range[0]\npoints[:, 2] = points[:, 2] * (pc_range[4] - pc_range[1]) + pc_range[1]\npoints[:, 3] = points[:, 3] * (pc_range[5] - pc_range[2]) + pc_range[2]\n\n# Forward pass\nfeatures, unq, unq_inv, grid_size = pillar_voxel_net(points)\n\n# # Plot original point cloud\nplt.figure(figsize=(10, 6))\nplt.scatter(points[:, 1], points[:, 2], c=points[:, 3], cmap='viridis', s=5)\nplt.colorbar(label='Z Coordinate')\nplt.title('Original Point Cloud')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n# Plot voxelized point cloud (colored by unique pillar indices)\nunique_pillar_colors = plt.cm.get_cmap('tab20', len(unq))\ncolor_map = unique_pillar_colors(unq_inv.numpy() % len(unq))\nplt.figure(figsize=(10, 6))\nplt.scatter(points[:, 1], points[:, 2], c=color_map, s=5)\nplt.title('Voxelized Point Cloud (Colored by Unique Pillar Indices)')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsource\n\n\nCylinderNet\n\n CylinderNet (voxel_size, pc_range)\n\n*Base class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n\nivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool*\n\n\n\nDetails\n\n\n\n\nvoxel_size\nSize of each voxel, only utilize x and y size\n\n\npc_range\nPoint cloud range, only utilize x and y min\n\n\n\n\n\nExported source\nclass CylinderNet(nn.Module):\n    def __init__(self,\n                voxel_size, # Size of each voxel, only utilize x and y size\n                pc_range # Point cloud range, only utilize x and y min\n                ):\n        super().__init__()\n        self.voxel_size = np.array(voxel_size)\n        self.pc_range = np.array(pc_range)\n\n    def forward(self, points):\n        device = points.device\n        dtype = points.dtype\n        points_x = points[:, 1:2]\n        points_y = points[:, 2:3]\n        points_z = points[:, 3:4]\n        points_phi = torch.atan2(points_y, points_x) / np.pi * 180\n        points_rho = torch.sqrt(points_x ** 2 + points_y ** 2)\n        points_cylinder = torch.cat(\n            (points[:, 0:1], points_phi, points_z, points_rho, points[:, 4:]), dim=-1)\n\n        grid_size = (self.pc_range[3:] - self.pc_range[:3]\n                     )/self.voxel_size  # phi, z, rho\n        grid_size = np.round(grid_size, 0, grid_size).astype(np.int64)\n\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(points).to(device)\n        pc_range = torch.from_numpy(self.pc_range).type_as(points).to(device)\n\n        points_coords = (\n            points_cylinder[:, 1:4] - pc_range[:3].view(-1, 3)) / voxel_size.view(-1, 3)\n        points_coords[:, 0] = torch.clamp(\n            points_coords[:, 0], 0, grid_size[0] - 1)\n        points_coords[:, 1] = torch.clamp(\n            points_coords[:, 1], 0, grid_size[1] - 1)\n        points_coords[:, 2] = torch.clamp(\n            points_coords[:, 2], 0, grid_size[2] - 1)\n        points_coords = points_coords.long()\n        batch_idx = points_cylinder[:, 0:1].long()\n\n        points_index = torch.cat((batch_idx, points_coords[:, :2]), dim=1)\n        unq, unq_inv = torch.unique(points_index, return_inverse=True, dim=0)\n        unq = unq.int()\n\n        points_mean_scatter = torch_scatter.scatter_mean(\n            points_cylinder[:, 1:4], unq_inv, dim=0)\n        f_cluster = points_cylinder[:, 1:4] - points_mean_scatter[unq_inv]\n\n        # Find distance of x, y, and z from pillar center\n        f_center = points_cylinder[:, 1:3] - (points_coords[:, :2].to(dtype) * voxel_size[:2].unsqueeze(0) +\n                                              voxel_size[:2].unsqueeze(0) / 2 + pc_range[:2].unsqueeze(0))\n\n        # Combine together feature decorations\n        features = torch.cat(\n            [points_cylinder[:, 1:], f_cluster, f_center], dim=-1)\n\n        return features, unq[:, [0, 2, 1]], unq_inv, grid_size[[1, 0]]\n\n\nThe CylinderNet class is a neural network module designed to transform the raw point cloud data into a cylindrical coordinate system, then organizes the points into a structured grid based on voxelization.\n\nFunctionality:\n\nConversion to Cylindrical Coordinates: The point cloud data initially represented in Cartesian coordinates (x, y, z) is transformed into cylindrical coordinates (φ, z, ρ), where φ is the azimuthal angle, z is the height, and ρ is the radial distance from the origin. This conversion helps align the data with the sensor’s scanning pattern, making it easier to capture relevant features.\nVoxelization and Grid Size Calculation: After converting to cylindrical coordinates, the point cloud is voxelized based on the specified voxel_size. The grid_size is computed by dividing the range of the point cloud by the voxel size, resulting in the number of voxels along each axis.\nPoint Indexing and Unique Coordinates: The transformed points are then mapped to voxel coordinates. Each point is assigned to a voxel based on its cylindrical coordinates. The code identifies unique voxel indices and computes scatter-based statistics, such as the mean, for each voxel, which are crucial for creating features.\nFeature Decoration: Additional features are computed for each point, including the difference from the voxel center and the cluster center. These features help the network learn more about the local geometry of the point cloud.\n\n\n\nOutput:\n\nfeatures: A tensor containing the decorated features for each point, combining the cylindrical coordinates with additional features like cluster and center distances.\nunq: The unique voxel indices.\nunq_inv: The inverse mapping from unique indices to the original points, used to reconstruct the point-to-voxel relationship.\ngrid_size: The size of the voxel grid, reordered to match the required format.\n\n\nsource\n\n\n\nSingleView\n\n SingleView (in_channels, num_filters, layer_nums, ds_layer_strides,\n             ds_num_filters, kernel_size, mode, voxel_size, pc_range,\n             norm_cfg=None, act_cfg=None)\n\nauthoured by Beijing-jinyu convolution for single view\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nin_channels\n\n\nNumber of input channels\n\n\nnum_filters\n\n\nNumber of features in each of the N PFNLayers\n\n\nlayer_nums\n\n\nNumber of blocks in each layer\n\n\nds_layer_strides\n\n\nStrides of each layer\n\n\nds_num_filters\n\n\nNumber of features in each layer\n\n\nkernel_size\n\n\nKernel size of each layer\n\n\nmode\n\n\nMode of the network\n\n\nvoxel_size\n\n\nSize of voxels, only utilize x and y size\n\n\npc_range\n\n\nPoint cloud range, only utilize x and y min\n\n\nnorm_cfg\nNoneType\nNone\nNormalization config\n\n\nact_cfg\nNoneType\nNone\nActivation config\n\n\n\n\n\nExported source\nclass SingleView(nn.Module):\n    \"\"\"\n    authoured by Beijing-jinyu\n    convolution for single view\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,  # Number of input channels\n                 num_filters,  # Number of features in each of the N PFNLayers\n                 layer_nums,  # Number of blocks in each layer\n                 ds_layer_strides,  # Strides of each layer\n                 ds_num_filters,  # Number of features in each layer\n                 kernel_size,  # Kernel size of each layer\n                 mode,  # Mode of the network\n                 voxel_size,  # Size of voxels, only utilize x and y size\n                 pc_range,  # Point cloud range, only utilize x and y min\n                 norm_cfg=None,  # Normalization config\n                 act_cfg=None,  # Activation config\n                 ):\n        super().__init__()\n        self.mode = mode\n        self.voxel_size = np.array(voxel_size[:2])\n        self.bias = np.array(pc_range[:2])\n        num_filters = [in_channels] + list(num_filters)\n        pfn_layers = []\n        for i in range(len(num_filters) - 1):\n            in_filters = num_filters[i]\n            out_filters = num_filters[i + 1]\n            if i &lt; len(num_filters) - 2:\n                last_layer = False\n            else:\n                last_layer = True\n            pfn_layers.append(\n                PFNLayer(\n                    in_filters, out_filters, norm_cfg=norm_cfg, last_layer=last_layer\n                )\n            )\n        in_filters = [num_filters[-1], *ds_num_filters[:-1]]\n        self.pfn_layers = nn.ModuleList(pfn_layers)\n        blocks = []\n        for i, layer_num in enumerate(layer_nums):\n            block = self._make_layer(\n                in_filters[i],\n                ds_num_filters[i],\n                kernel_size[i],\n                ds_layer_strides[i],\n                layer_num)\n            blocks.append(block)\n\n        self.blocks = nn.ModuleList(blocks)\n        self.ds_rate = np.prod(np.array(ds_layer_strides))\n\n    def _make_layer(self, inplanes, planes, kernel_size, stride, num_blocks):\n\n        layers = []\n        layers.append(SparseConvBlock(inplanes, planes,\n                      kernel_size=kernel_size, stride=stride, use_subm=False))\n\n        for j in range(num_blocks):\n            layers.append(SparseBasicBlock(planes, kernel_size=kernel_size))\n\n        return spconv.pytorch.SparseSequential(*layers)\n\n    def forward(self, features, unq, unq_inv, grid_size):\n        feature_pos = features[:,\n                               0:2] if self.mode == 'pillar' else features[:, 10:12]\n        device = feature_pos.device\n        voxel_size = torch.from_numpy(\n            self.voxel_size).type_as(feature_pos).to(device)\n        bias = torch.from_numpy(self.bias).type_as(feature_pos).to(device)\n        feature_pos = (feature_pos - bias) / voxel_size\n\n        for pfn in self.pfn_layers:\n            features = pfn(features, unq_inv)  # num_points, dim_feat\n        features_voxel = torch_scatter.scatter_max(features, unq_inv, dim=0)[0]\n        batch_size = len(torch.unique(unq[:, 0]))\n        x = spconv.pytorch.SparseConvTensor(\n            features_voxel, unq, grid_size, batch_size)\n\n        for i in range(len(self.blocks)):\n            x = self.blocks[i](x)\n        x = x.dense()\n        feature_pos = torch.cat(\n            (unq[unq_inv][:, 0:1], feature_pos / self.ds_rate), dim=-1)\n\n        return self.bilinear_interpolate(x, feature_pos)\n\n    def bilinear_interpolate(self, image, coords):\n        \"\"\"\n        image: (B, C, H, W)\n        coords: (N, 3): (B, y, x)\n        \"\"\"\n        x = coords[:, 1]\n        x0 = torch.floor(x).long()\n        x1 = x0 + 1\n\n        y = coords[:, 2]\n        y0 = torch.floor(y).long()\n        y1 = y0 + 1\n\n        B = coords[:, 0].long()\n\n        x0 = torch.clamp(x0, 0, image.shape[3] - 1)\n        x1 = torch.clamp(x1, 0, image.shape[3] - 1)\n        y0 = torch.clamp(y0, 0, image.shape[2] - 1)\n        y1 = torch.clamp(y1, 0, image.shape[2] - 1)\n\n        Ia = image[B, :, y0, x0]\n        Ib = image[B, :, y1, x0]\n        Ic = image[B, :, y0, x1]\n        Id = image[B, :, y1, x1]\n\n        wa = ((x1.type(torch.float32)-x) *\n              (y1.type(torch.float32)-y)).unsqueeze(-1)\n        wb = ((x1.type(torch.float32)-x) *\n              (y-y0.type(torch.float32))).unsqueeze(-1)\n        wc = ((x-x0.type(torch.float32)) *\n              (y1.type(torch.float32)-y)).unsqueeze(-1)\n        wd = ((x-x0.type(torch.float32)) *\n              (y-y0.type(torch.float32))).unsqueeze(-1)\n\n        features = Ia * wa + Ib * wb + Ic * wc + Id * wd\n\n        return features\n\n\nThe SingleView class is designed to perform convolutions on single views of LiDAR data. This class processes the input data through a series of convolutional layers.\n\nMain Methods\n\n_make_layer(): This method constructs a sequence of convolutional blocks (using sparse convolutions) for a particular layer, depending on the number of blocks specified.\nforward(): This method defines the forward pass of the network. It processes the input features through the PFNLayers and blocks, applies voxelization, and finally performs bilinear interpolation to match the original resolution.\nbilinear_interpolate(): This method performs bilinear interpolation to upscale the sparse feature maps to a dense format, using the original spatial coordinates of the features.\n\n\n# Instantiate the SingleView model\nin_channels = 64\nnum_filters = [128, 256]\nlayer_nums = [2, 2]\nds_layer_strides = [2, 2]\nds_num_filters = [256, 512]\nkernel_size = [3, 3]\nmode = 'pillar'\nvoxel_size = [0.2, 0.2]\npc_range = [-50, -50]\nnorm_cfg = None\nact_cfg = None\n\nmodel = SingleView(in_channels, num_filters, layer_nums, ds_layer_strides, ds_num_filters, kernel_size, mode, voxel_size, pc_range, norm_cfg, act_cfg).to(DEVICE)\n\n# Create dummy data and transfer to the same device\nfeatures = torch.randn(1000, 64, dtype=torch.float32).to(DEVICE)\nunq = torch.randint(0, 10, (1000, 3), dtype=torch.int32).to(DEVICE)\nunq_inv = torch.randint(0, 1000, (1000,), dtype=torch.int64).to(DEVICE)\ngrid_size = [200, 200]  # 2D spatial shape\n\n# Input shapes\nprint(\"Features Shape:\", features.shape)\n\n# Forward pass\noutput_features = model(features, unq, unq_inv, grid_size)\n\nprint(\"Output Features Shape:\", output_features.shape)\n\nFeatures Shape: torch.Size([1000, 64])\nOutput Features Shape: torch.Size([1000, 512])\n\n\n\nsource\n\n\n\nMVFFeatureNet\n\n MVFFeatureNet (in_channels, voxel_size, pc_range, cylinder_size,\n                cylinder_range, num_filters, layer_nums, ds_layer_strides,\n                ds_num_filters, kernel_size, out_channels)\n\nauthoured by Beijing-jinyu\n\n\n\n\nDetails\n\n\n\n\nin_channels\nNumber of input channels\n\n\nvoxel_size\nSize of voxels, only utilize x and y size\n\n\npc_range\nPoint cloud range, only utilize x and y min\n\n\ncylinder_size\nSize of cylinders, only utilize x and y size\n\n\ncylinder_range\nCylinder range, only utilize x and y min\n\n\nnum_filters\nNumber of features in each of the N PFNLayers\n\n\nlayer_nums\nNumber of blocks in each layer\n\n\nds_layer_strides\nStrides of each layer\n\n\nds_num_filters\nNumber of features in each layer\n\n\nkernel_size\nKernel size of each layer\n\n\nout_channels\nNumber of output channels\n\n\n\n\n\nExported source\nclass MVFFeatureNet(nn.Module):\n    \"\"\"\n    authoured by Beijing-jinyu\n    \"\"\"\n\n    def __init__(self,\n                in_channels, # Number of input channels\n                voxel_size, # Size of voxels, only utilize x and y size\n                pc_range, # Point cloud range, only utilize x and y min\n                cylinder_size, # Size of cylinders, only utilize x and y size\n                cylinder_range, # Cylinder range, only utilize x and y min\n                num_filters, # Number of features in each of the N PFNLayers\n                layer_nums, # Number of blocks in each layer\n                ds_layer_strides, # Strides of each layer\n                ds_num_filters, # Number of features in each layer\n                kernel_size, # Kernel size of each layer\n                out_channels # Number of output channels\n                ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.voxel_size = voxel_size\n        self.pc_range = pc_range\n        self.cylinder_range = cylinder_range\n        self.cylinder_size = cylinder_size\n\n        self.voxelization = PillarVoxelNet(voxel_size, pc_range)\n        self.cylinderlization = CylinderNet(cylinder_size, cylinder_range)\n\n        self.pillarview = SingleView((in_channels + 5) * 2, num_filters, layer_nums, ds_layer_strides,\n                                     ds_num_filters, kernel_size, 'pillar', self.voxel_size, self.pc_range)\n        self.cylinderview = SingleView((in_channels + 5) * 2, num_filters, layer_nums, ds_layer_strides,\n                                       ds_num_filters, kernel_size, 'cylinder', self.cylinder_size, self.cylinder_range)\n        self.ds_rate = np.prod(np.array(ds_layer_strides))\n\n        self.pointnet1 = PointNet((in_channels + 5) * 2, ds_num_filters[-1])\n        self.pointnet2 = PointNet(ds_num_filters[-1] * 3, out_channels)\n\n    def forward(self, points):\n        dtype = points.dtype\n        pc_range = torch.tensor(self.pc_range, dtype=dtype)\n        mask = reduce(torch.logical_and, (points[:, 1] &gt;= pc_range[0],\n                                          points[:, 1] &lt; pc_range[3],\n                                          points[:, 2] &gt;= pc_range[1],\n                                          points[:, 2] &lt; pc_range[4],\n                                          points[:, 3] &gt;= pc_range[2],\n                                          points[:, 3] &lt; pc_range[5]))\n        points = points[mask]\n\n        pillar_feature, pillar_coords, pillar_inv, pillar_size = self.voxelization(\n            points)\n        cylinder_feature, cylinder_coords, cylinder_inv, cylinder_size = self.cylinderlization(\n            points)\n        points_feature = torch.cat((pillar_feature, cylinder_feature), dim=-1)\n\n        pillar_view = self.pillarview(\n            points_feature, pillar_coords, pillar_inv, pillar_size)\n        cylinder_view = self.cylinderview(\n            points_feature, cylinder_coords, cylinder_inv, cylinder_size)\n\n        points_feature = self.pointnet1(points_feature)\n        points_feature = torch.cat(\n            (points_feature, pillar_view, cylinder_view), dim=-1)\n        pillar_feature = self.pointnet2(points_feature)\n        pillar_feature = torch_scatter.scatter_max(\n            pillar_feature, pillar_inv, dim=0)[0]\n        batch_size = len(torch.unique(pillar_coords[:, 0]))\n        pillar_coords[:, 1:] = pillar_coords[:, 1:] // self.ds_rate\n        pillar_size = pillar_size // self.ds_rate\n        x = spconv.pytorch.SparseConvTensor(\n            pillar_feature, pillar_coords, pillar_size, batch_size)\n        return x.dense()\n\n\nThe MVFFeatureNet class aggregates multiple views of the point cloud data by combining the already presented classes, PillarVoxelNet and CylinderNet, to create a richer feature representation for 3D object detection tasks by using pillar-based and cylinder-based voxelization methods. The goal is to create a richer feature representation by leveraging the complementary strengths of these two methods:\n\nPillar Voxelization (PillarVoxelNet): Converts point cloud data into a pseudo-image representation by dividing the space into vertical columns (pillars). This method is efficient and captures the overall structure of the point cloud.\nCylinder Voxelization (CylinderNet): Divides the point cloud into cylindrical segments, which can better capture radial features and angular information that might be missed by pillar-based methods.\n\nThe MVFFeatureNet then processes these two types of voxelized features through separate convolutional pathways (SingleView) and integrates them using a series of PointNet modules to produce a final, unified feature representation.\n\nVoxelization and Cylinderization: The point cloud data is first pre-processed by the PillarVoxelNet and CylinderNet, which convert the raw point cloud data into structured formats that are easier to process with convolutional layers.\nSingleView Processing: Each view (pillar and cylinder) is processed independently using the SingleView class, which applies a series of convolutional layers to extract features. These features are later merged to provide a multi-view representation of the data.\nPointNet: This module processes the combined features from both views. PointNet is well-suited for handling irregular point cloud data and is used here to further refine the feature representation.\nSparse Convolution Tensor: Finally, the processed features are packed into a sparse tensor format using spconv.pytorch.SparseConvTensor and returned as the output of the network.",
    "crumbs": [
      "Model: readers"
    ]
  }
]